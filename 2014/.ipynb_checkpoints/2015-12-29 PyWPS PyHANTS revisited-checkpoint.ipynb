{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# # WPS process for regional HANTS processing\n",
    "# This process implements the HANTS reconstruction algorithm, which can be used to process reginal data. In addition, this algorithm has been published by WPS protocal. \n",
    "# The input for the wps is a valid link to a xml, where one can set all parameters involed in the processing.\n",
    "# The wps procedure return a url link where users can donwload the reconstruction result.\n",
    "# \n",
    "# The processing process is implemented under parallel schema, whcih can speed up the processing by adding more cpu resources to the computation server. Specifically, the basic process structure is composed of one reading (data retrieving) thread, one writing thread and several paralel CPU processes. The reason of using just one thead for reading and writing is that the speed of I/O on the server is limited. \n",
    "# Currently, on a VM ubuntu 14.4 system (3 cpu cores), to precess a NDVI time series data set (141(column)X100(row)X23(sences)) cost 90 second in total.\n",
    "# \n",
    "# Limitations & further expected improvement\n",
    "#     *Data source. Now the process can only retrieve data from rasdaman database. It should be improved to support file   system based data. In that way, normal users can upload original data to specific ftp server and then request the reconstruction service.\n",
    "#     *Data type. Now the process only suport 16-bit integer data for I/O. More data type support need to be added and users can set the data type based on their real data.\n",
    "#     *Output in random time interval. Now the output file hold the same time stamp as the input file. \n",
    "#     *Redesign the parameter setting file by xlsd.\n",
    "#     *A web form which can be used by users to fill parameter setting file.\n",
    "#     *Client side visualization of reconstruction result.\n",
    "# Script to excute the wps:\n",
    "#     http://localhost/cgi-bin/pywps.cgi?service=wps&version=1.0.0&request=execute&identifier=WPS_HANTS_RECON_BATCH&datainputs=[ps_xml=http://localhost/html/HANTS_PS.xml]\n",
    "# \n",
    "# Author: Jie Zhou, Martijn\n",
    "# Date: 14/04/2015\n",
    "# \n",
    "\n",
    "#from pywps.Process import WPSProcess \n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import urllib2\n",
    "import urllib\n",
    "import gdal\n",
    "import osr\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import multiprocessing as mp\n",
    "import threading\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_values(id, json_repr):\n",
    "    results = []\n",
    "    def _decode_dict(a_dict):\n",
    "        try: results.append(a_dict[id])\n",
    "        except KeyError: pass\n",
    "        return a_dict\n",
    "    json.loads(json_repr, object_hook=_decode_dict)  # return value ignored\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The kernal of HANTS algorithm. Implemented by Mattijn.\n",
    "def makediag3d(M):\n",
    "    b = np.zeros((M.shape[0], M.shape[1] * M.shape[1]))\n",
    "    b[:, ::M.shape[1] + 1] = M\n",
    "    \n",
    "    logging.info('function `makediag3d` complete')    \n",
    "    return b.reshape(M.shape[0], M.shape[1], M.shape[1]) \n",
    "\n",
    "def get_starter_matrix(base_period_len, sample_count, frequencies_considered_count):\n",
    "    nr = min(2 * frequencies_considered_count + 1,\n",
    "                  sample_count)  # number of 2*+1 frequencies, or number of input images\n",
    "    mat = np.zeros(shape=(nr, sample_count))\n",
    "    mat[0, :] = 1\n",
    "    ang = 2 * np.pi * np.arange(base_period_len) / base_period_len\n",
    "    cs = np.cos(ang)\n",
    "    sn = np.sin(ang)\n",
    "    # create some standard sinus and cosinus functions and put in matrix\n",
    "    i = np.arange(1, frequencies_considered_count + 1)\n",
    "    ts = np.arange(sample_count)\n",
    "    for column in xrange(sample_count):\n",
    "        index = np.mod(i * ts[column], base_period_len)\n",
    "        # index looks like 000, 123, 246, etc, until it wraps around (for len(i)==3)\n",
    "        mat[2 * i - 1, column] = cs.take(index)\n",
    "        mat[2 * i, column] = sn.take(index)\n",
    "\n",
    "    logging.info('function `get_starter_matrix` complete')\n",
    "    return mat\n",
    "\n",
    "def HANTS(sample_count, inputs,\n",
    "          frequencies_considered_count=3,\n",
    "          outliers_to_reject='Lo',\n",
    "          low=0., high=255,\n",
    "          fit_error_tolerance=5,\n",
    "          dod = 5,\n",
    "          delta=0.1):\n",
    "    \"\"\"\n",
    "    Function to apply the Harmonic analysis of time series applied to arrays\n",
    "\n",
    "    sample_count    = nr. of images (total number of actual samples of the time series)\n",
    "    base_period_len    = length of the base period, measured in virtual samples\n",
    "            (days, dekads, months, etc.)\n",
    "    frequencies_considered_count    = number of frequencies to be considered above the zero frequency\n",
    "    inputs     = array of input sample values (e.g. NDVI values)\n",
    "    ts    = array of size sample_count of time sample indicators\n",
    "            (indicates virtual sample number relative to the base period);\n",
    "            numbers in array ts maybe greater than base_period_len\n",
    "            If no aux file is used (no time samples), we assume ts(i)= i,\n",
    "            where i=1, ..., sample_count\n",
    "    outliers_to_reject  = 2-character string indicating rejection of high or low outliers\n",
    "            select from 'Hi', 'Lo' or 'None'\n",
    "    low   = valid range minimum\n",
    "    high  = valid range maximum (values outside the valid range are rejeced\n",
    "            right away)\n",
    "    fit_error_tolerance   = fit error tolerance (points deviating more than fit_error_tolerance from curve\n",
    "            fit are rejected)\n",
    "    dod   = degree of overdeterminedness (iteration stops if number of\n",
    "            points reaches the minimum required for curve fitting, plus\n",
    "            dod). This is a safety measure\n",
    "    delta = small positive number (e.g. 0.1) to suppress high amplitudes\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info('HANTS is active %s', inputs.shape())\n",
    "\n",
    "    # define some parameters\n",
    "    base_period_len = sample_count  #\n",
    "    \n",
    "    # check which setting to set for outlier filtering\n",
    "    if outliers_to_reject == 'Hi':\n",
    "        sHiLo = -1\n",
    "    elif outliers_to_reject == 'Lo':\n",
    "        sHiLo = 1\n",
    "    else:\n",
    "        sHiLo = 0\n",
    "\n",
    "    nr = min(2 * frequencies_considered_count + 1,\n",
    "             sample_count)  # number of 2*+1 frequencies, or number of input images\n",
    "\n",
    "    # create empty arrays to fill\n",
    "    outputs = np.zeros(shape=(inputs.shape[0], sample_count))\n",
    "\n",
    "    mat = get_starter_matrix(base_period_len, sample_count, frequencies_considered_count)\n",
    "\n",
    "    # repeat the mat array over the number of arrays in inputs\n",
    "    # and create arrays with ones with shape inputs where high and low values are set to 0\n",
    "    mat = np.tile(mat[None].T, (1, inputs.shape[0])).T\n",
    "    p = np.ones_like(inputs)\n",
    "    p[(low >= inputs) | (inputs > high)] = 0\n",
    "    nout = np.sum(p == 0, axis=-1)  # count the outliers for each timeseries\n",
    "\n",
    "\n",
    "    # prepare for while loop\n",
    "    ready = np.zeros((inputs.shape[0]), dtype=bool)  # all timeseries set to false\n",
    "\n",
    "    #dod = 1  # (2*frequencies_considered_count-1)  # Um, no it isn't :/\n",
    "    noutmax = sample_count - nr - dod\n",
    "    # prepare to add delta to suppress high amplitudes but not for [0,0]\n",
    "    Adelta = np.tile(np.diag(np.ones(nr))[None].T, (1, inputs.shape[0])).T * delta\n",
    "    Adelta[:, 0, 0] -= delta\n",
    "    \n",
    "    \n",
    "    for _ in xrange(sample_count):\n",
    "        if ready.all():\n",
    "            break        \n",
    "        \n",
    "        # multiply outliers with timeseries\n",
    "        za = np.einsum('ijk,ik->ij', mat, p * inputs)\n",
    "        #print za\n",
    "\n",
    "        # multiply mat with the multiplication of multiply diagonal of p with transpose of mat\n",
    "        diag = makediag3d(p)\n",
    "        #print diag\n",
    "        \n",
    "        A = np.einsum('ajk,aki->aji', mat, np.einsum('aij,jka->ajk', diag, mat.T))\n",
    "        # add delta to suppress high amplitudes but not for [0,0]\n",
    "        A += Adelta\n",
    "        #A[:, 0, 0] = A[:, 0, 0] - delta\n",
    "        #print A\n",
    "\n",
    "        # solve linear matrix equation and define reconstructed timeseries\n",
    "        zr = np.linalg.solve(A, za)\n",
    "        #print zr\n",
    "        \n",
    "        outputs = np.einsum('ijk,kj->ki', mat.T, zr)\n",
    "        #print outputs\n",
    "\n",
    "        # calculate error and sort err by index\n",
    "        err = p * (sHiLo * (outputs - inputs))\n",
    "        rankVec = np.argsort(err, axis=1, )\n",
    "\n",
    "        # select maximum error and compute new ready status\n",
    "        maxerr = np.max(err, axis=-1)\n",
    "        #maxerr = np.diag(err.take(rankVec[:, sample_count - 1], axis=-1))\n",
    "        ready = (maxerr <= fit_error_tolerance) | (nout == noutmax)        \n",
    "\n",
    "        # if ready is still false\n",
    "        if not ready.all():\n",
    "            j = rankVec.take(sample_count - 1, axis=-1)\n",
    "\n",
    "            p.T[j.T, np.indices(j.shape)] = p.T[j.T, np.indices(j.shape)] * ready.astype(\n",
    "                int)  #*check\n",
    "            nout += 1\n",
    "\n",
    "    logging.info('function `HANTS` complete')\n",
    "    print 'function HANTS complete'\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Paralel schema for region processing using HANTS\n",
    "def __ReadLineData(inq, paramsDict, latlist, status, tFlag):\n",
    "    try:\n",
    "        logger = mp.get_logger()\n",
    "        logger.info(\"%s start.\", 'Reading')  \n",
    "        iline=0\n",
    "        \n",
    "        while True:\n",
    "            with status.get_lock():\n",
    "                if status.value == 10 : break\n",
    "            if inq.qsize() > 100 :\n",
    "                continue\n",
    "\n",
    "            #retrieve the data\n",
    "            inStartDate = (datetime.strptime(paramsDict['inStartDate'], \"%Y-%m-%d\")-datetime(1601,1,1)).days\n",
    "            inEndDate = (datetime.strptime(paramsDict['inEndDate'], \"%Y-%m-%d\")-datetime(1601,1,1)).days\n",
    "            field = {}\n",
    "            #field['SERVICE'] = 'WCS'\n",
    "            #field['VERSION'] = '2.0.1'\n",
    "            #field['REQUEST'] = 'ProcessCoverages'\n",
    "            field['query'] = 'for c in ('+paramsDict['coverageID']+') return encode( scale( c[ansi('+str(inStartDate)+':'+str(inEndDate)+'),'+'Lat('+str(latlist[iline])+'),'+'Long('+str(paramsDict['lLon'])+':'+str(paramsDict['rLon'])+')],{ansi('+str(inStartDate)+':'+str(inEndDate)+'),Long(0:'+str(paramsDict['xScale'])+')}),\"netcdf\")'            \n",
    "            url_values = urllib.unquote_plus(urllib.urlencode(field, doseq=True))\n",
    "            full_url = paramsDict['wcsEndpoint'] + '?' + url_values\n",
    "            logger.info('full_url to rasdaman is: %s', full_url)\n",
    "            \n",
    "            # retrieve the file\n",
    "            tmpfilename = 'test.nc'\n",
    "            f,h = urllib.urlretrieve(full_url,tmpfilename)\n",
    "            \n",
    "            data = gdal.Open('NETCDF:'+tmpfilename+':Band1').ReadAsArray()\n",
    "            logger.info('data shape: %s', data.shape)\n",
    "            \n",
    "            #fcsv = urllib.urlopen(full_url)\n",
    "            #str1 = fcsv.read()\n",
    "            #str1 = (str1[38:len(str1)-19].split('},{'))\n",
    "            #nsample = len(str1)\n",
    "            #data = np.array(map(int, ','.join(str1).split(','))).reshape(nsample,-1)\n",
    "            \n",
    "            with tFlag.get_lock():\n",
    "                inq.put((iline, data.T))\n",
    "                logger.info('iline and data: %s', iline, data.T.shape)\n",
    "                tFlag[iline] = 1\n",
    "                iline = iline + 1\n",
    "                if iline == len(tFlag): break\n",
    "\n",
    "    except:\n",
    "        logger.warning(\"Reading Failed: %s\", sys.exc_info())\n",
    "        logging.error(\"reading Failed: %s\", mp.current_process().name, sys.exc_info())\n",
    "        with status.get_lock():\n",
    "                status.value = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def __WriteLineData(outq, paramsDict, status, tFlag):\n",
    "    \n",
    "    try:\n",
    "        logger = mp.get_logger()\n",
    "        logger.info(\"%s start.\", 'Writing')  \n",
    "        print 'Writing...'\n",
    "        #generate filenames\n",
    "        startDate = datetime.strptime(paramsDict['outStartDate'], \"%Y-%m-%d\")\n",
    "        endDate = datetime.strptime(paramsDict['outEndDate'], \"%Y-%m-%d\")\n",
    "        numdays = (endDate - startDate).days\n",
    "        print 'startDate, endDate, numdays:', startDate, endDate, numdays\n",
    "        date_list = [(startDate + timedelta(days = x)).strftime('%Y-%m-%d') for x in np.arange(0, numdays, paramsDict['outInterval'])]\n",
    "        print 'date_list:', date_list\n",
    "        odir = paramsDict['outDir'] + '\\\\' + 'recon'\n",
    "        if os.path.isdir(odir):\n",
    "            shutil.rmtree(odir)\n",
    "        os.mkdir(odir)\n",
    "        filenames = [os.path.join(odir, '.'.join([paramsDict['outPrefix'], \n",
    "                                                  str(dt), paramsDict['outSuffix'], \"tiff\"])) for dt in date_list]\n",
    "        \n",
    "        #create tiff files\n",
    "        for ifile in filenames:\n",
    "            print 'ifile:',ifile\n",
    "            driver = gdal.GetDriverByName( \"GTiff\" )\n",
    "            ds = driver.Create(ifile, paramsDict['xOSize'], paramsDict['yOSize'], 1, gdal.GDT_Int16)\n",
    "            \n",
    "            #set projection information \n",
    "            srs = osr.SpatialReference()\n",
    "            srs.ImportFromEPSG(paramsDict['ProjectionEPSG'])\n",
    "            ds.SetProjection(srs.ExportToWkt()) \n",
    "            geotransform = (float(paramsDict['lLon']), paramsDict['outSpatialResolution'], 0,\n",
    "                            float(paramsDict['uLat']), 0, float(paramsDict['outSpatialResolution']))  \n",
    "            ds.SetGeoTransform(geotransform) \n",
    "            ds = None\n",
    "            \n",
    "        while True:\n",
    "            #print \"outq size:\", outq.qsize()\n",
    "            with status.get_lock():\n",
    "                if status.value == 10: break\n",
    "            #sys.stdout.flush()\n",
    "            with tFlag.get_lock():\n",
    "                felist = np.where(np.asarray(tFlag)!= 4)[0]\n",
    "                if len(felist) == 0: break\n",
    "                #now begin to write \n",
    "                if outq.qsize() == 0: continue\n",
    "                outTruple = outq.get()\n",
    "                \n",
    "                data = outTruple[1]\n",
    "                data = data.transpose()\n",
    "                iline = outTruple[0]\n",
    "                tFlag[iline] = 4\n",
    "            for i in range(len(filenames)):\n",
    "                ds = gdal.Open(filenames[i], gdal.GA_Update)\n",
    "                ds.GetRasterBand(1).WriteArray(data[i:i+1,:], 0, paramsDict['yOSize'] - iline - 1)\n",
    "                ds = None\n",
    "        logger.info(\"%s end.\", 'Writing')  \n",
    "\n",
    "    except:\n",
    "        \n",
    "        logger.warning(\"Writing Failed: %s\", sys.exc_info())\n",
    "        logging.error(\"Writing Failed: %s\", sys.exc_info())\n",
    "        with status.get_lock():\n",
    "                status.value = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __RegionHants_Subprocess(inq, outq, paramsDict, status, tFlag):\n",
    "    #logger.info(\"%s start.\", \"__RegionHants_Subprocess\")\n",
    "    try:\n",
    "        logger = mp.get_logger()\n",
    "        logger.info(\"%s start.\", mp.current_process().name)\n",
    "\n",
    "        while True:\n",
    "            with status.get_lock():\n",
    "                if status.value == 10: break\n",
    "            with tFlag.get_lock():\n",
    "                if  len(np.where(np.asarray(tFlag) < 2)[0]) == 0:\n",
    "                    break\n",
    "                if inq.qsize() == 0: continue\n",
    "                    \n",
    "                inTruple = inq.get()\n",
    "                logger.info('inTrupple: %s', inTruple)\n",
    "                tFlag[inTruple[0]]=2\n",
    "\n",
    "            data = inTruple[1]\n",
    "            outData = np.zeros_like(data)\n",
    "\n",
    "            for i in range(data.shape[0]-1):\n",
    "                   outData[i:i+1,:] = HANTS(data.shape[1], data[i:i+1, :],\n",
    "                                            frequencies_considered_count = paramsDict['nf'],\n",
    "                                            outliers_to_reject = 'Lo',\n",
    "                                            low = paramsDict['low'], \n",
    "                                            high = paramsDict['high'],\n",
    "                                            fit_error_tolerance = paramsDict['toe'],\n",
    "                                            dod = paramsDict['dod'],\n",
    "                                            delta = 0.1)\n",
    "\n",
    "            with tFlag.get_lock():\n",
    "                tFlag[inTruple[0]] = 3\n",
    "                outq.put((inTruple[0], outData))\n",
    "        logger.info(\"%s end successfully with status %s\", mp.current_process().name, status.value) \n",
    "    except:\n",
    "        logger.warning(\"%s Failed: %s\", mp.current_process().name, sys.exc_info())\n",
    "        logging.error(\"%s Failed: %s\", mp.current_process().name, sys.exc_info())\n",
    "        with status.get_lock():\n",
    "                status.value = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RegionHants(ps_json): #ps_xml\n",
    "    try:        \n",
    "        with open(ps_json) as data_file:\n",
    "            hants_ps = data_file.read()\n",
    "        \n",
    "        paramsDict = {}\n",
    "        #Data source params. Currently only support retrieving data from rasdaman\n",
    "        paramsDict['wcsEndpoint'] = find_values('WCSEndpoint', hants_ps)[0]\n",
    "        paramsDict['coverageID'] = find_values('CoverageID', hants_ps)[0]\n",
    "\n",
    "        paramsDict['inStartDate'] = find_values('StartDate', hants_ps)[0] # DateRange\n",
    "        paramsDict['inEndDate'] = find_values('EndDate', hants_ps)[0] # DateRange\n",
    "        paramsDict['inInterval'] = find_values('Interval', hants_ps)[0] # DateRange\n",
    "\n",
    "        paramsDict['SpatialResolution'] = find_values('SpatialResolution', hants_ps)[0]\n",
    "        paramsDict['ProjectionEPSG'] = find_values('ProjectionEPSG', hants_ps)[0]\n",
    "        paramsDict['FillValue'] = find_values('FillValue', hants_ps)[0]\n",
    "\n",
    "        paramsDict['lLon'] = find_values('LeftLon', hants_ps)[0]\n",
    "        paramsDict['rLon'] = find_values('RightLon', hants_ps)[0]\n",
    "        paramsDict['uLat'] = find_values('UpperLat', hants_ps)[0]\n",
    "        paramsDict['bLat'] = find_values('BottomLat', hants_ps)[0]\n",
    "\n",
    "        #Algorithm parameters\n",
    "        paramsDict['bPer'] = find_values('BasePeriod', hants_ps)[0] \n",
    "        paramsDict['nf'] = find_values('NumberOfFrequencies', hants_ps)[0]\n",
    "        paramsDict['per'] = map(int,find_values('Periods', hants_ps)[0].split(',')) \n",
    "        paramsDict['toe'] = find_values('ToleranceOfError', hants_ps)[0]\n",
    "        paramsDict['low'] = find_values('LowValue', hants_ps)[0]\n",
    "        paramsDict['high'] = find_values('HighValue', hants_ps)[0]\n",
    "        paramsDict['dod'] = find_values('DegreeOfOverdetermination', hants_ps)[0]\n",
    "\n",
    "        #output settings\n",
    "        paramsDict['outDir'] = find_values('OutDataDir', hants_ps)[0]\n",
    "        paramsDict['outPrefix'] = find_values('OutPrefix', hants_ps)[0]\n",
    "        paramsDict['outSuffix'] = find_values('OutSuffix', hants_ps)[0]\n",
    "        paramsDict['outSpatialResolution'] = find_values('OutSpatialResolution', hants_ps)[0]\n",
    "\n",
    "        paramsDict['outStartDate'] = find_values('StartDate', hants_ps)[1] # RegularOut\n",
    "        paramsDict['outEndDate'] = find_values('EndDate', hants_ps)[1] # RegularOut \n",
    "        paramsDict['outInterval'] = find_values('Interval', hants_ps)[1] # RegularOut\n",
    "\n",
    "        baseRes = paramsDict['SpatialResolution']\n",
    "        outRes = paramsDict['outSpatialResolution'] #basic resolution of output data\n",
    "        paramsDict['xBSize'] = int((int(paramsDict['rLon']) - int(paramsDict['lLon'])) / baseRes)\n",
    "        paramsDict['yBSize'] = int((int(paramsDict['uLat']) - int(paramsDict['bLat'])) / baseRes)\n",
    "        paramsDict['xOSize'] = int(paramsDict['xBSize'] * baseRes / outRes) + 1\n",
    "        paramsDict['yOSize'] = int(paramsDict['yBSize'] * baseRes / outRes) + 1\n",
    "        paramsDict['xScale'] = 20#(paramsDict['xOSize']-1) * baseRes\n",
    "        paramsDict['yScale'] = (paramsDict['yOSize']-1) * baseRes        \n",
    "        \n",
    "        ##Now begin to processing the regional data. \n",
    "        latlist = np.arange(float(paramsDict['bLat']),\n",
    "                            float(paramsDict['uLat']) + 0.1 * outRes,\n",
    "                            outRes)\n",
    "\n",
    "        # Define an input data queue\n",
    "        inq = mp.Queue()\n",
    "        # Define an output data queue\n",
    "        outq = mp.Queue()\n",
    "        #Define a status indicator array\n",
    "        status = mp.Value('b', 0)\n",
    "\n",
    "        latlist = latlist[:]\n",
    "        tFlag = mp.Array('i', np.zeros(len(latlist), np.int))\n",
    "\n",
    "        #config a filehandles for mp logger\n",
    "        logger = mp.get_logger()\n",
    "        fhandler = logging.FileHandler(filename=paramsDict['outDir'] + '/RegionalHants.log', mode='w')\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        fhandler.setFormatter(formatter)\n",
    "        logger.addHandler(fhandler)\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "        logger.info(\"%s start.\",'Main process') \n",
    "\n",
    "        rt = threading.Thread(target = __ReadLineData, \n",
    "                              name = 'Reading data',\n",
    "                              args = (inq, paramsDict, latlist, status, tFlag,))\n",
    "        rt.start()\n",
    "\n",
    "\n",
    "        procs=[]\n",
    "        for i in range(mp.cpu_count() + 2):\n",
    "            p = mp.Process(target = __RegionHants_Subprocess, \n",
    "                           args = (inq, outq, paramsDict, status, tFlag,),\n",
    "                           name = \"reconstruction process\" + str(i))\n",
    "            p.start()\n",
    "            logger.info('%s is alive: %s.', p.name, p.is_alive())\n",
    "            procs.append(p)\n",
    "\n",
    "        wt = threading.Thread(target = __WriteLineData,\n",
    "                              name = 'Writing data',\n",
    "                              args = (outq,paramsDict,status,tFlag))\n",
    "        wt.start()\n",
    "        \n",
    "        rt.join()\n",
    "\n",
    "        wt.join()\n",
    "\n",
    "        for p in procs: p.terminate()\n",
    "\n",
    "        inq = None\n",
    "        outq = None\n",
    "        status = None\n",
    "        logger.info(\"%s end.\", 'Main process') \n",
    "\n",
    "        return paramsDict['outDir']\n",
    "    except:\n",
    "        logger = mp.get_logger()\n",
    "        logger.warning(\"%s Failed: %s\", mp.current_process().name, sys.exc_info())\n",
    "        logging.error(\"%s Failed: %s\", mp.current_process().name, sys.exc_info())\n",
    "        return 'Processing Failed with following error information:' + str(sys.exc_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HANTS_PS_JSON = u'D:\\\\tmp\\\\HANTS_PS.json'\n",
    "RegionHants(HANTS_PS_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3402, 4020, 3927, 3319, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_url = 'http://159.226.117.95:58080/rasdaman/ows/wcs?query=for c in (NDVI_MOD13C1005_uptodate) return encode( scale( c[ansi(147192:147558),Lat(15.0),Long(90:110)],{ansi(147192:147558),Long(0:20.0)}),\"csv\")'\n",
    "fcsv = urllib.urlopen(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str1 = fcsv.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str2 = str1[1:-1].split('},{')\n",
    "nsample = len(str2)\n",
    "str3 = map(int, ','.join(str2).split(','))\n",
    "data = np.array(str3).reshape(nsample,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data.shape\n",
    "plt.imshow(np.ma.masked_equal(data,-3000), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_url = 'http://159.226.117.95:58080/rasdaman/ows/wcs?query=for c in (NDVI_MOD13C1005_uptodate) return encode( scale( c[ansi(147192:147558),Lat(15.0),Long(90:110)],{ansi(147192:147558),Long(0:20.0)}),\"netcdf\")'\n",
    "# retrieve the file\n",
    "tmpfilename = 'test2.nc'\n",
    "f,h = urllib.urlretrieve(full_url,tmpfilename)\n",
    "\n",
    "data = gdal.Open('NETCDF:'+tmpfilename+':Band1').ReadAsArray()\n",
    "print('data first line: %s', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.ma.masked_equal(data.T,-3000), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str1 = fcsv.read()\n",
    "#str1 = (str1[38:len(str1)-19].split('},{'))\n",
    "#nsample = len(str1)\n",
    "#data = np.array(map(int, ','.join(str1).split(','))).reshape(nsample,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver = gdal.GetDriverByName( \"GTiff\" )\n",
    "ds = driver.Create(r'D:\\tmp\\HANTS_OUT\\recon\\HANTS.2004-01-01.recon.tiff', \n",
    "                   paramsDict['xOSize'], paramsDict['yOSize'], 1, gdal.GDT_Int16)\n",
    "\n",
    "#set projection information \n",
    "\n",
    "srs = osr.SpatialReference()\n",
    "srs.ImportFromEPSG(paramsDict['ProjectionEPSG'])\n",
    "ds.SetProjection(srs.ExportToWkt())  \n",
    "geotransform = (float(paramsDict['lLon']), paramsDict['outSpatialResolution'], 0,\n",
    "                float(paramsDict['uLat']), 0, float(paramsDict['outSpatialResolution']))  \n",
    "ds.SetGeoTransform(geotransform) \n",
    "ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(HANTS_PS_JSON) as data_file:\n",
    "    hants_ps = data_file.read()\n",
    "\n",
    "\n",
    "paramsDict = {}\n",
    "#Data source params. Currently only support retrieving data from rasdaman\n",
    "paramsDict['wcsEndpoint'] = find_values('WCSEndpoint', hants_ps)[0]\n",
    "paramsDict['coverageID'] = find_values('CoverageID', hants_ps)[0]\n",
    "\n",
    "paramsDict['inStartDate'] = find_values('StartDate', hants_ps)[0] # DateRange\n",
    "paramsDict['inEndDate'] = find_values('EndDate', hants_ps)[0] # DateRange\n",
    "paramsDict['inInterval'] = find_values('Interval', hants_ps)[0] # DateRange\n",
    "\n",
    "paramsDict['SpatialResolution'] = find_values('SpatialResolution', hants_ps)[0]\n",
    "paramsDict['ProjectionEPSG'] = find_values('ProjectionEPSG', hants_ps)[0]\n",
    "paramsDict['FillValue'] = find_values('FillValue', hants_ps)[0]\n",
    "\n",
    "paramsDict['lLon'] = find_values('LeftLon', hants_ps)[0]\n",
    "paramsDict['rLon'] = find_values('RightLon', hants_ps)[0]\n",
    "paramsDict['uLat'] = find_values('UpperLat', hants_ps)[0]\n",
    "paramsDict['bLat'] = find_values('BottomLat', hants_ps)[0]\n",
    "\n",
    "#Algorithm parameters\n",
    "paramsDict['bPer'] = find_values('BasePeriod', hants_ps)[0] \n",
    "paramsDict['nf'] = find_values('NumberOfFrequencies', hants_ps)[0]\n",
    "paramsDict['per'] = map(int,find_values('Periods', hants_ps)[0].split(',')) \n",
    "paramsDict['toe'] = find_values('ToleranceOfError', hants_ps)[0]\n",
    "paramsDict['low'] = find_values('LowValue', hants_ps)[0]\n",
    "paramsDict['high'] = find_values('HighValue', hants_ps)[0]\n",
    "paramsDict['dod'] = find_values('DegreeOfOverdetermination', hants_ps)[0]\n",
    "\n",
    "#output settings\n",
    "paramsDict['outDir'] = find_values('OutDataDir', hants_ps)[0]\n",
    "paramsDict['outPrefix'] = find_values('OutPrefix', hants_ps)[0]\n",
    "paramsDict['outSuffix'] = find_values('OutSuffix', hants_ps)[0]\n",
    "paramsDict['outSpatialResolution'] = find_values('OutSpatialResolution', hants_ps)[0]\n",
    "\n",
    "paramsDict['outStartDate'] = find_values('StartDate', hants_ps)[1] # RegularOut\n",
    "paramsDict['outEndDate'] = find_values('EndDate', hants_ps)[1] # RegularOut \n",
    "paramsDict['outInterval'] = find_values('Interval', hants_ps)[1] # RegularOut\n",
    "\n",
    "baseRes = paramsDict['SpatialResolution']\n",
    "outRes = paramsDict['outSpatialResolution'] #basic resolution of output data\n",
    "paramsDict['xBSize'] = int((int(paramsDict['rLon']) - int(paramsDict['lLon'])) / baseRes)\n",
    "paramsDict['yBSize'] = int((int(paramsDict['uLat']) - int(paramsDict['bLat'])) / baseRes)\n",
    "paramsDict['xOSize'] = int(paramsDict['xBSize'] * baseRes / outRes) + 1\n",
    "paramsDict['yOSize'] = int(paramsDict['yBSize'] * baseRes / outRes) + 1\n",
    "paramsDict['xScale'] = (paramsDict['xOSize']-1) * baseRes\n",
    "paramsDict['yScale'] = (paramsDict['yOSize']-1) * baseRes   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int((110 - 70) / 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(800 * 0.05 / 0.5) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(81-1) * baseRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramsDict[\"xScale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enclosing the regional processing as a WPS service.\n",
    "class Process(WPSProcess):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        ##\n",
    "        # Process initialization\n",
    "        WPSProcess.__init__(self,\n",
    "            identifier = \"WPS_HANTS_RECON_BATCH\",\n",
    "            title=\"HANTS regional processing\",\n",
    "            abstract=\"\"\"This process intend to reconstruct regional EO data-set using HANTS.\"\"\",\n",
    "            version = \"1.0\",\n",
    "            storeSupported = True,\n",
    "            statusSupported = True)\n",
    "\n",
    "        ##\n",
    "        # Adding process inputs\n",
    "        \n",
    "        self.settingIn = self.addLiteralInput(identifier = \"ps_xml\",\n",
    "                title = \"Parameter settings xml file\",\n",
    "                type = type(''))\n",
    "\n",
    "        ##\n",
    "        # Adding process outputs\n",
    "\n",
    "        self.dataLinkOut = self.addLiteralOutput(identifier = \"result_dirlink\",\n",
    "                title = \"Result files location\",\n",
    "                type = type(''))\n",
    "\n",
    "        #self.textOut = self.addLiteralOutput(identifier = \"text\",\n",
    "         #       title=\"Output literal data\")\n",
    "    ##\n",
    "    # Execution part of the process\n",
    "    def execute(self):\n",
    "        \n",
    "        #Get the xml setting file string\n",
    "        psfile = self.settingIn.getValue()\n",
    "        logging.info(psfile)\n",
    "        \n",
    "        outDir = RegionHants(psfile)\n",
    "        self.dataLinkOut.setValue( outDir )\n",
    "        return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    starttime = datetime.now()\n",
    "    HANTS_PS_JSON = r'D:\\tmp\\\\HANTS_PS.json'\n",
    "    RegionHants(HANTS_PS_JSON)\n",
    "    #print RegionHants('http://localhost/html/HANTS_PS.xml')\n",
    "    timedelta = (datetime.now()-starttime)\n",
    "    print 'Total running time: %s' % timedelta.seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
