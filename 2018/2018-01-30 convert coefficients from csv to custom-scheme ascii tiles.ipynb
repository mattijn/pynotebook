{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "from xml.dom import minidom\n",
    "import glob\n",
    "import os\n",
    "from fnmatch import fnmatch\n",
    "import sys\n",
    "import time\n",
    "from shutil import copy\n",
    "from xml.etree.ElementTree import Element, SubElement, Comment\n",
    "import subprocess as sp\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta\n",
    "from osgeo import ogr\n",
    "import logging\n",
    "import re\n",
    "from itertools import *\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import ntpath\n",
    "from IPython.core.debugger import set_trace\n",
    "non_decimal = re.compile(r'[^\\d.,-]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def cmd(command, parse_directly=False):\n",
    "    \n",
    "    if parse_directly==True:\n",
    "        print(command)\n",
    "        norm = sp.Popen(command,stdout=sp.PIPE, shell=True)\n",
    "    else:\n",
    "        print(sp.list2cmdline(command))\n",
    "        norm = sp.Popen(sp.list2cmdline(command),stdout=sp.PIPE, shell=True)            \n",
    "             \n",
    "    out_cmd = norm.communicate()\n",
    "    return out_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def get_features(shape):\n",
    "    driver = ogr.GetDriverByName('ESRI Shapefile')\n",
    "\n",
    "    dataSource = driver.Open(shape, 0)  # 0 means read-only. 1 means writeable.\n",
    "\n",
    "    # Check to see if shapefile is found.\n",
    "    if dataSource is None:\n",
    "        logger.error('Could not open %s' % (shape))\n",
    "        return\n",
    "    else:\n",
    "        logger.info('Opened %s' % (shape))\n",
    "        layer = dataSource.GetLayer()\n",
    "        shape_features = layer.GetFeatureCount()\n",
    "        logger.info('Name of layer: %s' % layer.GetDescription())\n",
    "        logger.info(\"Number of features in %s: %d\" %\n",
    "                    (os.path.basename(shape), shape_features))\n",
    "\n",
    "        features_shape = []\n",
    "        for i in range(shape_features):\n",
    "            feat = layer.GetFeature(i)\n",
    "            obID = feat.GetField('OBJECTID')\n",
    "            features_shape.append(obID)\n",
    "#         logger.info (obID)\n",
    "        features_shape = list(set(features_shape))\n",
    "        return dataSource, layer, features_shape\n",
    "\n",
    "\n",
    "def iter_incrementing_file_names(path):\n",
    "    \"\"\"\n",
    "    Iterate incrementing file names. Start with path and add \" (n)\" before the\n",
    "    extension, where n starts at 1 and increases.\n",
    "\n",
    "    :param path: Some path\n",
    "    :return: An iterator.\n",
    "    \"\"\"\n",
    "    yield path\n",
    "    prefix, ext = os.path.splitext(path)\n",
    "    for i in itertools.count(start=1, step=1):\n",
    "        no = str(i).zfill(2)\n",
    "        # yield prefix + '_{0}'.format(i) + ext\n",
    "        yield prefix + '_' + no + ext\n",
    "\n",
    "\n",
    "def get_unique_filename(file_in):\n",
    "    for filename in iter_incrementing_file_names(file_in):\n",
    "        new_fn = Path(filename)\n",
    "        if new_fn.is_file():\n",
    "            pass\n",
    "        else:\n",
    "            return filename\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "def filterbyvalue(seq, value, end_prefix):\n",
    "    for el in seq:\n",
    "        #         print (el)\n",
    "        if el[0:end_prefix] == value:\n",
    "            yield el\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "def get_unique_lists(in_files, end_prefix=10):\n",
    "    all_lists = []\n",
    "    for idx in range(len(in_files)):\n",
    "        new_list = []\n",
    "        for inf in filterbyvalue(in_files, in_files[idx][0:end_prefix], end_prefix):\n",
    "            new_list.append(inf)\n",
    "#         print (new_list)\n",
    "        all_lists.append(new_list)\n",
    "    unique_data = [list(x) for x in set(tuple(x) for x in all_lists)]\n",
    "    return ([item for item in unique_data if len(item) > 1])\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "def saveRaster(path, array, dsSource, datatype=3, formatraster=\"GTiff\", nan=None):\n",
    "    \"\"\"\n",
    "    Datatypes:\n",
    "    unknown = 0\n",
    "    byte = 1\n",
    "    unsigned int16 = 2\n",
    "    signed int16 = 3\n",
    "    unsigned int32 = 4\n",
    "    signed int32 = 5\n",
    "    float32 = 6\n",
    "    float64 = 7\n",
    "    complex int16 = 8\n",
    "    complex int32 = 9\n",
    "    complex float32 = 10\n",
    "    complex float64 = 11\n",
    "    float32 = 6, \n",
    "    signed int = 3\n",
    "\n",
    "    Formatraster:\n",
    "    GeoTIFF = GTiff\n",
    "    Erdas = HFA (output = .img)\n",
    "    OGC web map service = WMS\n",
    "    png = PNG\n",
    "    \"\"\"\n",
    "    # Set Driver\n",
    "    format_ = formatraster  # save as format\n",
    "    driver = gdal.GetDriverByName(format_)\n",
    "    driver.Register()\n",
    "\n",
    "    # Set Metadata for Raster output\n",
    "    cols = dsSource.RasterXSize\n",
    "    rows = dsSource.RasterYSize\n",
    "    bands = dsSource.RasterCount\n",
    "    datatype = datatype  # band.DataType\n",
    "\n",
    "    # Set Projection for Raster\n",
    "    outDataset = driver.Create(path, cols, rows, bands, datatype)\n",
    "    geoTransform = dsSource.GetGeoTransform()\n",
    "    outDataset.SetGeoTransform(geoTransform)\n",
    "    proj = dsSource.GetProjection()\n",
    "    outDataset.SetProjection(proj)\n",
    "\n",
    "    # Write output to band 1 of new Raster and write NaN value\n",
    "    outBand = outDataset.GetRasterBand(1)\n",
    "    if nan != None:\n",
    "        outBand.SetNoDataValue(nan)\n",
    "    outBand.WriteArray(array)  # save input array\n",
    "    # outBand.WriteArray(dem)\n",
    "\n",
    "    # Close and finalise newly created Raster\n",
    "    #F_M01 = None\n",
    "    outBand = None\n",
    "    proj = None\n",
    "    geoTransform = None\n",
    "    outDataset = None\n",
    "    driver = None\n",
    "    datatype = None\n",
    "    bands = None\n",
    "    rows = None\n",
    "    cols = None\n",
    "    driver = None\n",
    "    array = None\n",
    "\n",
    "\n",
    "def prettify(elem):\n",
    "    \"\"\"Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = ElementTree.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"  \")\n",
    "\n",
    "\n",
    "def writeOGRVRT(basename, fullpath):\n",
    "    top = Element('OGRVRTDataSource')\n",
    "    child = SubElement(top, 'OGRVRTLayer')\n",
    "    child.set('name', basename)\n",
    "    sub_child_1 = SubElement(child, 'SrcDataSource')\n",
    "    sub_child_1.text = fullpath\n",
    "    sub_child_2 = SubElement(child, 'GeometryType')\n",
    "    sub_child_2.text = 'wkbPoint'\n",
    "    sub_child_3 = SubElement(child, 'LayerSRS')\n",
    "    sub_child_3.text = 'EPSG:28992'\n",
    "    sub_child_4 = SubElement(child, 'GeometryField')\n",
    "    sub_child_4.set('encoding', 'PointFromColumns')\n",
    "    sub_child_4.set('x', 'field_1')\n",
    "    sub_child_4.set('y', 'field_2')\n",
    "    sub_child_4.set('z', 'field_3')\n",
    "\n",
    "    return prettify(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def get_WD_folder(base, workdir):\n",
    "    base_list = base.split('_')\n",
    "    if base_list[5] == 'a':\n",
    "        coef = 'ruweData_A'        \n",
    "\n",
    "    if base_list[5] == 'b':\n",
    "        coef = 'ruweData_B'\n",
    "    \n",
    "    wd_dir = 'WD_{}_{}'.format(base_list[6].zfill(3), \n",
    "                                   base_list[7].zfill(3))    \n",
    "    \n",
    "    workdir_WD = os.path.join(workdir, coef, wd_dir)   \n",
    "    return workdir_WD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def datafolder_to_tiles(sourceRoot, coef, wd, asciiRoot, tmpRoot, backupRoot, y):\n",
    "\n",
    "    sourcedir = os.path.join(sourceRoot, coef, wd)\n",
    "    workdir = os.path.join(tmpRoot, coef, wd)\n",
    "    backupdir = os.path.join(backupRoot, coef, wd)\n",
    "    asciiDataDir = os.path.join(asciiRoot, coef, wd)\n",
    "\n",
    "    for root, dirs, files in os.walk(sourcedir):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                print(file)\n",
    "\n",
    "                init_file = os.path.join(os.path.abspath(root), file)\n",
    "                base, extension = os.path.splitext(file)\n",
    "                print(init_file)\n",
    "\n",
    "                # 0 get timestamp\n",
    "                print('# 0 get timestamp')\n",
    "                dateISO = datetime(\n",
    "                    int(base[0:4]), int(base[4:6]), int(base[6:8]))\n",
    "                t = dateISO.strftime(\"%Y%m%d%H%M%S\")\n",
    "                print(t)\n",
    "\n",
    "                # 1 convert survey data to CSV format\n",
    "                print('# 1 convert survey data to CSV format')\n",
    "                a = get_unique_filename(os.path.join(workdir, 'step1_{}.csv'.format(t)))\n",
    "                df = pd.read_csv(init_file, header=None, sep=';')\n",
    "                # include this to check the\n",
    "                if df.shape[1] != 3:\n",
    "                    print('no 3 columns, try delimiting whitespaces while parsing')\n",
    "                    df = pd.read_csv(init_file, header=None,\n",
    "                                     delim_whitespace=True)\n",
    "                    if df.shape[1] != 3:\n",
    "                        print(df.head())\n",
    "                        sys.exit(\n",
    "                            'Source file contains not exactly 3 columns. Check datasource')\n",
    "                print('file succesfully read and three columns are parsed')\n",
    "                df.dropna(inplace=True)\n",
    "                df.to_csv(a, header=False, index=False, sep=';')\n",
    "\n",
    "                # 2 build OGRVRT from CSV file\n",
    "                print('# 2 build OGRVRT from CSV file')\n",
    "                basename_a = os.path.basename(a)\n",
    "                base_a, extension = os.path.splitext(basename_a)\n",
    "                #base_a = 'step_{}.vrt'.format(t)\n",
    "                b = os.path.join(workdir, 'step2_{}.vrt'.format(t))\n",
    "                with open(b, 'w') as the_file:\n",
    "                    the_file.write(writeOGRVRT(base_a, a))\n",
    "\n",
    "                # 2.1 get Extent from OGRVRT\n",
    "                print('# 2.1 get Extent from OGRVRT')\n",
    "                # ogrinfo + ' -so ' + b + ' ' + base_a + ' | find \"Extent\"'\n",
    "                command = '{0} -so {1} {2} | find \"Extent\"'.format(\n",
    "                    ogrinfo, b, base_a)\n",
    "                norm = cmd(command, parse_directly=True)\n",
    "                extent = non_decimal.sub('', str(norm[0])).replace('-', ',')\n",
    "                bb = [x.strip() for x in extent.split(',')]\n",
    "                print(bb)\n",
    "\n",
    "                # 2.2 spatial query extent feature achtergrond SHP\n",
    "                print('# 2.2 spatial query extent feature achtergrond SHP')\n",
    "                try:\n",
    "                    z = get_unique_filename(\n",
    "                        os.path.join(workdir, 'step22_{}bg_sel.shp'.format(t)))\n",
    "\n",
    "                    layer_polygon = os.path.splitext(ntpath.basename(y))[0]\n",
    "                    layer_point = os.path.splitext(ntpath.basename(a))[0]\n",
    "                    file_layer_point = \"'{}'.{}\".format(b, layer_point)\n",
    "\n",
    "                    command = [ogr2ogr, '-f', \"ESRI Shapefile\", z, y, '-dialect', 'sqlite', '-sql',\n",
    "                               \"SELECT g.Geometry, g.OBJECTID, g.NoDataValu FROM {} g, {} p WHERE ST_Within(p.geometry, g.geometry)\".format(layer_polygon, file_layer_point)]\n",
    "                    cmd(command)\n",
    "                    gdf_z = gpd.read_file(z)\n",
    "                    #set_trace()\n",
    "                    gdf_z.drop_duplicates([\"OBJECTID\"], inplace=True)\n",
    "                    try: \n",
    "                        os.remove(z)\n",
    "                    except OSError:\n",
    "                        pass\n",
    "                    gdf_z.to_file(z)\n",
    "                    gdf_z = None\n",
    "\n",
    "#                     command = [ogr2ogr, z, y, '-spat', bb[0], bb[1], bb[2], bb[3]]\n",
    "#                     cmd(command)\n",
    "                except Exception as ex:\n",
    "                    print(ex)\n",
    "                    # continue\n",
    "\n",
    "                # 3 create RASTER from OGRVRT\n",
    "                print('# 3 create RASTER from OGRVRT')\n",
    "                c = get_unique_filename(os.path.join(workdir, 'step3_{}.tif'.format(t)))\n",
    "                command = [gdal_rasterize, '-a', 'field_3',\n",
    "                           '-tr', '1.0', '1.0', '-l', layer_point, b, c]\n",
    "                cmd(command)\n",
    "\n",
    "                try:\n",
    "                    ds, layer, z_features = get_features(z)\n",
    "                    print(z_features)\n",
    "                except Exception as ex:\n",
    "                    print(ex)\n",
    "                    # continue\n",
    "\n",
    "                for obID in (z_features):\n",
    "                    print(obID)\n",
    "\n",
    "                    # 4 clip point RASTER with feature achtergrond SHP\n",
    "                    print('# 4 clip point RASTER with feature achtergrond SHP')\n",
    "                    d = get_unique_filename(os.path.join(\n",
    "                        workdir, 'step4_' + str(obID).zfill(3) + '_' + t + '.tif'))\n",
    "                    command = [gdalwarp, '-srcnodata', '0', '-dstnodata', '-9999', '-overwrite', '-of', 'GTiff', '-crop_to_cutline',\n",
    "                               '-cutline', z, '-cwhere', 'OBJECTID=' + str(obID), c, d]\n",
    "                    cmd(command)\n",
    "\n",
    "                    # 5A convert feature achtergrond SHP to RASTER\n",
    "                    print('# 5A convert feature achtergrond SHP to RASTER')\n",
    "                    e = get_unique_filename(os.path.join(\n",
    "                        workdir, 'step5A_' + str(obID).zfill(3) + '_' + t + '_bg_tmp' + '.tif'))\n",
    "                    command = [gdal_rasterize, '-a', 'NoDataValu', '-a_srs', 'EPSG:28992', '-where', 'OBJECTID=' + str(obID),\n",
    "                               '-tr', '1.0', '1.0', '-l', layer.GetDescription(), z, e]\n",
    "                    cmd(command)\n",
    "\n",
    "                    # 5B clip achtergrond RASTER\n",
    "                    print('# 5B clip achtergrond RASTER')\n",
    "                    f = get_unique_filename(os.path.join(\n",
    "                        workdir, 'step5B_' + str(obID).zfill(3) + '_' + t + '_bg' + '.tif'))\n",
    "                    command = [gdalwarp, '-srcnodata', '-9999', '-dstnodata', '-9999', '-of', 'GTiff', '-tr', '1.0', '1.0',\n",
    "                               '-overwrite', '-crop_to_cutline', '-cutline', z, '-cwhere', 'OBJECTID=' + str(obID), e, f]\n",
    "                    cmd(command)\n",
    "\n",
    "                    # 6 Build VRT data source of point RASTER and feature achtergrond RASTER\n",
    "                    print(\n",
    "                        '# 6 Build VRT data source of point RASTER and feature achtergrond RASTER')\n",
    "                    g = get_unique_filename(os.path.join(\n",
    "                        workdir, 'step6_' + str(obID).zfill(3) + '_' + t + '.vrt'))\n",
    "                    command = [gdalbuildvrt, '-srcnodata', '-9999', g, d, f]\n",
    "                    cmd(command)\n",
    "\n",
    "                    # 7 Convert VRT to ArcInfoASCII\n",
    "                    print('# 7 Convert VRT to ArcInfoASCII')\n",
    "                    h = get_unique_filename(os.path.join(\n",
    "                        asciiDataDir, 'grid' + str(obID).zfill(3) + '_' + t + '.asc'))\n",
    "                    command = [gdal_translate, '-of',\n",
    "                               'AAIGrid', '-tr', '1.0', '1.0', g, h]\n",
    "                    cmd(command)\n",
    "\n",
    "                    # 8 Only keep the ArcInfoASCIIs that contains data\n",
    "                    print('# 8 Only keep the ArcInfoASCIIs that contains data')\n",
    "                    command = gdalinfo + ' -mm ' + h + ' | find \"Computed\"'\n",
    "                    norm = cmd(command, parse_directly=True)\n",
    "                    if len(norm[0]) == 0:\n",
    "                        for fl in glob.glob(h[0:-4] + '*'):\n",
    "                            print('remove grid since it doesnt contain data')\n",
    "                            os.remove(fl)\n",
    "\n",
    "                    # 8A Purge the bg_tmp files\n",
    "                    print('# 8A Purge all tmp files')\n",
    "                    try:\n",
    "#                         print('not forget to remove rasterized bg-clip')\n",
    "                        os.remove(d)  # point-clip vrt-file\n",
    "                        os.remove(e)  # rasterized bg-clip\n",
    "                        os.remove(f)  # wrap to cutline bg-clip\n",
    "                        os.remove(g)  # clip point-clip vrt with bg-clip\n",
    "                    except Exception as ex:\n",
    "                        print(ex)\n",
    "                        # continue\n",
    "                    # raise SystemExit(0)\n",
    "\n",
    "                # 9 Move file to ImportBackup\n",
    "                print('# 9 Move file to ImportBackup')\n",
    "                backup_file = os.path.join(\n",
    "                    os.path.abspath(backupdir), file)\n",
    "                try:\n",
    "                    os.remove(backup_file)\n",
    "                except OSError:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    os.rename(init_file, backup_file)\n",
    "                except Exception as ex:\n",
    "                    print(ex)\n",
    "                    # continue\n",
    "\n",
    "                # 10 Purge all tmp files\n",
    "                print('# 10 Purge all tmp files')\n",
    "                try:\n",
    "                    os.remove(a)  # csv-file\n",
    "                    os.remove(b)  # OGR vrt-file\n",
    "                    os.remove(z)  # shp-file\n",
    "                    os.remove(c)  # bg-clip shp-file\n",
    "#                     os.remove(d)  # point-clip vrt-file\n",
    "#                     os.remove(e)  # rasterized bg-clip\n",
    "#                     os.remove(f)  # wrap to cutline bg-clip\n",
    "#                     os.remove(g)  # clip point-clip vrt with bg-clip\n",
    "                except Exception as ex:\n",
    "                    print(ex)\n",
    "                    # continue\n",
    "                #raise SystemExit(0)\n",
    "\n",
    "    print('function complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def multi_tile_to_single_tile(asciiDataDir):\n",
    "    # merge ascis from same date and same grid\n",
    "    asc_files = []\n",
    "    for file in os.listdir(asciiDataDir):\n",
    "        if file.endswith(\".asc\"):\n",
    "            asc_files.append(file)\n",
    "\n",
    "    grid_date_groups = get_unique_lists(asc_files, 16) # 16 is geoxyz_ + timestamp yymmdd\n",
    "\n",
    "    # if none groups exist, do quit\n",
    "    if len(grid_date_groups) > 0:\n",
    "\n",
    "        for group in grid_date_groups:\n",
    "            i = os.path.join(asciiDataDir, group[0])\n",
    "            ds_i = gdal.Open(i, gdal.GA_ReadOnly)\n",
    "            ds_d = ds_i.ReadAsArray()\n",
    "            ds_d_cp = np.copy(ds_d)\n",
    "            ds_d_cp = np.ma.masked_equal(ds_d_cp, -9999.)\n",
    "\n",
    "            for idx in range(len(group)-1):\n",
    "                j = os.path.join(asciiDataDir, group[idx+1])\n",
    "                ds = gdal.Open(j, gdal.GA_ReadOnly).ReadAsArray()\n",
    "                ds_msk = np.ma.masked_equal(ds, -9999.)\n",
    "\n",
    "                # get first slice of copy, get second slice of original\n",
    "                k = ds_d_cp[::] \n",
    "                l = ds_msk[::]  \n",
    "\n",
    "                # update first slice based on second slice\n",
    "                k[~l.mask] = l.compressed()\n",
    "\n",
    "            # create file \n",
    "            m = get_unique_filename(i)[:-4]+'.tif'\n",
    "            saveRaster(m, ds_d_cp.data, ds_i, datatype=7, formatraster=\"GTiff\", nan=-9999.)\n",
    "\n",
    "            # flush files        \n",
    "            del ds_i, ds_d, ds_d_cp, ds, ds_msk, k, l\n",
    "\n",
    "            # covert to AAIGrid\n",
    "            # 11 Merge VRT to ascii-grid\n",
    "            n = m[:-4]+'.asc' # create unique asc file\n",
    "            command = [gdal_translate, '-of', 'AAIGrid', '-a_nodata', '-9999', m, n]\n",
    "            cmd(command)\n",
    "\n",
    "            # remove grids from before merge and change grid from new_name to org_name        \n",
    "            try:\n",
    "                os.remove(m)\n",
    "                for grid in group:\n",
    "                    logger.info (os.path.join(asciiDataDir, grid))\n",
    "                    os.remove(os.path.join(asciiDataDir, grid))    \n",
    "                os.rename(n, i)            \n",
    "            except Exception as ex:\n",
    "                logger.info (ex)\n",
    "                continue   \n",
    "\n",
    "    # purge redundant xml and prj files \n",
    "    xmlFiles = []\n",
    "    prjFiles = []\n",
    "    for root, dirs, files in os.walk(asciiDataDir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".xml\"):\n",
    "                 xmlFiles.append(os.path.join(root, file))\n",
    "            if file.endswith(\".prj\"):\n",
    "                 prjFiles.append(os.path.join(root, file)) \n",
    "\n",
    "    for xmlFile in xmlFiles:\n",
    "        os.remove(xmlFile)\n",
    "\n",
    "    for prjFile in prjFiles:\n",
    "        os.remove(prjFile)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "rootDir = r'D:\\FEWSProjecten\\OmsWaddenzee\\trunk\\fews'\n",
    "configDir = r'D:\\FEWSProjecten\\OmsWaddenzee\\trunk\\fews\\Config'\n",
    "\n",
    "ruweDataDir = os.path.join(rootDir, r'Import\\hkv\\coefficienten\\bronCsv')\n",
    "ruweDataDir_fixed_lines = os.path.join(rootDir, r'Import\\hkv\\coefficienten\\bronFixedlineCsv')\n",
    "ruweDataDirBackup = os.path.join(\n",
    "    rootDir, r'ImportBackup\\hkv\\coefficienten\\ruweDataBackup')\n",
    "\n",
    "asciiDataDir = os.path.join(rootDir, r'Import\\hkv\\coefficienten\\asciiData')\n",
    "workdir = os.path.join(rootDir, r'ImportInterim\\hkv\\coefficienten\\tmpData')\n",
    "\n",
    "y = os.path.join(\n",
    "    configDir, r'MapLayerFiles\\Achtergrond_polygonen//Achtergrond_polygonen.shp')\n",
    "\n",
    "logFile = os.path.join(\n",
    "    rootDir, r'ImportInterim\\hkv\\coefficienten//log_file.out')\n",
    "xmldir = os.path.join(\n",
    "    rootDir, r'ImportInterim\\hkv\\coefficienten\\XMLGenerated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "rootOgrGdal = r'C:\\Python35\\Lib\\site-packages\\osgeo'\n",
    "ogr2ogr = os.path.join(rootOgrGdal, 'ogr2ogr.exe')\n",
    "gdalwarp = os.path.join(rootOgrGdal, 'gdalwarp.exe')\n",
    "gdal_rasterize = os.path.join(rootOgrGdal, 'gdal_rasterize.exe')\n",
    "gdal_translate = os.path.join(rootOgrGdal, 'gdal_translate.exe')\n",
    "gdalbuildvrt = os.path.join(rootOgrGdal, 'gdalbuildvrt.exe')\n",
    "gdalinfo = os.path.join(rootOgrGdal, 'gdalinfo.exe')\n",
    "ogrinfo = os.path.join(rootOgrGdal, 'ogrinfo.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('survey2arcinfoascii')\n",
    "hdlr = logging.FileHandler(logFile)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "hdlr.setFormatter(formatter)\n",
    "logger.addHandler(hdlr)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# first get the tmp-csv files\n",
    "for root, dirs, files in os.walk(ruweDataDir):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):            \n",
    "            print(file)\n",
    "\n",
    "            init_file = os.path.join(os.path.abspath(root), file)\n",
    "            base, extension = os.path.splitext(file)\n",
    "            print(init_file)\n",
    "            \n",
    "            wd_folder = get_WD_folder(base, workdir)\n",
    "            print(wd_folder)\n",
    "\n",
    "            # 0 get timestamp\n",
    "            print('# 0 get timestamp')\n",
    "            #tmod = os.path.getmtime(init_file)\n",
    "            #t = time.strftime('%Y%m%d%H%M%S', time.localtime(int(tmod)))\n",
    "#           print (tmod.strftime(\"%Y%m%d%H%M%S\"))\n",
    "            dateISO = datetime(\n",
    "                int(base[14:18]), int(base[18:20]), int(base[20:22]))\n",
    "            # dateISO_yd = dateISO - timedelta(1)  # weird bug in fews\n",
    "            t = dateISO.strftime(\"%Y%m%d%H%M%S\")\n",
    "            print(t)\n",
    "\n",
    "            # 1 convert coeffients data to CSV format            \n",
    "            print('# 1 convert coeffients data to CSV format')\n",
    "            df = pd.read_csv(init_file, header=None)\n",
    "            # include this to check the\n",
    "            if df.shape[1] != 3:\n",
    "                print('no 3 columns, try delimiting whitespaces while parsing')\n",
    "                df = pd.read_csv(init_file, header=None, delim_whitespace=True)                \n",
    "                if df.shape[1] != 3:\n",
    "                    print(df.head())\n",
    "                    sys.exit(\n",
    "                        'Source file contains not exactly 3 columns. Check datasource')\n",
    "            print('file succesfully read and three columns are parsed')\n",
    "            #df.replace(0.0, np.nan, inplace=True)\n",
    "            #df.replace(0, np.nan, inplace=True)\n",
    "            df.dropna(inplace=True)\n",
    "            \n",
    "            \n",
    "            # 1,1 make tmp-csv files not longer than 100000 rows\n",
    "            print('# 1.1 make tmp-csv files not longer than 1000000 rows')\n",
    "            max_rows = 1000000\n",
    "            dataframes = []\n",
    "            while len(df) > max_rows:\n",
    "                top = df[:max_rows]\n",
    "                dataframes.append(top)\n",
    "                df = df[max_rows:]\n",
    "            else:\n",
    "                dataframes.append(df)            \n",
    "\n",
    "            for _, frame in enumerate(dataframes):                \n",
    "                a = get_unique_filename(os.path.join(wd_folder, t + '.csv'))\n",
    "                frame.to_csv(a, header=False, index=False, sep=';', float_format='%.10f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceRoot = ruweDataDir_fixed_lines\n",
    "tmpRoot = workdir\n",
    "asciiRoot = asciiDataDir\n",
    "backupRoot = ruweDataDirBackup\n",
    "                        \n",
    "for root, dirs, files in os.walk(sourceRoot):\n",
    "    for coef in dirs:        \n",
    "        if coef in ['ruweData_A']:\n",
    "            print(coef)\n",
    "            for root, dirs, files in os.walk(os.path.join(sourceRoot, coef)):\n",
    "                print (dirs)\n",
    "                for wd in dirs:\n",
    "                    if wd in ['WD_000_090']\n",
    "                        # check if file exist in folder otherwise skip the folder\n",
    "                        if os.listdir(os.path.join(sourceRoot, coef, wd)):                    \n",
    "                            print('WD folder exist:{0}{1}'.format(coef, wd))\n",
    "                            # following line execute function to create ASCII file from csv file\n",
    "                            datafolder_to_tiles(sourceRoot, coef, wd, asciiRoot, tmpRoot, backupRoot, y) \n",
    "                            asciiCoefWD = os.path.join(asciiRoot, coef, wd)\n",
    "                            print(asciiCoefWD)\n",
    "                            # following line execute function to combine ASCII files into a single ASCII file\n",
    "                            multi_tile_to_single_tile(asciiCoefWD)                        "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
