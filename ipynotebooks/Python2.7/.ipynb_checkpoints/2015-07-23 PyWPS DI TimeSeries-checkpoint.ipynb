{
 "metadata": {
  "name": "",
  "signature": "sha256:775e84c6182e0cd7566cad4778e6bf9f223ae91f4fda31d78ddd3377f2bffb58"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pywps.Process import WPSProcess \n",
      "import logging\n",
      "import os\n",
      "import sys\n",
      "import urllib2\n",
      "#from osgeo import gdal\n",
      "#import numpy\n",
      "import numpy as np\n",
      "#import numpy.ma as ma\n",
      "from lxml import etree\n",
      "import datetime\n",
      "#import matplotlib\n",
      "#import matplotlib.colors as mcolors\n",
      "#import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "from cStringIO import StringIO\n",
      "#import cStringIO\n",
      "import jdcal\n",
      "import json\n",
      "#%matplotlib inline\n",
      "#plt.style.use('ggplot')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Computing diagonal for each row of a 2d array. See: http://stackoverflow.com/q/27214027/2459096\n",
      "def makediag3d(M):\n",
      "    b = np.zeros((M.shape[0], M.shape[1] * M.shape[1]))\n",
      "    b[:, ::M.shape[1] + 1] = M\n",
      "    \n",
      "    logging.info('function `makediag3d` complete')    \n",
      "    return b.reshape(M.shape[0], M.shape[1], M.shape[1]) \n",
      "\n",
      "def get_starter_matrix(base_period_len, sample_count, frequencies_considered_count):\n",
      "    nr = min(2 * frequencies_considered_count + 1,\n",
      "                  sample_count)  # number of 2*+1 frequencies, or number of input images\n",
      "    mat = np.zeros(shape=(nr, sample_count))\n",
      "    mat[0, :] = 1\n",
      "    ang = 2 * np.pi * np.arange(base_period_len) / base_period_len\n",
      "    cs = np.cos(ang)\n",
      "    sn = np.sin(ang)\n",
      "    # create some standard sinus and cosinus functions and put in matrix\n",
      "    i = np.arange(1, frequencies_considered_count + 1)\n",
      "    ts = np.arange(sample_count)\n",
      "    for column in xrange(sample_count):\n",
      "        index = np.mod(i * ts[column], base_period_len)\n",
      "        # index looks like 000, 123, 246, etc, until it wraps around (for len(i)==3)\n",
      "        mat[2 * i - 1, column] = cs.take(index)\n",
      "        mat[2 * i, column] = sn.take(index)\n",
      "\n",
      "    logging.info('function `get_starter_matrix` complete')\n",
      "    return mat\n",
      "\n",
      "def HANTS(sample_count, inputs,\n",
      "          frequencies_considered_count=3,\n",
      "          outliers_to_reject='Lo',\n",
      "          low=0., high=255,\n",
      "          fit_error_tolerance=5,\n",
      "          delta=0.1):\n",
      "    \"\"\"\n",
      "    Function to apply the Harmonic analysis of time series applied to arrays\n",
      "\n",
      "    sample_count    = nr. of images (total number of actual samples of the time series)\n",
      "    base_period_len    = length of the base period, measured in virtual samples\n",
      "            (days, dekads, months, etc.)\n",
      "    frequencies_considered_count    = number of frequencies to be considered above the zero frequency\n",
      "    inputs     = array of input sample values (e.g. NDVI values)\n",
      "    ts    = array of size sample_count of time sample indicators\n",
      "            (indicates virtual sample number relative to the base period);\n",
      "            numbers in array ts maybe greater than base_period_len\n",
      "            If no aux file is used (no time samples), we assume ts(i)= i,\n",
      "            where i=1, ..., sample_count\n",
      "    outliers_to_reject  = 2-character string indicating rejection of high or low outliers\n",
      "            select from 'Hi', 'Lo' or 'None'\n",
      "    low   = valid range minimum\n",
      "    high  = valid range maximum (values outside the valid range are rejeced\n",
      "            right away)\n",
      "    fit_error_tolerance   = fit error tolerance (points deviating more than fit_error_tolerance from curve\n",
      "            fit are rejected)\n",
      "    dod   = degree of overdeterminedness (iteration stops if number of\n",
      "            points reaches the minimum required for curve fitting, plus\n",
      "            dod). This is a safety measure\n",
      "    delta = small positive number (e.g. 0.1) to suppress high amplitudes\n",
      "    \"\"\"\n",
      "\n",
      "    # define some parameters\n",
      "    base_period_len = sample_count  #\n",
      "\n",
      "    # check which setting to set for outlier filtering\n",
      "    if outliers_to_reject == 'Hi':\n",
      "        sHiLo = -1\n",
      "    elif outliers_to_reject == 'Lo':\n",
      "        sHiLo = 1\n",
      "    else:\n",
      "        sHiLo = 0\n",
      "\n",
      "    nr = min(2 * frequencies_considered_count + 1,\n",
      "             sample_count)  # number of 2*+1 frequencies, or number of input images\n",
      "\n",
      "    # create empty arrays to fill\n",
      "    outputs = np.zeros(shape=(inputs.shape[0], sample_count))\n",
      "\n",
      "    mat = get_starter_matrix(base_period_len, sample_count, frequencies_considered_count)\n",
      "\n",
      "    # repeat the mat array over the number of arrays in inputs\n",
      "    # and create arrays with ones with shape inputs where high and low values are set to 0\n",
      "    mat = np.tile(mat[None].T, (1, inputs.shape[0])).T\n",
      "    p = np.ones_like(inputs)\n",
      "    p[(low >= inputs) | (inputs > high)] = 0\n",
      "    nout = np.sum(p == 0, axis=-1)  # count the outliers for each timeseries\n",
      "\n",
      "    # prepare for while loop\n",
      "    ready = np.zeros((inputs.shape[0]), dtype=bool)  # all timeseries set to false\n",
      "\n",
      "    dod = 1  # (2*frequencies_considered_count-1)  # Um, no it isn't :/\n",
      "    noutmax = sample_count - nr - dod\n",
      "    # prepare to add delta to suppress high amplitudes but not for [0,0]\n",
      "    Adelta = np.tile(np.diag(np.ones(nr))[None].T, (1, inputs.shape[0])).T * delta\n",
      "    Adelta[:, 0, 0] -= delta\n",
      "    \n",
      "    for _ in xrange(sample_count):\n",
      "        if ready.all():\n",
      "            break        \n",
      "        \n",
      "        # multiply outliers with timeseries\n",
      "        za = np.einsum('ijk,ik->ij', mat, p * inputs)\n",
      "        #print za\n",
      "\n",
      "        # multiply mat with the multiplication of multiply diagonal of p with transpose of mat\n",
      "        diag = makediag3d(p)\n",
      "        #print diag\n",
      "        \n",
      "        A = np.einsum('ajk,aki->aji', mat, np.einsum('aij,jka->ajk', diag, mat.T))\n",
      "        # add delta to suppress high amplitudes but not for [0,0]\n",
      "        A += Adelta\n",
      "        #A[:, 0, 0] = A[:, 0, 0] - delta\n",
      "        #print A\n",
      "\n",
      "        # solve linear matrix equation and define reconstructed timeseries\n",
      "        zr = np.linalg.solve(A, za)\n",
      "        #print zr\n",
      "        \n",
      "        outputs = np.einsum('ijk,kj->ki', mat.T, zr)\n",
      "        #print outputs\n",
      "\n",
      "        # calculate error and sort err by index\n",
      "        err = p * (sHiLo * (outputs - inputs))\n",
      "        rankVec = np.argsort(err, axis=1, )\n",
      "\n",
      "        # select maximum error and compute new ready status\n",
      "        maxerr = np.max(err, axis=-1)\n",
      "        #maxerr = np.diag(err.take(rankVec[:, sample_count - 1], axis=-1))\n",
      "        ready = (maxerr <= fit_error_tolerance) | (nout == noutmax)        \n",
      "\n",
      "        # if ready is still false\n",
      "        if not ready.all():\n",
      "            j = rankVec.take(sample_count - 1, axis=-1)\n",
      "\n",
      "            p.T[j.T, np.indices(j.shape)] = p.T[j.T, np.indices(j.shape)] * ready.astype(\n",
      "                int)  #*check\n",
      "            nout += 1\n",
      "\n",
      "    logging.info('function `HANTS` complete')\n",
      "    return outputs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def convert_ansi_date(date, offset=0.5):\n",
      "    logging.info('function `convert_ansi_date` complete')\n",
      "    return jdcal.jd2gcal(2305812.5, date + offset) # 0.5 offset is to adjust from night to noon\n",
      "\n",
      "def unix_time(dt):\n",
      "    epoch = datetime.datetime.utcfromtimestamp(0)\n",
      "    delta = dt - epoch\n",
      "    logging.info('function `unix_time` complete')\n",
      "    return delta.total_seconds()    \n",
      "\n",
      "def unix_time_millis(dt):\n",
      "    logging.info('function `unix_time_millis` complete')\n",
      "    return int(unix_time(dt) * 1000)\n",
      "\n",
      "def region(pixel):\n",
      "    \"\"\"\n",
      "    Extract pixel or regio:\n",
      "    region1pix = single lat/lon\n",
      "    region4pix = block of 4 pixels [2x2]\n",
      "    region9pix = block of 9 pixels [3x3]\n",
      "    \"\"\"\n",
      "    if pixel == 0:\n",
      "        regionpix = [0,0]\n",
      "    elif pixel == 4:\n",
      "        regionpix = [-0.025,0.025]\n",
      "    elif pixel == 9:\n",
      "        regionpix = [-0.075,0.075]\n",
      "    else:\n",
      "        return\n",
      "    return regionpix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def COMPUTE_NVAI(lon_center=5.6943, lat_center=53.1533, pix_offset=0, \n",
      "                 from_date='2010-01-01', to_date='2012-01-01', pyhants=0, \n",
      "                 ann_freq=6, outliers_rj='None'):\n",
      "    \"\"\"\n",
      "    pyhants: 0 means will NOT use, 1 will use\n",
      "    \"\"\"\n",
      "    \n",
      "    regionpix = region(pix_offset)\n",
      "    \n",
      "    Long1 = str(lon_center + regionpix[0])\n",
      "    Long2 = str(lon_center + regionpix[1])\n",
      "    Lat1 = str(lat_center + regionpix[0])\n",
      "    Lat2 = str(lat_center + regionpix[1])\n",
      "    \n",
      "    full_url = \"http://159.226.117.95:8080/rasdaman/ows/wcs2?service=WCS&version=2.0.1&request=GetCoverage&coverageId=modis_13c1_cov&subset=Long(\"+Long1+\",\"+Long2+\")&subset=Lat(\"+Lat1+\",\"+Lat2+\")\"\n",
      "    f = urllib2.urlopen(full_url)     \n",
      "    root = etree.fromstring(f.read())    \n",
      "    \n",
      "    # read grid envelope of domain set\n",
      "    xml_low_env = StringIO(root[1][0][0][0][0].text)\n",
      "    xml_high_env = StringIO(root[1][0][0][0][1].text)\n",
      "\n",
      "    # load grid envelope as numpy array\n",
      "    low_env = np.loadtxt(xml_low_env, dtype='int', delimiter=' ')\n",
      "    high_env = np.loadtxt(xml_high_env, dtype='int', delimiter=' ')\n",
      "    ts_shape = high_env - low_env + 1\n",
      "\n",
      "    easting = ts_shape[0]\n",
      "    northing = ts_shape[1]\n",
      "    time = ts_shape[2]    \n",
      "\n",
      "    ## extract the dates\n",
      "    #sd = ansi_date_to_greg_date(low_env[2]+140734)\n",
      "    #ed = ansi_date_to_greg_date(high_env[2]+140734)\n",
      "\n",
      "    # extract the values we need from the parsed XML\n",
      "    sta_date_ansi = np.loadtxt(StringIO(root[0][0][0].text))[2] # 150116\n",
      "    end_date_ansi = np.loadtxt(StringIO(root[0][0][1].text))[2] # 150852\n",
      "    sta_date_rasd = np.loadtxt(StringIO(root[1][0][0][0][0].text))[2] # 9382\n",
      "    end_date_rasd = np.loadtxt(StringIO(root[1][0][0][0][1].text))[2] # 9427\n",
      "    timestep_date = np.loadtxt(StringIO(root[1][0][5].text))[2] # 16\n",
      "\n",
      "    # compute the start and end-date\n",
      "    dif_date_anra = sta_date_ansi - sta_date_rasd\n",
      "    dif_date_rasd = end_date_rasd - sta_date_rasd + 1\n",
      "    end_date_anra = dif_date_rasd * timestep_date + sta_date_rasd + dif_date_anra\n",
      "\n",
      "    sd = convert_ansi_date(sta_date_ansi) # (2012, 1, 2, 0.5)\n",
      "    ed = convert_ansi_date(end_date_anra) # (2014, 1, 7, 0.5)\n",
      "\n",
      "    # convert dates to pandas date_range\n",
      "    str_date = str(sd[1])+'.'+str(sd[2])+'.'+str(sd[0])+'.'+str(int(np.round(sd[3]*24)))+':00'\n",
      "    end_date = str(ed[1])+'.'+str(ed[2])+'.'+str(ed[0])+'.'+str(int(np.round(ed[3]*24)))+':00'\n",
      "    freq_date = str(int(timestep_date))+'D'\n",
      "    dates = pd.date_range(str_date,end_date, freq=freq_date)\n",
      "    dates = dates[:-1]\n",
      "\n",
      "    logging.info('dates converted from ANSI to ISO 8601')    \n",
      "    \n",
      "    # read data block of range set\n",
      "    xml_ts = StringIO(root[2][0][1].text)\n",
      "\n",
      "    # load data block as numpy array\n",
      "    ts = np.loadtxt(xml_ts, dtype='float', delimiter=',')\n",
      "    ts_reshape = ts.reshape((easting*northing,time)) #Easting = ts_shape[0], Northing = ts_shape[1], time = ts_shape[2]\n",
      "    \n",
      "    # compute mean so regional statistics can be computed as well\n",
      "    ts_mean = ts_reshape.mean(axis=0)\n",
      "    ndvi = pd.Series(ts_mean.flatten()/10000.,dates, name='nvdi')\n",
      "\n",
      "    # Interpolate NDVI 16 day interval to 1 day interval using linear interpolation\n",
      "    x = pd.date_range(str_date,end_date,freq='D')\n",
      "    ndvi_int = ndvi.reindex(x)\n",
      "    #ndvi_int = ndvi_int.fillna(method='pad')\n",
      "    ndvi_int = ndvi_int.interpolate(method='linear')\n",
      "    ndvi_int.name = 'ndvi_int'\n",
      "    \n",
      "    \n",
      "    if pyhants == 0:\n",
      "\n",
      "        #Compute NVAI using 16 day interval NDVI data        \n",
      "        #nvai = ndvi.groupby([ndvi.index.month]).apply(lambda g: (g - g.mean())/(g.max() - g.min()))\n",
      "        #nvai.name = 'nvai'\n",
      "        \n",
      "        # Compute NVAI using daily interpolated NDVI data\n",
      "        nvai = ndvi_int.groupby([ndvi_int.index.month,ndvi_int.index.day]).apply(lambda g: (g - g.mean())/(g.max() - g.min()))\n",
      "        nvai.name = 'nvai'  \n",
      "        nvai_sel = nvai.ix[from_date:to_date]        \n",
      "    \n",
      "    elif pyhants == 1:\n",
      "        # Compute PyHANTS first and then calculate mean\n",
      "        frequencies = len(ndvi_int) / 365 * ann_freq\n",
      "        outliers = outliers_rj\n",
      "        pyhants = HANTS(sample_count=time, inputs=ts_reshape/100, frequencies_considered_count=frequencies,  outliers_to_reject=outliers)\n",
      "        pyhants *= 100\n",
      "        pyhants_mean = pyhants.mean(axis=0)\n",
      "        ndvi_pyhants = pd.Series(pyhants_mean.flatten()/10000.,dates, name='ndvi_pyhants')        \n",
      "        \n",
      "        # interpolate reconstructed NDVI to daily values\n",
      "        x = pd.date_range(str_date,end_date,freq='D')\n",
      "        ndvi_pyhants_int = ndvi_pyhants.reindex(x)\n",
      "        ndvi_pyhants_int = ndvi_pyhants_int.interpolate(method='linear')\n",
      "        ndvi_pyhants_int.name = 'ndvi_pyhants_int'     \n",
      "        \n",
      "        # Compute NVAI using daily interpolated PyHANTS reconstructed NDVI data\n",
      "        nvai = ndvi_pyhants_int.groupby([ndvi_pyhants_int.index.month,ndvi_pyhants_int.index.day]).apply(lambda g: (g - g.mean())/(g.max() - g.min()))\n",
      "        nvai.name = 'nvai'\n",
      "        nvai_sel = nvai.ix[from_date:to_date]\n",
      "    \n",
      "    # data preparation for HighCharts: Output need to be in JSON format with time \n",
      "    # in Unix milliseconds\n",
      "    dthandler = lambda obj: (\n",
      "    unix_time_millis(obj)\n",
      "    if isinstance(obj, datetime.datetime)\n",
      "    or isinstance(obj, datetime.date)\n",
      "    else None)\n",
      "\n",
      "    nvai_json = StringIO()\n",
      "\n",
      "    logging.info('ready to dump files to JSON')\n",
      "    # np.savetxt(output, pyhants, delimiter=',')\n",
      "    out1 = json.dump(nvai_sel.reset_index().as_matrix().tolist(), nvai_json, default=dthandler)\n",
      "\n",
      "    logging.info('dates converted from ISO 8601 to UNIX in ms')             \n",
      "    \n",
      "    return nvai_json #nvai_sel"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Process(WPSProcess):\n",
      "\n",
      "\n",
      "    def __init__(self):\n",
      "\n",
      "        ##\n",
      "        # Process initialization\n",
      "        WPSProcess.__init__(self,\n",
      "            identifier = \"WPS_NVAI_DI_CAL_TS\",\n",
      "            title=\"Compute PAP TIMESERIES\",\n",
      "            abstract=\"\"\"Module to compute PAP TimeSeries based on GPCP data\"\"\",\n",
      "            version = \"1.0\",\n",
      "            storeSupported = True,\n",
      "            statusSupported = True)\n",
      "\n",
      "        ##\n",
      "        # Adding process inputs\n",
      "        self.lonIn = self.addLiteralInput(identifier=\"lon_center\",\n",
      "                    title=\"Longitude\",\n",
      "                    type=type(''))\n",
      "\n",
      "        self.latIn = self.addLiteralInput(identifier=\"lat_center\",\n",
      "                    title=\"Latitude\",\n",
      "                    type=type(''))\n",
      "\n",
      "        self.fromDateIn = self.addLiteralInput(identifier=\"from_date\",\n",
      "                    title = \"The start date to be calcualted\",\n",
      "                                          type=type(''))\n",
      "\n",
      "        self.toDateIn = self.addLiteralInput(identifier=\"to_date\",\n",
      "                    title = \"The final date to be calcualted\",\n",
      "                                          type=type(''))   \n",
      "        \n",
      "        self.pixOff = self.addLiteralInput(identifier=\"pix_offset\",\n",
      "                    title=\"pixel offset, 0, 4 or 9\",\n",
      "                    type=type(''))\n",
      "\n",
      "        self.pyHants = self.addLiteralInput(identifier=\"pyhants\",\n",
      "                    title=\"exclude pyhants (0) or include pyhants (1)\",\n",
      "                    type=type(''))\n",
      "\n",
      "        self.freqan = self.addLiteralInput(identifier=\"ann_freq\",\n",
      "                    title = \"number of annual (!) frequencies\",\n",
      "                                          type=type(''))\n",
      "\n",
      "        self.outlier = self.addLiteralInput(identifier=\"outliers_rj\",\n",
      "                    title = \"what type of outliers to reject, Hi, Lo, or None\",\n",
      "                                          type=type(''))        \n",
      "        \n",
      "\n",
      "        ##\n",
      "        # Adding process outputs\n",
      "\n",
      "        self.nvaiOut = self.addComplexOutput(identifier  = \"nvai_ts\", \n",
      "                                        title       = \"NVAI Timeseries\",\n",
      "                                        formats     = [{'mimeType':'text/xml'}]) #xml/application ||application/json   \n",
      "\n",
      "\n",
      "    ##\n",
      "    # Execution part of the process\n",
      "    def execute(self):\n",
      "        # Load the data\n",
      "        lon_center = float(self.lonIn.getValue())\n",
      "        lat_center = float(self.latIn.getValue())\n",
      "        pix_offset = float(self.pixOff.getValue())                \n",
      "        \n",
      "        from_date = str(self.fromDateIn.getValue())\n",
      "        to_date = str(self.toDateIn.getValue())\n",
      "        \n",
      "        pyhants = float(self.pyHants.getValue())\n",
      "        ann_freq = float(self.freqan.getValue())\n",
      "        outliers_rj = str(self.outlier.getValue())        \n",
      "        \n",
      "        # Do the Work\n",
      "        NVAI_OUT = COMPUTE_NVAI(lon_center=lon_center, lat_center=lat_center, pix_offset=pix_offset, \n",
      "                         from_date=from_date, to_date=to_date, pyhants=pyhants, \n",
      "                         ann_freq=ann_freq, outliers_rj=outliers_rj)\n",
      "        \n",
      "        # Save to out\n",
      "        self.nvaiOut.setValue( NVAI_OUT )\n",
      "        return"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_ = COMPUTE_NVAI(lon_center=5.6943, lat_center=53.1533, pix_offset=9, \n",
      "                 from_date='2010-01-01', to_date='2012-01-01', pyhants=1, \n",
      "                 ann_freq=6, outliers_rj='None')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from_date='2010-01-01'\n",
      "frm_year = int(from_date[0:4])\n",
      "frm_month = int(from_date[6:8])\n",
      "frm_day = int(from_date[6:8])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frm = datetime.datetime(2010,1,1)\n",
      "to = datetime.datetime(2012,1,1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_.ix['2010-01-01':'2012-01-01']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "year = ndvi.index.year\n",
      "sel = ((2009 <= year) & (year <= 2011))\n",
      "x_sel_min = ndvi.index[sel][0]\n",
      "x_sel_max = ndvi.index[sel][-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Long1 = str(lon_center + regionpix[0])\n",
      "Long2 = str(lon_center + regionpix[1])\n",
      "Lat1 = str(lat_center + regionpix[0])\n",
      "Lat2 = str(lat_center + regionpix[1])\n",
      "in_date_str = '2010-01-01' \n",
      "in_date_end = '2012-01-01'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "full_url = \"http://159.226.117.95:8080/rasdaman/ows/wcs2?service=WCS&version=2.0.1&request=GetCoverage&coverageId=modis_13c1_cov&subset=Long(\"+Long1+\",\"+Long2+\")&subset=Lat(\"+Lat1+\",\"+Lat2+\")\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = urllib2.urlopen(full_url)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "root = etree.fromstring(f.read())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read grid envelope of domain set\n",
      "xml_low_env = StringIO(root[1][0][0][0][0].text)\n",
      "xml_high_env = StringIO(root[1][0][0][0][1].text)\n",
      "\n",
      "# load grid envelope as numpy array\n",
      "low_env = np.loadtxt(xml_low_env, dtype='int', delimiter=' ')\n",
      "high_env = np.loadtxt(xml_high_env, dtype='int', delimiter=' ')\n",
      "ts_shape = high_env - low_env + 1\n",
      "\n",
      "easting = ts_shape[0]\n",
      "northing = ts_shape[1]\n",
      "time = ts_shape[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## extract the dates\n",
      "#sd = ansi_date_to_greg_date(low_env[2]+140734)\n",
      "#ed = ansi_date_to_greg_date(high_env[2]+140734)\n",
      "\n",
      "# extract the values we need from the parsed XML\n",
      "sta_date_ansi = np.loadtxt(StringIO(root[0][0][0].text))[2] # 150116\n",
      "end_date_ansi = np.loadtxt(StringIO(root[0][0][1].text))[2] # 150852\n",
      "sta_date_rasd = np.loadtxt(StringIO(root[1][0][0][0][0].text))[2] # 9382\n",
      "end_date_rasd = np.loadtxt(StringIO(root[1][0][0][0][1].text))[2] # 9427\n",
      "timestep_date = np.loadtxt(StringIO(root[1][0][5].text))[2] # 16\n",
      "\n",
      "# compute the start and end-date\n",
      "dif_date_anra = sta_date_ansi - sta_date_rasd\n",
      "dif_date_rasd = end_date_rasd - sta_date_rasd + 1\n",
      "end_date_anra = dif_date_rasd * timestep_date + sta_date_rasd + dif_date_anra\n",
      "\n",
      "sd = convert_ansi_date(sta_date_ansi) # (2012, 1, 2, 0.5)\n",
      "ed = convert_ansi_date(end_date_anra) # (2014, 1, 7, 0.5)\n",
      "\n",
      "# convert dates to pandas date_range\n",
      "str_date = str(sd[1])+'.'+str(sd[2])+'.'+str(sd[0])+'.'+str(int(np.round(sd[3]*24)))+':00'\n",
      "end_date = str(ed[1])+'.'+str(ed[2])+'.'+str(ed[0])+'.'+str(int(np.round(ed[3]*24)))+':00'\n",
      "freq_date = str(int(timestep_date))+'D'\n",
      "dates = pd.date_range(str_date,end_date, freq=freq_date)\n",
      "dates = dates[:-1]\n",
      "\n",
      "logging.info('dates converted from ANSI to ISO 8601')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read data block of range set\n",
      "xml_ts = StringIO(root[2][0][1].text)\n",
      "\n",
      "# load data block as numpy array\n",
      "ts = np.loadtxt(xml_ts, dtype='float', delimiter=',')\n",
      "ts_reshape = ts.reshape((easting*northing,time)) #Easting = ts_shape[0], Northing = ts_shape[1], time = ts_shape[2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute mean so regional statistics can be computed as well\n",
      "ts_mean = ts_reshape.mean(axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ndvi = pd.Series(ts_mean.flatten()/10000.,dates, name='nvdi')\n",
      "year = ndvi.index.year\n",
      "sel = ((2009 <= year) & (year <= 2011))\n",
      "x_sel_min = ndvi.index[sel][0]\n",
      "x_sel_max = ndvi.index[sel][-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualise normal NDVI 16 day interval"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Selector close-up for the year 2009-2011 for a detailed view"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "matplotlib.pylab.rcParams['figure.figsize'] = (18.0, 5.0)\n",
      "ndvi.plot(legend=True, title=ndvi.name)\n",
      "ax = ndvi.plot(legend=True, title=ndvi.name, style='.')\n",
      "ymin, ymax = ax.get_ylim()\n",
      "ax.axvspan(x_sel_min,x_sel_max, color='gray', alpha=0.2)\n",
      "\n",
      "plt.show()\n",
      "\n",
      "ndvi[sel].plot(legend=True, title=ndvi.name)\n",
      "ndvi[sel].plot(legend=True, title=ndvi.name, style='.')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute and visualise NVAI using 16 day interval NDVI data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nvai = ndvi.groupby([ndvi.index.month]).apply(lambda g: (g - g.mean())/(g.max() - g.min()))\n",
      "nvai.name = 'nvai'\n",
      "nvai.plot(legend=True, title=nvai.name)\n",
      "ax = nvai.plot(legend=True, title=nvai.name, style='.')\n",
      "ymin, ymax = ax.get_ylim()\n",
      "ax.axvspan(x_sel_min,x_sel_max, color='gray', alpha=0.2)\n",
      "\n",
      "plt.show()\n",
      "nvai[sel].plot(legend=True, title=nvai.name)\n",
      "nvai[sel].plot(legend=True, title=nvai.name, style='.')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Interpolate NDVI 16 day interval to 1 day interval using linear interpolation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = pd.date_range(str_date,end_date,freq='D')\n",
      "ndvi_int = ndvi.reindex(x)\n",
      "#ndvi_int = ndvi_int.fillna(method='pad')\n",
      "ndvi_int = ndvi_int.interpolate(method='linear')\n",
      "ndvi_int.name = 'ndvi_int'\n",
      "\n",
      "year2 = ndvi_int.index.year\n",
      "sel2= ((2009 <= year2) & (year2 <= 2011))\n",
      "x_sel2_min = ndvi_int.index[sel2][0]\n",
      "x_sel2_max = ndvi_int.index[sel2][-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ndvi_int.plot(legend=True, title=ndvi_int.name)\n",
      "ax = ndvi_int.plot(legend=True, title=ndvi_int.name, style='.')\n",
      "ymin, ymax = ax.get_ylim()\n",
      "ax.axvspan(x_sel_min,x_sel_max, color='gray', alpha=0.2)\n",
      "plt.show()\n",
      "\n",
      "ndvi_int[sel2].plot(legend=True, title=ndvi_int.name)\n",
      "ndvi_int[sel2].plot(legend=True, title=ndvi_int.name, style='.')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute and visualise NVAI using 1 day interpolated NDVI data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nvai_int = ndvi_int.groupby([ndvi_int.index.month,ndvi_int.index.day]).apply(lambda g: (g - g.mean())/(g.max() - g.min()))\n",
      "nvai_int.name = 'nvai_int'\n",
      "nvai_int.plot(legend=True, title=nvai_int.name)\n",
      "ax = nvai_int.plot(legend=True, title=nvai_int.name, style='.')\n",
      "ymin, ymax = ax.get_ylim()\n",
      "ax.axvspan(x_sel_min,x_sel_max, color='gray', alpha=0.2)\n",
      "plt.show()\n",
      "\n",
      "nvai_int[sel2].plot(legend=True, title=nvai_int.name)\n",
      "nvai_int[sel2].plot(legend=True, title=nvai_int.name, style='.')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define parameters PyHANTS, settings are set that Low values are seen as outliers and we use 6 frequencies / year"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "frequencies = len(ndvi_int) / 365 * 6\n",
      "outliers = \"Lo\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Apply PyHANTS reconstruction on the original 16 day interval NDVI data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pyhants = HANTS(sample_count=time, inputs=ts_reshape/100, frequencies_considered_count=frequencies,  outliers_to_reject=outliers)\n",
      "pyhants *= 100\n",
      "pyhants_mean = pyhants.mean(axis=0)\n",
      "ndvi_pyhants = pd.Series(pyhants_mean.flatten()/10000.,dates, name='ndvi_pyhants')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ndvi_pyhants.plot(legend=True, title=ndvi_pyhants.name)\n",
      "ax = ndvi_pyhants.plot(legend=True, title=ndvi_pyhants.name, style='.')\n",
      "ymin, ymax = ax.get_ylim()\n",
      "ax.axvspan(x_sel_min,x_sel_max, color='gray', alpha=0.2)\n",
      "plt.show()\n",
      "\n",
      "ndvi_pyhants[sel].plot(legend=True, title=ndvi_pyhants.name)\n",
      "ndvi_pyhants[sel].plot(legend=True, title=ndvi_pyhants.name, style='.')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualise normal NDVI 16 day interval AND PyHANTS reconstructed NDVI 16 day interval"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ndvi_comb = pd.concat([ndvi, ndvi_pyhants],axis=1)\n",
      "ax = ndvi_comb.plot(legend=True, title='ndvi_comb of original and reconstructed NDVI')\n",
      "ymin, ymax = ax.get_ylim()\n",
      "ax.axvspan(x_sel_min,x_sel_max, color='gray', alpha=0.2)\n",
      "plt.show()\n",
      "\n",
      "ndvi_comb[sel].plot(legend=True, title='ndvi_comb of original and reconstructed NDVI')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Interpolate PyHANTS reconstructed NDVI 16 day interval to 1 day interval using linear interpolation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = pd.date_range(str_date,end_date,freq='D')\n",
      "ndvi_pyhants_int = ndvi_pyhants.reindex(x)\n",
      "ndvi_pyhants_int = ndvi_pyhants_int.interpolate(method='linear')\n",
      "ndvi_pyhants_int.name = 'ndvi_pyhants_int'\n",
      "ndvi_pyhants_int.plot(legend=True, title=ndvi_pyhants_int.name)\n",
      "ax = ndvi_pyhants_int.plot(legend=True, title=ndvi_pyhants_int.name, style=',')\n",
      "ymin, ymax = ax.get_ylim()\n",
      "ax.axvspan(x_sel_min,x_sel_max, color='gray', alpha=0.2)\n",
      "plt.show()\n",
      "\n",
      "ndvi_pyhants_int[sel2].plot(legend=True, title=ndvi_pyhants_int.name)\n",
      "ndvi_pyhants_int[sel2].plot(legend=True, title=ndvi_pyhants_int.name, style=',')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute and visualise NVAI using 1 day interpolated PyHANTS reconstructed NDVI data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nvai_pyhants_int = ndvi_pyhants_int.groupby([ndvi_pyhants_int.index.month,ndvi_pyhants_int.index.day]).apply(lambda g: (g - g.mean())/(g.max() - g.min()))\n",
      "nvai_pyhants_int.name = 'nvai_pyhants_int'\n",
      "nvai_pyhants_int.plot(legend=True, title=nvai_pyhants_int.name)\n",
      "ax = nvai_pyhants_int.plot(legend=True, title=nvai_pyhants_int.name, style=',')\n",
      "ymin, ymax = ax.get_ylim()\n",
      "ax.axvspan(x_sel_min,x_sel_max, color='gray', alpha=0.2)\n",
      "plt.show()\n",
      "\n",
      "nvai_pyhants_int[sel2].plot(legend=True, title=nvai_pyhants_int.name)\n",
      "nvai_pyhants_int[sel2].plot(legend=True, title=nvai_pyhants_int.name, style=',')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualise normal NDVI 16 day interval AND PyHANTS reconstructed NDVI 16 day interval"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nvai_comb = pd.concat([nvai_int, nvai_pyhants_int], axis=1)\n",
      "ax = nvai_comb.plot(legend=True, title='nvai_comb')\n",
      "ymin, ymax = ax.get_ylim()\n",
      "ax.axvspan(x_sel_min,x_sel_max, color='gray', alpha=0.2)\n",
      "plt.show()\n",
      "\n",
      "nvai_comb[sel2].plot(legend=True, title='nvai_comb')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualise the difference between NVAI using original NDVI data and PyHANTS reconstructed NDVI data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nvai_comb_dif = nvai_comb.nvai_int - nvai_comb.nvai_pyhants_int\n",
      "nvai_comb_dif.name = 'nvai_diff'\n",
      "ax = nvai_comb_dif.plot(legend=True, title=nvai_comb_dif.name, kind='area', stacked=False)\n",
      "ymin, ymax = ax.get_ylim()\n",
      "ax.axvspan(x_sel_min,x_sel_max, color='gray', alpha=0.2)\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "\n",
      "nvai_comb_dif[sel2].plot(legend=True, title=nvai_comb_dif.name, kind='area', stacked=False)\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compute and visualise the standard deviation of the interpolated NVAI using PyHANTS reconstructed NDVI data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nvai_pyhants_int_std_low = nvai_pyhants_int.groupby([nvai_pyhants_int.index.month,nvai_pyhants_int.index.day]).apply(lambda g: ((g - g.std())))\n",
      "nvai_pyhants_int_std_upp = nvai_pyhants_int.groupby([nvai_pyhants_int.index.month,nvai_pyhants_int.index.day]).apply(lambda g: ((g + g.std())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(1, 1, sharex=True)\n",
      "ax.plot(nvai_pyhants_int.index, nvai_pyhants_int, color='g')\n",
      "ax.fill_between(nvai_pyhants_int.index, nvai_pyhants_int, nvai_pyhants_int_std_upp,facecolor='g', alpha=0.2)\n",
      "ax.fill_between(nvai_pyhants_int.index, nvai_pyhants_int, nvai_pyhants_int_std_low,facecolor='g', alpha=0.2)\n",
      "ymin, ymax = ax.get_ylim()\n",
      "ax.axvspan(x_sel_min,x_sel_max, color='gray', alpha=0.2)\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "fig, ax = plt.subplots(1, 1, sharex=True)\n",
      "ax.plot(nvai_pyhants_int.index[sel2], nvai_pyhants_int[sel2], color='g')\n",
      "ax.fill_between(nvai_pyhants_int.index[sel2], nvai_pyhants_int[sel2], nvai_pyhants_int_std_upp[sel2],facecolor='g', alpha=0.2)\n",
      "ax.fill_between(nvai_pyhants_int.index[sel2], nvai_pyhants_int[sel2], nvai_pyhants_int_std_low[sel2],facecolor='g', alpha=0.2)\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compare with the computed NVAI using raw NDVI data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(1, 1, sharex=True)\n",
      "ax.plot(nvai_int.index, nvai_int, color='r', lw=1)\n",
      "ax.plot(nvai_pyhants_int.index, nvai_pyhants_int, color='g', alpha=0.2)\n",
      "ax.fill_between(nvai_pyhants_int.index, nvai_pyhants_int, nvai_pyhants_int_std_upp,facecolor='g', alpha=0.2)\n",
      "ax.fill_between(nvai_pyhants_int.index, nvai_pyhants_int, nvai_pyhants_int_std_low,facecolor='g', alpha=0.2)\n",
      "ymin, ymax = ax.get_ylim()\n",
      "ax.axvspan(x_sel_min,x_sel_max, color='gray', alpha=0.2)\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "fig, ax = plt.subplots(1, 1, sharex=True)\n",
      "ax.plot(nvai_int.index[sel2], nvai_int[sel2], color='r', lw=1)\n",
      "ax.plot(nvai_pyhants_int.index[sel2], nvai_pyhants_int[sel2], color='g', alpha=0.2)\n",
      "ax.fill_between(nvai_pyhants_int.index[sel2], nvai_pyhants_int[sel2], nvai_pyhants_int_std_upp[sel2],facecolor='g', alpha=0.2)\n",
      "ax.fill_between(nvai_pyhants_int.index[sel2], nvai_pyhants_int[sel2], nvai_pyhants_int_std_low[sel2],facecolor='g', alpha=0.2)\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = nvai_pyhants_int_std_upp > nvai_int \n",
      "y = nvai_pyhants_int_std_low < nvai_int\n",
      "z = x==y\n",
      "q = np.where(z == True)\n",
      "z = np.invert(z)\n",
      "z.ix[q] = np.nan\n",
      "r = z * nvai_int"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig, ax = plt.subplots(1, 1, sharex=True)\n",
      "ax.plot(nvai_pyhants_int.index, nvai_pyhants_int, color='g', alpha=0.5)\n",
      "ax.plot(nvai_int.index, nvai_int, color='r')\n",
      "ax.fill_between(nvai_pyhants_int.index, nvai_pyhants_int, nvai_pyhants_int_std_upp,facecolor='g', alpha=0.2)\n",
      "ax.fill_between(nvai_pyhants_int.index, nvai_pyhants_int, nvai_pyhants_int_std_low,facecolor='g', alpha=0.2)\n",
      "plt.plot(r.index, r, color='m', linestyle='-', lw=3)\n",
      "plt.title( 'NVAI using raw NDVI data is for: ' + str(np.round(100 - (float(r.isnull().sum()) / float(len(r))) * 100.,3)) +'% outside PyHANTS reconstructed NVAI +/- 1 standard deviation')\n",
      "ymin, ymax = ax.get_ylim()\n",
      "ax.axvspan(x_sel_min,x_sel_max, color='gray', alpha=0.2)\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "fig, ax = plt.subplots(1, 1, sharex=True)\n",
      "ax.plot(nvai_pyhants_int.index[sel2], nvai_pyhants_int[sel2], color='g', alpha=0.5)\n",
      "ax.plot(nvai_int.index[sel2], nvai_int[sel2], color='r')\n",
      "ax.fill_between(nvai_pyhants_int.index[sel2], nvai_pyhants_int[sel2], nvai_pyhants_int_std_upp[sel2],facecolor='g', alpha=0.2)\n",
      "ax.fill_between(nvai_pyhants_int.index[sel2], nvai_pyhants_int[sel2], nvai_pyhants_int_std_low[sel2],facecolor='g', alpha=0.2)\n",
      "plt.plot(r.index[sel2], r[sel2], color='m', linestyle='-', lw=3)\n",
      "plt.title( 'NVAI using raw NDVI data is for: ' + str(np.round(100 - (float(r.isnull().sum()) / float(len(r))) * 100.,3)) +'% outside PyHANTS reconstructed NVAI +/- 1 standard deviation')\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Applying some statistics on the original derived NVAI and PyHANTS reconstructed derived NVAI"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.stats as stats\n",
      "import statsmodels.api as sm\n",
      "import statsmodels.formula.api as smf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = smf.ols('nvai_pyhants_int ~ nvai_int', nvai_comb).fit()\n",
      "print results.summary()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "intercept, slope = results.params\n",
      "r2 = results.rsquared\n",
      "pearsson = nvai_comb.nvai_int.corr(nvai_comb.nvai_pyhants_int)\n",
      "print slope, intercept, r2, pearsson"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_lim = [min(nvai_comb.nvai_int),max(nvai_comb.nvai_int)]\n",
      "y_lim = [min(nvai_comb.nvai_pyhants_int),max(nvai_comb.nvai_pyhants_int)]\n",
      "\n",
      "matplotlib.pylab.rcParams['figure.figsize'] = (10, 10.0)\n",
      "f = plt.figure()\n",
      "ax = f.add_subplot(111)\n",
      "ax.plot(nvai_comb.nvai_int,nvai_comb.nvai_pyhants_int, marker='.', ls='', c='c')\n",
      "ax.vlines(0,min(x_lim, y_lim)[0],max(x_lim, y_lim)[1])\n",
      "ax.hlines(0,min(x_lim, y_lim)[0],max(x_lim, y_lim)[1])\n",
      "ax.set_xlim(min(x_lim, y_lim)[0],max(x_lim, y_lim)[1])\n",
      "ax.set_ylim(min(x_lim, y_lim)[0],max(x_lim, y_lim)[1])\n",
      "ax.set_xlabel('NVAI (original NDVI)')\n",
      "ax.set_ylabel('NVAI (PyHANTS reconstructed NDVI)')\n",
      "ax.hold(True)\n",
      "x = np.array([min(x_lim, y_lim)[0],max(x_lim, y_lim)[1]])\n",
      "y = intercept + slope * x\n",
      "ax.plot(x, y, 'm-', lw=2)\n",
      "ax.set_title('Correlation between NVAI (original NDVI) vs NVAI (PyHANTS reconstructed NDVI)')\n",
      "ax.text(0.1, 0.95,'$r^2$ = '+str(np.round(r2,3)), ha='center', va='center', transform=ax.transAxes, size=14)\n",
      "ax.text(0.14, 0.90,'Pearsson = '+str(np.round(pearsson,3)), ha='center', va='center', transform=ax.transAxes, size=14)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "final combination plot"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ndvi_comb[sel].plot(legend=True, title='ndvi_comb of original and reconstructed NDVI', color=['r','g'])\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "nvai_comb[sel2].plot(legend=True, title='nvai_comb', color=['r','g'])\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "fig, ax = plt.subplots(1, 1, sharex=True)\n",
      "l2 = ax.plot(nvai_pyhants_int.index[sel2], nvai_pyhants_int[sel2], color='g', alpha=0.5, label='nvai_pyhants_int')\n",
      "l4 = ax.plot([], [], color='g', alpha=0.2, linewidth=10, label='+/- 1std')\n",
      "l3 = ax.plot(nvai_int.index[sel2], nvai_int[sel2], color='r', label='nvai_int')\n",
      "l1 = ax.plot(r.index[sel2], r[sel2], color='m', linestyle='-', lw=3, label='nvai_int out std')\n",
      "ax.fill_between(nvai_pyhants_int.index[sel2], nvai_pyhants_int[sel2], nvai_pyhants_int_std_upp[sel2],facecolor='g', alpha=0.2)\n",
      "ax.fill_between(nvai_pyhants_int.index[sel2], nvai_pyhants_int[sel2], nvai_pyhants_int_std_low[sel2],facecolor='g', alpha=0.2)\n",
      "\n",
      "\n",
      "#p2 = plt.Rectangle((0, 0), 1, 1, fc=\"red\")\n",
      "\n",
      "ax.legend()\n",
      "#ax.legend(p2, a2_label)\n",
      "\n",
      "\n",
      "plt.title( 'NVAI using raw NDVI data is for: ' + str(np.round(100 - (float(r.isnull().sum()) / float(len(r))) * 100.,3)) +'% outside PyHANTS reconstructed NVAI +/- 1 standard deviation')\n",
      "plt.tight_layout()\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}