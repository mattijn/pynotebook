{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get backend before set\n",
      "D:\\Python27x64\\lib\\site-packages\\matplotlib\\mpl-data\\matplotlibrc\n",
      "TkAgg\n",
      "D:\\Python27x64\\lib\\site-packages\\matplotlib\\mpl-data\\matplotlibrc\n",
      "TkAgg\n"
     ]
    }
   ],
   "source": [
    "from osgeo import gdal, ogr, osr\n",
    "from osgeo.gdalconst import *\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "#logging.info('get backend before set')\n",
    "print('get backend before set')\n",
    "#logging.info(matplotlib.matplotlib_fname())\n",
    "print(matplotlib.matplotlib_fname())\n",
    "#logging.info(matplotlib.get_backend())\n",
    "print(matplotlib.get_backend())\n",
    "#matplotlib.rcParams['backend'] = 'AGG'\n",
    "#matplotlib.use('AGG')\n",
    "print(matplotlib.matplotlib_fname())\n",
    "print(matplotlib.get_backend())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "gdal.PushErrorHandler('CPLQuietErrorHandler')\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cStringIO\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.mpl.gridliner import LATITUDE_FORMATTER, LONGITUDE_FORMATTER\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib import gridspec\n",
    "from cartopy.io import shapereader\n",
    "import shapely.geometry as sgeom\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib\n",
    "#%matplotlib inline\n",
    "import datetime\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from pywps.Process import WPSProcess \n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "from osgeo import gdal\n",
    "import numpy\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from lxml import etree\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datelist_regular_coverage(root, start_date, start, cur_date):\n",
    "    \"\"\"\n",
    "    retrieve regular datelist and requested current position in regards to total no. of observations\n",
    "    \"\"\"\n",
    "\n",
    "    #print start\n",
    "    tmp_date=datetime(start.year,cur_date.month,cur_date.day)\n",
    "    if tmp_date > start :\n",
    "        start=(tmp_date-datetime(1601,1,1)).days\n",
    "    else: start=(datetime(start.year+1,cur_date.month,cur_date.day)-datetime(1601,1,1)).days\n",
    "    datelist=range(start+1,end_date-1,365)\n",
    "    print datelist\n",
    "\n",
    "    #find the position of the requested date in the datelist\n",
    "    cur_epoch=(cur_date-datetime(1601,1,1)).days\n",
    "    cur_pos=min(range(len(datelist)),key=lambda x:abs(datelist[x]-cur_epoch))\n",
    "    print ('Current position:',cur_pos)    \n",
    "    \n",
    "    return datelist, cur_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datelist_irregular_coverage(root, start_date, start, cur_date):\n",
    "    \"\"\"\n",
    "    retrieve irregular datelist and requested current position in regards to total no. of observations\n",
    "    \"\"\"\n",
    "    \n",
    "    #root[0]                - wcs:CoverageDescription\n",
    "    #root[0][0]             - boundedBy \n",
    "    #root[0][0][0]          - Envelope\n",
    "    #root[0][0][0][0]       - lowerCorner\n",
    "    # --- \n",
    "    #root[0]                - wcs:CoverageDescription\n",
    "    #root[0][3]             - domainSet\n",
    "    #root[0][3][0]          - gmlrgrid:ReferenceableGridByVectors\n",
    "    #root[0][3][0][5]       - gmlrgrid:generalGridAxis\n",
    "    #root[0][3][0][5][0]    - gmlrgrid:GeneralGridAxis\n",
    "    #root[0][3][0][5][0][1] - gmlrgrid:coefficients\n",
    "\n",
    "    # get sample size coefficients from XML root\n",
    "    sample_size = root[0][3][0][5][0][1].text #sample size\n",
    "    #print root[0][3][0][5][0][1].text #sample size\n",
    "    \n",
    "    # use coverage start_date and sample_size array to create all dates in ANSI\n",
    "    array_stepsize = np.fromstring(sample_size, dtype=int, sep=' ')\n",
    "    #print np.fromstring(sample_size, dtype=int, sep=' ')\n",
    "    array_all_ansi = array_stepsize + start_date   \n",
    "    \n",
    "    # create array of all dates in ISO\n",
    "    list_all_dates = []\n",
    "    for stepsize in array_stepsize:\n",
    "        date_and_stepsize = start + timedelta(stepsize - 1)\n",
    "        list_all_dates.append(date_and_stepsize)\n",
    "        #print date_and_stepsize\n",
    "    array_all_dates = np.array(list_all_dates)  \n",
    "    \n",
    "    # create array of all dates as DOY\n",
    "    list_all_yday = []\n",
    "    for j in array_all_dates:\n",
    "        yday = j.timetuple().tm_yday\n",
    "        list_all_yday.append(yday)\n",
    "        #print yday\n",
    "    array_all_yday = np.array(list_all_yday)    \n",
    "    \n",
    "    # subtract user date of all dates in ISO \n",
    "    # to find the nearest available coverage date\n",
    "    array_diff_dates = array_all_dates - cur_date\n",
    "    idx_nearest_date = find_nearest(array_diff_dates, timedelta(0))\n",
    "    nearest_date = array_all_dates[idx_nearest_date]    \n",
    "    \n",
    "    # select all coresponding DOY of all years for ANSI and ISO dates\n",
    "    array_selected_ansi = array_all_ansi[array_all_yday == nearest_date.timetuple().tm_yday]\n",
    "    array_selected_dates = array_all_dates[array_all_yday == nearest_date.timetuple().tm_yday]\n",
    "    print array_selected_ansi\n",
    "    \n",
    "    # get index of nearest date in selection array\n",
    "    idx_nearest_date_selected = numpy.where(array_selected_dates==nearest_date)[0][0]  \n",
    "    print idx_nearest_date_selected\n",
    "    \n",
    "    # return datelist in ANSI and the index of the nearest date\n",
    "    return array_selected_ansi, idx_nearest_date_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_nearest(array,value):\n",
    "    return (np.abs(array-value)).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _NVAI_CAL(date,spl_arr):\n",
    "\n",
    "    ##request image cube for the specified date and area by WCS.\n",
    "    #firstly we get the temporal length of avaliable NDVI data from the DescribeCoverage of WCS\n",
    "    endpoint='http://192.168.1.104:8080/rasdaman/ows'\n",
    "    field={}\n",
    "    field['SERVICE']='WCS'\n",
    "    field['VERSION']='2.0.1'\n",
    "    field['REQUEST']='DescribeCoverage'\n",
    "    field['COVERAGEID']='NDVI_MOD13C1005_uptodate'#'NDVI_MOD13C1005'#'trmm_3b42_coverage_1'\n",
    "    url_values = urllib.urlencode(field,doseq=True)\n",
    "    full_url = endpoint + '?' + url_values\n",
    "    data = urllib.urlopen(full_url).read()\n",
    "    root = etree.fromstring(data)\n",
    "    lc = root.find(\".//{http://www.opengis.net/gml/3.2}lowerCorner\").text\n",
    "    uc = root.find(\".//{http://www.opengis.net/gml/3.2}upperCorner\").text\n",
    "    start_date=int((lc.split(' '))[2])\n",
    "    end_date=int((uc.split(' '))[2])\n",
    "    #print [start_date, end_date]\n",
    "\n",
    "    #generate the dates list \n",
    "    cur_date=datetime.strptime(date,\"%Y-%m-%d\")\n",
    "    #startt=145775\n",
    "    start=datetime.fromtimestamp((start_date-(datetime(1970,01,01)-datetime(1601,01,01)).days)*24*60*60)\n",
    "    #print start\n",
    "\n",
    "    #tmp_date=datetime(start.year,cur_date.month,cur_date.day)\n",
    "    #if tmp_date > start :\n",
    "    #    start=(tmp_date-datetime(1601,01,01)).days\n",
    "    #else: start=(datetime(start.year+1,cur_date.month,cur_date.day)-datetime(1601,01,01)).days\n",
    "    #datelist=range(start+1,end_date-1,365)\n",
    "    #print datelist\n",
    "\n",
    "    #find the position of the requested date in the datelist\n",
    "    #cur_epoch=(cur_date-datetime(1601,01,01)).days\n",
    "    #cur_pos=min(range(len(datelist)),key=lambda x:abs(datelist[x]-cur_epoch))\n",
    "    #print ('Current position:',cur_pos)\n",
    "\n",
    "    try:    \n",
    "        datelist, cur_pos = datelist_irregular_coverage(root, start_date, start, cur_date)\n",
    "        print 'irregular'\n",
    "    except IndexError:\n",
    "        datelist, cur_pos = datelist_regular_coverage(root, start_date, start, cur_date)\n",
    "        print 'regular'\n",
    "\n",
    "    #retrieve the data cube\n",
    "    cube_arr=[]\n",
    "    for d in datelist:\n",
    "        print 'NDVI: ', d        \n",
    "        field={}\n",
    "        field['SERVICE']='WCS'\n",
    "        field['VERSION']='2.0.1'\n",
    "        field['REQUEST']='GetCoverage'\n",
    "        field['COVERAGEID']='NDVI_MOD13C1005_uptodate'#'NDVI_MOD13C1005'#'trmm_3b42_coverage_1'\n",
    "        field['SUBSET']=['ansi('+str(d)+')',\n",
    "                         'Lat('+str(spl_arr[1])+','+str(spl_arr[3])+')',\n",
    "                        'Long('+str(spl_arr[0])+','+str(spl_arr[2])+')']\n",
    "        field['FORMAT']='image/tiff'\n",
    "        url_values = urllib.urlencode(field,doseq=True)\n",
    "        full_url = endpoint + '?' + url_values\n",
    "        #print full_url\n",
    "        tmpfilename='test'+str(d)+'.tif'\n",
    "        f,h = urllib.urlretrieve(full_url,tmpfilename)\n",
    "        #print h\n",
    "        ds=gdal.Open(tmpfilename)\n",
    "\n",
    "        cube_arr.append(ds.ReadAsArray())\n",
    "        #print d\n",
    "\n",
    "    ##calculate the regional VCI\n",
    "    cube_arr_ma=ma.masked_equal(numpy.asarray(cube_arr),-3000)\n",
    "    NVAI=(cube_arr_ma[cur_pos,:,:]-numpy.mean(cube_arr_ma,0))*1.0/(numpy.amax(cube_arr_ma,0)-numpy.amin(cube_arr_ma,0))\n",
    "    \n",
    "    #VCI *= 1000\n",
    "    #VCI //= (1000 - 0 + 1) / 256.\n",
    "    #VCI = VCI.astype(numpy.uint8) \n",
    "    return NVAI,ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _NTAI_CAL(date,spl_arr):\n",
    "\n",
    "    ##request image cube for the specified date and area by WCS.\n",
    "    #firstly we get the temporal length of avaliable NDVI data from the DescribeCoverage of WCS\n",
    "    endpoint='http://192.168.1.104:8080/rasdaman/ows'\n",
    "    field={}\n",
    "    field['SERVICE']='WCS'\n",
    "    field['VERSION']='2.0.1'\n",
    "    field['REQUEST']='DescribeCoverage'\n",
    "    field['COVERAGEID']='LST_MOD11C2005_uptodate'#'LST_MOD11C2005'#'trmm_3b42_coverage_1'\n",
    "    url_values = urllib.urlencode(field,doseq=True)\n",
    "    full_url = endpoint + '?' + url_values\n",
    "    data = urllib.urlopen(full_url).read()\n",
    "    root = etree.fromstring(data)\n",
    "    lc = root.find(\".//{http://www.opengis.net/gml/3.2}lowerCorner\").text\n",
    "    uc = root.find(\".//{http://www.opengis.net/gml/3.2}upperCorner\").text\n",
    "    start_date=int((lc.split(' '))[2])\n",
    "    end_date=int((uc.split(' '))[2])\n",
    "    #print [start_date, end_date]\n",
    "\n",
    "    #generate the dates list \n",
    "    cur_date=datetime.strptime(date,\"%Y-%m-%d\")\n",
    "    #startt=145775\n",
    "    start=datetime.fromtimestamp((start_date-(datetime(1970,01,01)-datetime(1601,01,01)).days)*24*60*60)\n",
    "    #print start\n",
    "\n",
    "    #tmp_date=datetime(start.year,cur_date.month,cur_date.day)\n",
    "    #if tmp_date > start :\n",
    "    #    start=(tmp_date-datetime(1601,01,01)).days\n",
    "    #else: start=(datetime(start.year+1,cur_date.month,cur_date.day)-datetime(1601,01,01)).days\n",
    "    #datelist=range(start+1,end_date-1,365)\n",
    "    #print datelist\n",
    "\n",
    "    #find the position of the requested date in the datelist\n",
    "    #cur_epoch=(cur_date-datetime(1601,01,01)).days\n",
    "    #cur_pos=min(range(len(datelist)),key=lambda x:abs(datelist[x]-cur_epoch))\n",
    "    #print ('Current position:',cur_pos)\n",
    "\n",
    "    try:    \n",
    "        datelist, cur_pos = datelist_irregular_coverage(root, start_date, start, cur_date)\n",
    "        print 'irregular'\n",
    "    except IndexError:\n",
    "        datelist, cur_pos = datelist_regular_coverage(root, start_date, start, cur_date)\n",
    "        print 'regular'\n",
    "        \n",
    "    #retrieve the data cube\n",
    "    cube_arr=[]\n",
    "    for d in datelist:\n",
    "        print 'LST: ', d\n",
    "        field={}\n",
    "        field['SERVICE']='WCS'\n",
    "        field['VERSION']='2.0.1'\n",
    "        field['REQUEST']='GetCoverage'\n",
    "        field['COVERAGEID']='LST_MOD11C2005_uptodate'#'LST_MOD11C2005'#'trmm_3b42_coverage_1'\n",
    "        field['SUBSET']=['ansi('+str(d)+')',\n",
    "                         'Lat('+str(spl_arr[1])+','+str(spl_arr[3])+')',\n",
    "                        'Long('+str(spl_arr[0])+','+str(spl_arr[2])+')']\n",
    "        field['FORMAT']='image/tiff'\n",
    "        url_values = urllib.urlencode(field,doseq=True)\n",
    "        full_url = endpoint + '?' + url_values\n",
    "        #print full_url\n",
    "        tmpfilename='test'+str(d)+'.tif'\n",
    "        f,h = urllib.urlretrieve(full_url,tmpfilename)\n",
    "        #print h\n",
    "        ds=gdal.Open(tmpfilename)\n",
    "\n",
    "        cube_arr.append(ds.ReadAsArray())\n",
    "        #print d\n",
    "\n",
    "    ##calculate the regional VCI\n",
    "    cube_arr_ma=ma.masked_equal(numpy.asarray(cube_arr),-3000)\n",
    "    ##VCI=(cube_arr_ma[cur_pos,:,:]-numpy.amin(cube_arr_ma,0))*1.0/(numpy.amax(cube_arr_ma,0)-numpy.amin(cube_arr_ma,0))\n",
    "    NTAI=(cube_arr_ma[cur_pos,:,:]-numpy.mean(cube_arr_ma,0))*1.0/(numpy.amax(cube_arr_ma,0)-numpy.amin(cube_arr_ma,0))\n",
    "    \n",
    "    return NTAI, cur_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _NDAI_CAL(date,spl_arr,alpha = 0.5):\n",
    "    print 'NDAI start'\n",
    "    print 'calculate NTAI'\n",
    "    NTAI, cur_date = _NTAI_CAL(date,spl_arr)\n",
    "    print 'calculate NVAI'\n",
    "    NVAI, ds = _NVAI_CAL(date,spl_arr)\n",
    "    print 'calculate NDAI'\n",
    "    NDAI = (alpha * NVAI ) + (alpha * NTAI)\n",
    "    print 'NDAI end, lets save'\n",
    "    #VHI *= 1000\n",
    "    #VHI //= (1000 - 0 + 1) / 255. #instead of 256\n",
    "    #VHI = VHI.astype(numpy.uint8)\n",
    "    #VHI += 1 #so mask values are reserverd for 0 \n",
    "\n",
    "    ##write the result VCI to disk\n",
    "    # get parameters\n",
    "    geotransform = ds.GetGeoTransform()\n",
    "    spatialreference = ds.GetProjection()\n",
    "    ncol = ds.RasterXSize\n",
    "    nrow = ds.RasterYSize\n",
    "    nband = 1\n",
    "\n",
    "    # create dataset for output\n",
    "    fmt = 'GTiff'\n",
    "    vhiFileName = 'NDAI'+cur_date.strftime(\"%Y%m%d\")+'.tif'\n",
    "    driver = gdal.GetDriverByName(fmt)\n",
    "    dst_dataset = driver.Create(vhiFileName, ncol, nrow, nband, gdal.GDT_Float32)\n",
    "    dst_dataset.SetGeoTransform(geotransform)\n",
    "    dst_dataset.SetProjection(spatialreference)\n",
    "    dst_dataset.GetRasterBand(1).WriteArray(NDAI)\n",
    "    dst_dataset = None\n",
    "    return vhiFileName    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_colormap(seq):\n",
    "    \"\"\"Return a LinearSegmentedColormap\n",
    "    seq: a sequence of floats and RGB-tuples. The floats should be increasing\n",
    "    and in the interval (0,1).\n",
    "    \"\"\"\n",
    "    seq = [(None,) * 3, 0.0] + list(seq) + [1.0, (None,) * 3]\n",
    "    cdict = {'red': [], 'green': [], 'blue': []}\n",
    "    for i, item in enumerate(seq):\n",
    "        if isinstance(item, float):\n",
    "            r1, g1, b1 = seq[i - 1]\n",
    "            r2, g2, b2 = seq[i + 1]\n",
    "            cdict['red'].append([item, r1, r2])\n",
    "            cdict['green'].append([item, g1, g2])\n",
    "            cdict['blue'].append([item, b1, b2])\n",
    "    return mcolors.LinearSegmentedColormap('CustomMap', cdict)\n",
    "c = mcolors.ColorConverter().to_rgb\n",
    "\n",
    "def cmap_discretize(cmap, N):\n",
    "    \"\"\"Return a discrete colormap from the continuous colormap cmap.\n",
    "    \n",
    "        cmap: colormap instance, eg. cm.jet. \n",
    "        N: number of colors.\n",
    "    \n",
    "    Example\n",
    "        x = resize(arange(100), (5,100))\n",
    "        djet = cmap_discretize(cm.jet, 5)\n",
    "        imshow(x, cmap=djet)\n",
    "    \"\"\"\n",
    "\n",
    "    if type(cmap) == str:\n",
    "        cmap = get_cmap(cmap)\n",
    "    colors_i = np.concatenate((np.linspace(0, 1., N), (0.,0.,0.,0.)))\n",
    "    colors_rgba = cmap(colors_i)\n",
    "    indices = np.linspace(0, 1., N+1)\n",
    "    cdict = {}\n",
    "    for ki,key in enumerate(('red','green','blue')):\n",
    "        cdict[key] = [ (indices[i], colors_rgba[i-1,ki], colors_rgba[i,ki]) for i in xrange(N+1) ]\n",
    "    # Return colormap object.\n",
    "    return matplotlib.colors.LinearSegmentedColormap(cmap.name + \"_%d\"%N, cdict, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStatsCounty(cnty_array, feat, date):\n",
    "    \"\"\"\n",
    "    Core function to calculate statistics to be applied for each county\n",
    "    \n",
    "    Input:  \n",
    "    cnty_array   = Masked raster array of a single county    \n",
    "    feat         = feature of shapefile to extract ID\n",
    "    Output: \n",
    "    county_stats = Dictionary containing the stats for the county\n",
    "    \"\"\"\n",
    "    date = str(date.year)+str(date.month).zfill(2)+str(date.day).zfill(2)\n",
    "    \n",
    "    dc=0\n",
    "    #percentage of no drought\n",
    "    p0=(cnty_array[(cnty_array <= 0) ]).size*1.0/cnty_array.size\n",
    "    if p0>=0.5: dc=1\n",
    "    #percentage of no drought\n",
    "    p1=(cnty_array[(cnty_array<=-0.15)]).size*1.0/cnty_array.size\n",
    "    if p1>0.5: dc=2\n",
    "    #percentage of no drought\n",
    "    p2=(cnty_array[(cnty_array<=-0.25) ]).size*1.0/cnty_array.size\n",
    "    if p2>=0.5: dc=3\n",
    "    #percentage of no drought\n",
    "    p3=(cnty_array[cnty_array <=-0.35]).size*1.0/cnty_array.size\n",
    "    if p3>=0.5: dc=4\n",
    "    #print cnty_array.count(),np.nanmin(cnty_array)\n",
    "    ct=cnty_array.count()\n",
    "    county_stats = {\n",
    "        'MINIMUM': np.nan if ct<2 else float(np.nanmin(cnty_array)),\n",
    "        'MEAN'+date: np.nan if ct<2 else float(np.nanmean(cnty_array)),\n",
    "        'MAX': np.nan if ct<2 else float(np.nanmax(cnty_array)),\n",
    "        'STD': np.nan if ct<2 else float(np.nanstd(cnty_array)),\n",
    "        'SUM': np.nan if ct<2 else float(np.nansum(cnty_array)),\n",
    "        'COUNT': int(cnty_array.count()),\n",
    "        'FID': int(feat.GetFID()),  \n",
    "        'P0'+date:p0,\n",
    "        'P1'+date:p1,\n",
    "        'P2'+date:p2,\n",
    "        'P3'+date:p3,\n",
    "        'DC'+date:dc}\n",
    "    \n",
    "    return county_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bbox_to_pixel_offsets(gt, bbox):\n",
    "    originX = gt[0]\n",
    "    originY = gt[3]\n",
    "    pixel_width = gt[1]\n",
    "    pixel_height = gt[5]\n",
    "    x1 = int((bbox[0] - originX) / pixel_width)\n",
    "    x2 = int((bbox[1] - originX) / pixel_width) + 1\n",
    "\n",
    "    y1 = int((bbox[3] - originY) / pixel_height)\n",
    "    y2 = int((bbox[2] - originY) / pixel_height) + 1\n",
    "\n",
    "    xsize = x2 - x1\n",
    "    ysize = y2 - y1\n",
    "    return (x1, y1, xsize, ysize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def zonal_stats(vector_path, raster_path, nodata_value, date):\n",
    "        \n",
    "    # open raster layer\n",
    "    rds = gdal.Open(raster_path, GA_ReadOnly)\n",
    "    assert(rds)\n",
    "    rb = rds.GetRasterBand(1)\n",
    "    rgt = rds.GetGeoTransform()\n",
    "    \n",
    "    # set raster nodata value\n",
    "    if nodata_value:\n",
    "        nodata_value = float(nodata_value)\n",
    "        rb.SetNoDataValue(nodata_value)\n",
    "    \n",
    "    # open vector layer\n",
    "    vds = ogr.Open(vector_path, GA_ReadOnly)  \n",
    "    assert(vds)\n",
    "    vlyr = vds.GetLayer(0)    \n",
    "    \n",
    "    # compare EPSG values of vector and raster and change projection if necessary\n",
    "    sourceSR = vlyr.GetSpatialRef()\n",
    "    sourceSR.AutoIdentifyEPSG()\n",
    "    EPSG_sourceSR = sourceSR.GetAuthorityCode(None)\n",
    "    \n",
    "    targetSR = osr.SpatialReference(wkt=rds.GetProjection())\n",
    "    targetSR.AutoIdentifyEPSG()\n",
    "    EPSG_targetSR = targetSR.GetAuthorityCode(None)\n",
    "    \n",
    "    if EPSG_sourceSR != EPSG_sourceSR:\n",
    "        # reproject vector geometry to same projection as raster\n",
    "        print 'unequal projections'    \n",
    "        sourceSR = vlyr.GetSpatialRef()\n",
    "        targetSR = osr.SpatialReference()\n",
    "        targetSR.ImportFromWkt(rds.GetProjectionRef())\n",
    "        coordTrans = osr.CreateCoordinateTransformation(sourceSR,targetSR)    \n",
    "        \n",
    "    \"\"\"do the work\"\"\"\n",
    "    global_src_extent = None\n",
    "    mem_drv = ogr.GetDriverByName('Memory')\n",
    "    driver = gdal.GetDriverByName('MEM')\n",
    "    \n",
    "    # Loop through vectors\n",
    "    stats = []\n",
    "    feat = vlyr.GetNextFeature() \n",
    "    \n",
    "    while feat is not None:\n",
    "        # print statement after each hunderds features\n",
    "        fid = int(feat.GetFID())        \n",
    "        if fid % 500 == 0:\n",
    "            print(\"finished first %s features\" % (fid))\n",
    "    \n",
    "        if not global_src_extent:\n",
    "            #print 'bbox county'\n",
    "            # use local source extent\n",
    "            # fastest option when you have fast disks and well indexed raster (ie tiled Geotiff)\n",
    "            # advantage: each feature uses the smallest raster chunk\n",
    "            # disadvantage: lots of reads on the source raster\n",
    "            src_offset = bbox_to_pixel_offsets(rgt, feat.geometry().GetEnvelope())\n",
    "            src_array = rb.ReadAsArray(*src_offset)\n",
    "        \n",
    "            # calculate new geotransform of the feature subset\n",
    "            new_gt = (\n",
    "                (rgt[0] + (src_offset[0] * rgt[1])),\n",
    "                rgt[1],\n",
    "                0.0,\n",
    "                (rgt[3] + (src_offset[1] * rgt[5])),\n",
    "                0.0,\n",
    "                rgt[5]\n",
    "            )\n",
    "        \n",
    "        # Create a temporary vector layer in memory\n",
    "        mem_ds = mem_drv.CreateDataSource('out')\n",
    "        mem_layer = mem_ds.CreateLayer('poly', None, ogr.wkbPolygon)\n",
    "        mem_layer.CreateFeature(feat.Clone())\n",
    "        \n",
    "        # Rasterize it\n",
    "        rvds = driver.Create('', src_offset[2], src_offset[3], 1, gdal.GDT_Byte)\n",
    "        rvds.SetGeoTransform(new_gt)\n",
    "        gdal.RasterizeLayer(rvds, [1], mem_layer, burn_values=[1])\n",
    "        rv_array = rvds.ReadAsArray()\n",
    "        \n",
    "        # Mask the source data array with our current feature\n",
    "        # we take the logical_not to flip 0<->1 to get the correct mask effect\n",
    "        # we also mask out nodata values explictly\n",
    "        try:\n",
    "            masked = np.ma.MaskedArray(\n",
    "                src_array,\n",
    "                mask=np.logical_or(\n",
    "                    src_array == nodata_value,\n",
    "                    np.logical_not(rv_array)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            #print 'feature ID: ',int(feat.GetFID())\n",
    "            \n",
    "            # GET STATISTICS FOR EACH COUNTY\n",
    "            try:\n",
    "                county_stats = getStatsCounty(cnty_array = masked, feat=feat, date=date)            \n",
    "                stats.append(county_stats)\n",
    "\n",
    "                rvds = None\n",
    "                mem_ds = None\n",
    "                feat = vlyr.GetNextFeature()\n",
    "            except IndexError:\n",
    "                print 'feature ID: ',fid, 'IndexError, ignore county and lets continue'                \n",
    "                rvds = None\n",
    "                mem_ds = None\n",
    "                feat = vlyr.GetNextFeature()                \n",
    "                \n",
    "            \n",
    "        except np.ma.MaskError: \n",
    "            # catch MaskError, ignore feature containing no valid corresponding raster data set\n",
    "            # in my case the the most southern county of hainan is not totally within the raster extent            \n",
    "            print 'feature ID: ',fid, ' maskError, ignore county and lets continue'\n",
    "            \n",
    "            rvds = None\n",
    "            mem_ds = None\n",
    "            feat = vlyr.GetNextFeature()            \n",
    "    \n",
    "    vds = None\n",
    "    rds = None\n",
    "    return stats#, src_array, rv_array, masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_map(china_adm3_shp, date, extent, columns_shp):\n",
    "    \n",
    "    ax1_head = columns_shp[0] # P00082014\n",
    "    ax2_head = columns_shp[1] # P10082014\n",
    "    ax3_head = columns_shp[2] # P20082014\n",
    "    ax4_head = columns_shp[3] # P30082014\n",
    "    ax5_head = columns_shp[4] # MEAN\n",
    "    ax6_head = columns_shp[5] # DC0082014          \n",
    "    # print i\n",
    "    # ax1_head = 'P1'+str(i[-7:])\n",
    "    # ax2_head = 'P2'+str(i[-7:])\n",
    "    # ax3_head = 'P3'+str(i[-7:])        \n",
    "    # ax4_head = 'P4'+str(i[-7:])\n",
    "    # ax5_head = 'N'+str(i[-7:])\n",
    "    # ax6_head = 'C'+str(i[-7:])        \n",
    "    print ax1_head, ax2_head, ax3_head, ax4_head,ax5_head,ax6_head\n",
    "    \n",
    "    drought_cat_tci_cmap = make_colormap([c('#993406'), c('#D95E0E'),0.2, c('#D95E0E'), c('#FE9829'),0.4, \n",
    "                                          c('#FE9829'), c('#FFD98E'),0.6, c('#FFD98E'), c('#FEFFD3'),0.8, c('#C4DC73')])\n",
    "\n",
    "    drought_per_tci_cmap = make_colormap([c('#993406'), c('#D95E0E'),0.2, c('#D95E0E'), c('#FE9829'),0.4, \n",
    "                                          c('#FE9829'), c('#FFD98E'),0.6, c('#FFD98E'), c('#FEFFD3'),0.8, c('#FEFFD3')])\n",
    "\n",
    "    drought_avg_tci_cmap = make_colormap([c('#993406'), c('#D95E0E'),0.1, c('#D95E0E'), c('#FE9829'),0.2, \n",
    "                                          c('#FE9829'), c('#FFD98E'),0.3, c('#FFD98E'), c('#FEFFD3'),0.4, \n",
    "                                          c('#FEFFD3'), c('#C4DC73'),0.5, c('#C4DC73'), c('#93C83D'),0.6,\n",
    "                                          c('#93C83D'), c('#69BD45'),0.7, c('#69BD45'), c('#6ECCDD'),0.8,\n",
    "                                          c('#6ECCDD'), c('#3553A4'),0.9, c('#3553A4')])\n",
    "\n",
    "    #extent = [111.91693268, 123.85693268, 49.43324112, 40.67324112]\n",
    "    #extent = [73.5,140,14,53.6]    \n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(27.69123,12))\n",
    "    gs = gridspec.GridSpec(3, 3)\n",
    "\n",
    "    #############--------------################-------------#############--------------################-------------\n",
    "\n",
    "    # PLOT TOP LEFT\n",
    "    ax1 = fig.add_subplot(gs[0,0], projection=ccrs.InterruptedGoodeHomolosine(central_longitude=0))\n",
    "    ax1.set_extent(extent)\n",
    "    ax1.outline_patch.set_edgecolor('none')\n",
    "    ax1.coastlines(resolution='110m')\n",
    "\n",
    "    gl1 = ax1.gridlines()\n",
    "#     gl1.xlocator = mticker.FixedLocator([50, 70,90,110,130,150,170])\n",
    "#     gl1.ylocator = mticker.FixedLocator([10,  20,  30,  40,  50, 60])\n",
    "#     gl1.xformatter = LONGITUDE_FORMATTER\n",
    "#     gl1.yformatter = LATITUDE_FORMATTER\n",
    "\n",
    "    ax1.add_feature(cfeature.LAND, facecolor='0.85')      \n",
    "\n",
    "    # PLOT MIDDLE LEFT\n",
    "    ax2 = fig.add_subplot(gs[1,0], projection=ccrs.InterruptedGoodeHomolosine(central_longitude=0))\n",
    "    ax2.set_extent(extent)\n",
    "    ax2.outline_patch.set_edgecolor('none')\n",
    "    ax2.coastlines(resolution='110m')\n",
    "\n",
    "    gl2 = ax2.gridlines()\n",
    "#     gl2.xlocator = mticker.FixedLocator([50, 70,90,110,130,150,170])\n",
    "#     gl2.ylocator = mticker.FixedLocator([10,  20,  30,  40,  50, 60])\n",
    "#     gl2.xformatter = LONGITUDE_FORMATTER\n",
    "#     gl2.yformatter = LATITUDE_FORMATTER\n",
    "\n",
    "    ax2.add_feature(cfeature.LAND, facecolor='0.85')     \n",
    "\n",
    "    #############--------------################-------------#############--------------################-------------\n",
    "\n",
    "    # PLOT BOTTOM LEFT\n",
    "    ax3 = fig.add_subplot(gs[2, 0], projection=ccrs.InterruptedGoodeHomolosine(central_longitude=0))\n",
    "    ax3.set_extent(extent)\n",
    "    ax3.outline_patch.set_edgecolor('none')\n",
    "    ax3.coastlines(resolution='110m')\n",
    "\n",
    "    gl3 = ax3.gridlines()\n",
    "#     gl3.xlocator = mticker.FixedLocator([50, 70,90,110,130,150,170])\n",
    "#     gl3.ylocator = mticker.FixedLocator([10,  20,  30,  40,  50, 60])\n",
    "#     gl3.xformatter = LONGITUDE_FORMATTER\n",
    "#     gl3.yformatter = LATITUDE_FORMATTER\n",
    "\n",
    "    ax3.add_feature(cfeature.LAND, facecolor='0.85')        \n",
    "\n",
    "    #############--------------################-------------#############--------------################-------------\n",
    "\n",
    "    # PLOT BOTTOM MIDDLE\n",
    "    ax4 = fig.add_subplot(gs[2,1], projection=ccrs.InterruptedGoodeHomolosine(central_longitude=0))\n",
    "    ax4.set_extent(extent)\n",
    "    ax4.outline_patch.set_edgecolor('none')\n",
    "    ax4.coastlines(resolution='110m')\n",
    "\n",
    "    gl4 = ax4.gridlines()\n",
    "#     gl4.xlocator = mticker.FixedLocator([50, 70,90,110,130,150,170])\n",
    "#     gl4.ylocator = mticker.FixedLocator([10,  20,  30,  40,  50, 60])\n",
    "#     gl4.xformatter = LONGITUDE_FORMATTER\n",
    "#     gl4.yformatter = LATITUDE_FORMATTER\n",
    "\n",
    "    ax4.add_feature(cfeature.LAND, facecolor='0.85')     \n",
    "\n",
    "    #############--------------################-------------#############--------------################-------------\n",
    "\n",
    "    # PLOT BOTTOM RIGHT\n",
    "    ax5 = fig.add_subplot(gs[2,2], projection=ccrs.InterruptedGoodeHomolosine(central_longitude=0))\n",
    "    ax5.set_extent(extent)\n",
    "    ax5.outline_patch.set_edgecolor('none')\n",
    "    ax5.coastlines(resolution='110m')\n",
    "\n",
    "\n",
    "    gl5 = ax5.gridlines()\n",
    "#     gl5.xlocator = mticker.FixedLocator([50, 70,90,110,130,150,170])\n",
    "#     gl5.ylocator = mticker.FixedLocator([10,  20,  30,  40,  50, 60])\n",
    "#     gl5.xformatter = LONGITUDE_FORMATTER\n",
    "#     gl5.yformatter = LATITUDE_FORMATTER\n",
    "    ax5.add_feature(cfeature.LAND, facecolor='0.85')          \n",
    "\n",
    "    #############--------------################-------------#############--------------################-------------\n",
    "\n",
    "    # PLOT CENTER\n",
    "    ax6 = fig.add_subplot(gs[0:2,1:3], projection=ccrs.InterruptedGoodeHomolosine(central_longitude=0))\n",
    "    ax6.set_extent(extent)\n",
    "    ax6.outline_patch.set_edgecolor('none')\n",
    "    ax6.coastlines(resolution='110m')\n",
    "\n",
    "    gl6 = ax6.gridlines()\n",
    "    #gl6.xlocator = mticker.FixedLocator([50, 70,90,110,130,150,170])\n",
    "    #gl6.ylocator = mticker.FixedLocator([10,  20,  30,  40,  50, 60])\n",
    "    #gl6.xformatter = LONGITUDE_FORMATTER\n",
    "    #gl6.yformatter = LATITUDE_FORMATTER\n",
    "\n",
    "    ax6.add_feature(cfeature.LAND, facecolor='0.85')\n",
    "    ax6.add_feature(cfeature.COASTLINE, linewidth=0.2, edgecolor='black')\n",
    "    ax6.add_feature(cfeature.BORDERS, linewidth=0.2, edgecolor='black')   \n",
    "    linewidth=0.1\n",
    "    #         # classify each county based on column ID_3\n",
    "    #for record, county in zip(china_adm3_shp.records(), china_adm3_shp.geometries()): \n",
    "    for idx, record in enumerate(china_adm3_shp.records()):    \n",
    "        if idx % 250 == 0:\n",
    "            print(\"drawed first %s counties\" % (idx))  \n",
    "\n",
    "        # Ax1 -- Ax1 -- Ax1\n",
    "        # extract for each row the value corresponding to the column header \n",
    "        ID = float(record.attributes[ax1_head])\n",
    "        # Classify the records in to groups\n",
    "    #             if ID == 0:\n",
    "    #                 facecolor = '#C4DC73'\n",
    "    #                 edgecolor = 'k'#'#FEFFD3'\n",
    "    #                 linewidth = 0.05\n",
    "        if (ID >= .0) and (ID <= .25):\n",
    "            facecolor = '#FEFFD3'\n",
    "            edgecolor = '#FEFFD3'\n",
    "        if (ID > .25) and (ID <= .5):\n",
    "            facecolor = '#FFD98E'\n",
    "            edgecolor = '#FFD98E'    \n",
    "        if (ID > .5) and (ID <= .75):\n",
    "            facecolor = '#D95E0E'\n",
    "            edgecolor = '#D95E0E'\n",
    "        if ID > .75:\n",
    "            facecolor = '#993406'\n",
    "            edgecolor = '#993406'\n",
    "        ax1.add_geometries(record.geometry, ccrs.PlateCarree(),facecolor=facecolor, edgecolor=edgecolor, linewidth=linewidth)\n",
    "\n",
    "        # Ax2 -- Ax2 -- Ax2            \n",
    "        # extract for each row the value corresponding to the column header \n",
    "        ID = float(record.attributes[ax2_head])\n",
    "        # Classify the records in to groups\n",
    "    #             if ID == 0:\n",
    "    #                 facecolor = '#C4DC73'\n",
    "    #                 edgecolor = 'k'#'#FEFFD3'\n",
    "    #                 linewidth = 0.05\n",
    "        if (ID >= .0) and (ID <= .25):\n",
    "            facecolor = '#FEFFD3'\n",
    "            edgecolor = '#FEFFD3'\n",
    "        if (ID > .25) and (ID <= .5):\n",
    "            facecolor = '#FFD98E'\n",
    "            edgecolor = '#FFD98E'    \n",
    "        if (ID > .5) and (ID <= .75):\n",
    "            facecolor = '#D95E0E'\n",
    "            edgecolor = '#D95E0E'\n",
    "        if ID > .75:\n",
    "            facecolor = '#993406'\n",
    "            edgecolor = '#993406'\n",
    "        ax2.add_geometries(record.geometry, ccrs.PlateCarree(),facecolor=facecolor, edgecolor=edgecolor, linewidth=linewidth)  \n",
    "\n",
    "        # Ax3 -- Ax3 -- Ax3                        \n",
    "        # extract for each row the value corresponding to the column header \n",
    "        ID = float(record.attributes[ax3_head])\n",
    "        # Classify the records in to groups\n",
    "    #             if ID == 0:\n",
    "    #                 facecolor = '#C4DC73'\n",
    "    #                 edgecolor = 'k'#'#FEFFD3'\n",
    "    #                 linewidth = 0.05\n",
    "        if (ID >= .0) and (ID <= .25):\n",
    "            facecolor = '#FEFFD3'\n",
    "            edgecolor = '#FEFFD3'\n",
    "        if (ID > .25) and (ID <= .5):\n",
    "            facecolor = '#FFD98E'\n",
    "            edgecolor = '#FFD98E'    \n",
    "        if (ID > .5) and (ID <= .75):\n",
    "            facecolor = '#D95E0E'\n",
    "            edgecolor = '#D95E0E'\n",
    "        if ID > .75:\n",
    "            facecolor = '#993406'\n",
    "            edgecolor = '#993406'\n",
    "        ax3.add_geometries(record.geometry, ccrs.PlateCarree(),facecolor=facecolor, edgecolor=edgecolor, linewidth=linewidth) \n",
    "\n",
    "        # Ax4 -- Ax4 -- Ax4\n",
    "        # extract for each row the value corresponding to the column header             \n",
    "        ID = float(record.attributes[ax4_head])\n",
    "    #             if ID == 0:\n",
    "    #                 facecolor = '#C4DC73'\n",
    "    #                 edgecolor = 'k'#'#FEFFD3'\n",
    "    #                 linewidth = 0.05\n",
    "        if (ID >= .0) and (ID <= .25):\n",
    "            facecolor = '#FEFFD3'\n",
    "            edgecolor = '#FEFFD3'\n",
    "        if (ID > .25) and (ID <= .5):\n",
    "            facecolor = '#FFD98E'\n",
    "            edgecolor = '#FFD98E'    \n",
    "        if (ID > .5) and (ID <= .75):\n",
    "            facecolor = '#D95E0E'\n",
    "            edgecolor = '#D95E0E'\n",
    "        if ID > .75:\n",
    "            facecolor = '#993406'\n",
    "            edgecolor = '#993406'\n",
    "        ax4.add_geometries(record.geometry, ccrs.PlateCarree(),facecolor=facecolor, edgecolor=edgecolor, linewidth=linewidth)\n",
    "\n",
    "        # Ax5 -- Ax5 -- Ax5            \n",
    "        # extract for each row the value corresponding to the column header \n",
    "        ID = float(record.attributes[ax5_head])\n",
    "        # Classify the records in to groups\n",
    "        if ID <= -0.35:\n",
    "            facecolor = '#993406'\n",
    "            edgecolor = '#993406'\n",
    "        if (ID > -0.35) and (ID <= -0.25):\n",
    "            facecolor = '#E26D15'\n",
    "            edgecolor = '#E26D15'    \n",
    "        if (ID > -0.25) and (ID <= -0.15):\n",
    "            facecolor = '#FFB95C'\n",
    "            edgecolor = '#FFB95C'\n",
    "        if (ID > -0.15) and (ID <= 0):\n",
    "            facecolor = '#FEF6C3'\n",
    "            edgecolor = '#FEF6C3'\n",
    "        if (ID > 0) and (ID <= 0.15):\n",
    "            facecolor = '#A0CD4C'\n",
    "            edgecolor = '#A0CD4C'\n",
    "        if (ID > 0.15) and (ID <= 0.25):\n",
    "            facecolor = '#6ABF5A'\n",
    "            edgecolor = '#6ABF5A'    \n",
    "        if (ID > 0.25) and (ID <= 0.35):\n",
    "            facecolor = '#4C85BB'\n",
    "            edgecolor = '#4C85BB'    \n",
    "        if (ID > 0.35) and (ID <= 1):\n",
    "            facecolor = '#3553A4'\n",
    "            edgecolor = '#3553A4'                    \n",
    "        ax5.add_geometries(record.geometry, ccrs.PlateCarree(),facecolor=facecolor, edgecolor=edgecolor, linewidth=linewidth)            \n",
    "\n",
    "        # Ax6 -- Ax6 -- Ax6             \n",
    "        ID = int(float(record.attributes[ax6_head]))\n",
    "        # Classify the records in to groups\n",
    "        if ID == 0:\n",
    "            facecolor = '#C4DC73'\n",
    "            edgecolor = 'k'#'#FEFFD3'\n",
    "            linewidth = 0.05\n",
    "        if ID == 1:\n",
    "            facecolor = '#FEF6C3'\n",
    "            edgecolor = '#FEF6C3'\n",
    "        if ID == 2:\n",
    "            facecolor = '#FFB95C'\n",
    "            edgecolor = '#FFB95C'\n",
    "        if ID == 3:\n",
    "            facecolor = '#E26D15'\n",
    "            edgecolor = '#E26D15'\n",
    "        if ID == 4:\n",
    "            facecolor = '#993406'\n",
    "            edgecolor = '#993406'\n",
    "        ax6.add_geometries(record.geometry, ccrs.PlateCarree(),facecolor=facecolor, edgecolor=edgecolor, linewidth=linewidth)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #date = i[-7:]\n",
    "    #year = date[-4::]\n",
    "    #doy = date[-7:-4]\n",
    "    #date_out = datetime.datetime.strptime(str(year)+'-'+str(doy),'%Y-%j')\n",
    "    date_label = 'Date: '+str(date.year) +'-'+str(date.month).zfill(2)+'-'+str(date.day).zfill(2)\n",
    "    # ADD LABELS FOR EACH PLOT\n",
    "#     ax1.plot(116.4, 39.3, 'ks', markersize=5, transform=ccrs.Geodetic())\n",
    "#     ax1.text(64, 51, 'Percentage of Slight Drought', weight='semibold', fontsize=12, transform=ccrs.Geodetic())        \n",
    "#     ax2.plot(116.4, 39.3, 'ks', markersize=5, transform=ccrs.Geodetic())\n",
    "#     ax2.text(64, 51, 'Percentage of Moderate Drought', weight='semibold', fontsize=12, transform=ccrs.Geodetic())                \n",
    "#     ax3.plot(116.4, 39.3, 'ks', markersize=5, transform=ccrs.Geodetic())\n",
    "#     ax3.text(64, 51, 'Percentage of Severe Drought', weight='semibold', fontsize=12, transform=ccrs.Geodetic())                \n",
    "#     ax4.plot(116.4, 39.3, 'ks', markersize=5, transform=ccrs.Geodetic())\n",
    "#     ax4.text(64, 51, 'Percentage of Extreme Drought', weight='semibold', fontsize=12, transform=ccrs.Geodetic())                \n",
    "#     ax5.plot(116.4, 39.3, 'ks', markersize=5, transform=ccrs.Geodetic())        \n",
    "#     ax5.text(64, 51, 'Average of NDAI', weight='semibold', fontsize=12, transform=ccrs.Geodetic())                \n",
    "#     ax6.plot(116.4, 39.3, 'ks', markersize=7, transform=ccrs.Geodetic())\n",
    "#     ax6.text(64, 51, 'Drought Alert at County Level', fontsize=20, weight='semibold', color='k',transform=ccrs.Geodetic())\n",
    "#     ax6.text(65.5, 49, date_label, fontsize=20, weight='semibold', color='k',transform=ccrs.Geodetic())\n",
    "#     ax6.text(117, 40., 'Beijing', weight='semibold', transform=ccrs.Geodetic()) \n",
    "\n",
    "    # ADD LEGEND IN SOME PLOTS\n",
    "    # -------------------------Ax 1\n",
    "    #cbax1 = fig.add_axes([0.328, 0.67, 0.011, 0.16]) # without tight_layout()\n",
    "    cbax1 = fig.add_axes([0.03, 0.7, 0.011, 0.10]) # including tight_layout()\n",
    "\n",
    "    #cmap = mpl.colors.ListedColormap(['r', 'g', 'b', 'c'])\n",
    "    cmap = cmap_discretize(drought_per_tci_cmap,6)\n",
    "    cmap.set_over('0.25')\n",
    "    cmap.set_under('0.75')\n",
    "\n",
    "    # If a ListedColormap is used, the length of the bounds array must be\n",
    "    # one greater than the length of the color list.  The bounds must be\n",
    "    # monotonically increasing.\n",
    "    bounds = [1, 2, 3, 4, 5]\n",
    "    bounds_ticks = [1.5, 2.5, 3.5, 4.5]\n",
    "    bounds_ticks_name = ['>75%', '50-75%', '25-50%', '<25%']\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    cb2 = mpl.colorbar.ColorbarBase(cbax1, cmap=cmap,\n",
    "                                         norm=norm,\n",
    "                                         # to use 'extend', you must\n",
    "                                         # specify two extra boundaries:\n",
    "                                         #boundaries=[0]+bounds+[13],\n",
    "                                         #extend='both',\n",
    "                                         extendfrac='auto',\n",
    "                                         ticklocation='right',\n",
    "                                         ticks=bounds_ticks,#_name, # optional\n",
    "                                         spacing='proportional',\n",
    "                                         orientation='vertical')\n",
    "    #cb2.set_label('Discrete intervals, some other units')\n",
    "    cb2.set_ticklabels(bounds_ticks_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------------------Ax 2\n",
    "    #cbax1 = fig.add_axes([0.328, 0.67, 0.011, 0.16]) # without tight_layout()\n",
    "    cbax2 = fig.add_axes([0.03, 0.37, 0.011, 0.10]) # including tight_layout()\n",
    "\n",
    "    #cmap = mpl.colors.ListedColormap(['r', 'g', 'b', 'c'])\n",
    "    cmap = cmap_discretize(drought_per_tci_cmap,6)\n",
    "    cmap.set_over('0.25')\n",
    "    cmap.set_under('0.75')\n",
    "\n",
    "    # If a ListedColormap is used, the length of the bounds array must be\n",
    "    # one greater than the length of the color list.  The bounds must be\n",
    "    # monotonically increasing.\n",
    "    bounds = [1, 2, 3, 4, 5]\n",
    "    bounds_ticks = [1.5, 2.5, 3.5, 4.5]\n",
    "    bounds_ticks_name = ['>75%', '50-75%', '25-50%', '<25%']\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    cb2 = mpl.colorbar.ColorbarBase(cbax2, cmap=cmap,\n",
    "                                         norm=norm,\n",
    "                                         # to use 'extend', you must\n",
    "                                         # specify two extra boundaries:\n",
    "                                         #boundaries=[0]+bounds+[13],\n",
    "                                         #extend='both',\n",
    "                                         extendfrac='auto',\n",
    "                                         ticklocation='right',\n",
    "                                         ticks=bounds_ticks,#_name, # optional\n",
    "                                         spacing='proportional',\n",
    "                                         orientation='vertical')\n",
    "    #cb2.set_label('Discrete intervals, some other units')\n",
    "    cb2.set_ticklabels(bounds_ticks_name)   \n",
    "\n",
    "    \n",
    "    # -------------------------Ax 3\n",
    "    #cbax1 = fig.add_axes([0.328, 0.67, 0.011, 0.16]) # without tight_layout()\n",
    "    cbax3 = fig.add_axes([0.03, 0.04, 0.011, 0.10]) # including tight_layout()\n",
    "\n",
    "    #cmap = mpl.colors.ListedColormap(['r', 'g', 'b', 'c'])\n",
    "    cmap = cmap_discretize(drought_per_tci_cmap,6)\n",
    "    cmap.set_over('0.25')\n",
    "    cmap.set_under('0.75')\n",
    "\n",
    "    # If a ListedColormap is used, the length of the bounds array must be\n",
    "    # one greater than the length of the color list.  The bounds must be\n",
    "    # monotonically increasing.\n",
    "    bounds = [1, 2, 3, 4, 5]\n",
    "    bounds_ticks = [1.5, 2.5, 3.5, 4.5]\n",
    "    bounds_ticks_name = ['>75%', '50-75%', '25-50%', '<25%']\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    cb2 = mpl.colorbar.ColorbarBase(cbax3, cmap=cmap,\n",
    "                                         norm=norm,\n",
    "                                         # to use 'extend', you must\n",
    "                                         # specify two extra boundaries:\n",
    "                                         #boundaries=[0]+bounds+[13],\n",
    "                                         #extend='both',\n",
    "                                         extendfrac='auto',\n",
    "                                         ticklocation='right',\n",
    "                                         ticks=bounds_ticks,#_name, # optional\n",
    "                                         spacing='proportional',\n",
    "                                         orientation='vertical')\n",
    "    #cb2.set_label('Discrete intervals, some other units')\n",
    "    cb2.set_ticklabels(bounds_ticks_name)    \n",
    "    \n",
    "\n",
    "    # -------------------------Ax 4\n",
    "    #cbax1 = fig.add_axes([0.328, 0.67, 0.011, 0.16]) # without tight_layout()\n",
    "    cbax4 = fig.add_axes([0.36, 0.04, 0.011, 0.10]) # including tight_layout()\n",
    "\n",
    "    #cmap = mpl.colors.ListedColormap(['r', 'g', 'b', 'c'])\n",
    "    cmap = cmap_discretize(drought_per_tci_cmap,6)\n",
    "    cmap.set_over('0.25')\n",
    "    cmap.set_under('0.75')\n",
    "\n",
    "    # If a ListedColormap is used, the length of the bounds array must be\n",
    "    # one greater than the length of the color list.  The bounds must be\n",
    "    # monotonically increasing.\n",
    "    bounds = [1, 2, 3, 4, 5]\n",
    "    bounds_ticks = [1.5, 2.5, 3.5, 4.5]\n",
    "    bounds_ticks_name = ['>75%', '50-75%', '25-50%', '<25%']\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    cb2 = mpl.colorbar.ColorbarBase(cbax4, cmap=cmap,\n",
    "                                         norm=norm,\n",
    "                                         # to use 'extend', you must\n",
    "                                         # specify two extra boundaries:\n",
    "                                         #boundaries=[0]+bounds+[13],\n",
    "                                         #extend='both',\n",
    "                                         extendfrac='auto',\n",
    "                                         ticklocation='right',\n",
    "                                         ticks=bounds_ticks,#_name, # optional\n",
    "                                         spacing='proportional',\n",
    "                                         orientation='vertical')\n",
    "    #cb2.set_label('Discrete intervals, some other units')\n",
    "    cb2.set_ticklabels(bounds_ticks_name)        \n",
    "\n",
    "    \n",
    "    # -------------------------Ax 5\n",
    "    #cbax5 = fig.add_axes([0.85, 0.15, 0.011, 0.16]) # without tight_layout()\n",
    "    cbax5 = fig.add_axes([0.6922, 0.04, 0.011, 0.16]) # including tight_layout()    \n",
    "\n",
    "    #cmap = mpl.colors.ListedColormap(['r', 'g', 'b', 'c'])\n",
    "    cmap = cmap_discretize(drought_avg_tci_cmap,8)\n",
    "    cmap.set_over('0.25')\n",
    "    cmap.set_under('0.75')\n",
    "\n",
    "    # If a ListedColormap is used, the length of the bounds array must be\n",
    "    # one greater than the length of the color list.  The bounds must be\n",
    "    # monotonically increasing.\n",
    "    bounds = [1, 2, 3, 4, 5,6,7,8,9]\n",
    "    bounds_ticks = [1.5, 2.5, 3.5, 4.5,5.5,6.6,7.5,8.5]\n",
    "    bounds_ticks_name = [' ', '-0.35', ' ', '-0.15','0','0.15',' ','0.35',' ']\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    cb2 = mpl.colorbar.ColorbarBase(cbax5, cmap=cmap,\n",
    "                                         norm=norm,\n",
    "                                         # to use 'extend', you must\n",
    "                                         # specify two extra boundaries:\n",
    "                                         #boundaries=[0]+bounds+[13],\n",
    "                                         #extend='both',\n",
    "                                         extendfrac='auto',\n",
    "                                         ticklocation='right',\n",
    "                                         ticks=bounds,#_name, # optional\n",
    "                                         spacing='proportional',\n",
    "                                         orientation='vertical')        \n",
    "    cb2.set_ticklabels(bounds_ticks_name)     \n",
    "\n",
    "    # ------------------------Ax 6\n",
    "    #cbax6 = fig.add_axes([0.79, 0.48, 0.020, 0.30]) # without tight_layout()\n",
    "    cbax6 = fig.add_axes([0.37, 0.4, 0.025, 0.20]) # without tight_layout()    \n",
    "\n",
    "    #cmap = mpl.colors.ListedColormap(['r', 'g', 'b', 'c'])\n",
    "    cmap = cmap_discretize(drought_cat_tci_cmap,5)\n",
    "    cmap.set_over('0.25')\n",
    "    cmap.set_under('0.75')\n",
    "\n",
    "    # If a ListedColormap is used, the length of the bounds array must be\n",
    "    # one greater than the length of the color list.  The bounds must be\n",
    "    # monotonically increasing.\n",
    "    bounds = [1, 2, 3, 4, 5,6]\n",
    "    bounds_ticks = [1.5, 2.5, 3.5, 4.5,5.5]\n",
    "    bounds_ticks_name = ['Extreme Drought', 'Severe Drought', 'Moderate Drought', 'Slight Drought', 'No Drought']\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    cb2 = mpl.colorbar.ColorbarBase(cbax6, cmap=cmap,\n",
    "                                         norm=norm,\n",
    "                                         # to use 'extend', you must\n",
    "                                         # specify two extra boundaries:\n",
    "                                         #boundaries=[0]+bounds+[13],\n",
    "                                         #extend='both',\n",
    "                                         extendfrac='auto',\n",
    "                                         ticklocation='right',\n",
    "                                         ticks=bounds_ticks,#_name, # optional\n",
    "                                         spacing='proportional',\n",
    "                                         orientation='vertical')\n",
    "    #cb2.set_label('Discrete intervals, some other units')\n",
    "    cb2.set_ticklabels(bounds_ticks_name)\n",
    "    cb2.ax.tick_params(labelsize=12)\n",
    "    #         # ADD LAKES AND RIVERS \n",
    "    #         #FOR PLOT 1\n",
    "    #         lakes = cfeature.LAKES.scale='110m'\n",
    "    #         rivers = cfeature.RIVERS.scale='110m'        \n",
    "    #         ax1.add_feature(cfeature.LAKES)\n",
    "    #         ax1.add_feature(cfeature.RIVERS)         \n",
    "\n",
    "    #         #FOR PLOT 2        \n",
    "    #         ax2.add_feature(cfeature.LAKES)\n",
    "    #         ax2.add_feature(cfeature.RIVERS)         \n",
    "\n",
    "    #         #FOR PLOT 3        \n",
    "    #         ax3.add_feature(cfeature.LAKES)\n",
    "    #         ax3.add_feature(cfeature.RIVERS)                 \n",
    "\n",
    "    #         #FOR PLOT 4        \n",
    "    #         ax4.add_feature(cfeature.LAKES)\n",
    "    #         ax4.add_feature(cfeature.RIVERS)         \n",
    "\n",
    "    #         #FOR PLOT 5\n",
    "    #         ax5.add_feature(cfeature.LAKES)\n",
    "    #         ax5.add_feature(cfeature.RIVERS)                 \n",
    "\n",
    "    #FOR PLOT 6        \n",
    "    #lakes = cfeature.LAKES.scale='50m'\n",
    "    #rivers = cfeature.RIVERS.scale='50m'        \n",
    "    #ax6.add_feature(cfeature.LAKES)\n",
    "    #ax6.add_feature(cfeature.RIVERS)\n",
    "    ax1.add_feature(cfeature.COASTLINE, linewidth=0.2, edgecolor='black')\n",
    "    ax1.add_feature(cfeature.BORDERS, linewidth=0.2, edgecolor='black')        \n",
    "    ax2.add_feature(cfeature.COASTLINE, linewidth=0.2, edgecolor='black')\n",
    "    ax2.add_feature(cfeature.BORDERS, linewidth=0.2, edgecolor='black')        \n",
    "    ax3.add_feature(cfeature.COASTLINE, linewidth=0.2, edgecolor='black')\n",
    "    ax3.add_feature(cfeature.BORDERS, linewidth=0.2, edgecolor='black')        \n",
    "    ax4.add_feature(cfeature.COASTLINE, linewidth=0.2, edgecolor='black')\n",
    "    ax4.add_feature(cfeature.BORDERS, linewidth=0.2, edgecolor='black')                \n",
    "    ax5.add_feature(cfeature.COASTLINE, linewidth=0.2, edgecolor='black')\n",
    "    ax5.add_feature(cfeature.BORDERS, linewidth=0.2, edgecolor='black')        \n",
    "    ax6.add_feature(cfeature.COASTLINE, linewidth=0.2, edgecolor='black')\n",
    "    ax6.add_feature(cfeature.BORDERS, linewidth=0.2, edgecolor='black')                \n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        # This raises warnings since tight layout cannot\n",
    "        # handle gridspec automatically. We are going to\n",
    "        # do that manually so we can filter the warning.\n",
    "        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "        gs.tight_layout(fig, rect=[None,None,None,None])\n",
    "    \n",
    "    #gs.update(wspace=0.03, hspace=0.03)\n",
    "    path_out = r'D:\\Downloads\\Mattijn@Zhou\\DroughtCounty_PyWPS//_'\n",
    "    file_out = 'DroughtAlert_'+str(date.timetuple().tm_yday).zfill(3)+str(date.year).zfill(4)+'.png'\n",
    "    filepath = path_out+file_out \n",
    "    \n",
    "    #ram = cStringIO.StringIO()    \n",
    "    #fig.savefig(ram, dpi=200, pad_inches=1.,bbox_inches='tight')\n",
    "    #plt.close()    \n",
    "    #ram.seek(0)\n",
    "    #im = Image.open(ram)\n",
    "    #basewidth = 2000\n",
    "    #wpercent = (basewidth / float(im.size[0]))\n",
    "    #hsize = int((float(im.size[1]) * float(wpercent)))\n",
    "    #im = im.resize((basewidth, hsize), PIL.Image.ANTIALIAS)\n",
    "    #im.thumbnail((basewidth, hsize), PIL.Image.ANTIALIAS)\n",
    "    #im2 = im.convert('RGB').convert('P', palette=Image.WEB)\n",
    "    #im.save(filepath, format='PNG', quality=80,optimize=True)    \n",
    "    filepath = r'D:\\Data\\WorldShapefile//png20041015.png'\n",
    "    fig.savefig(filepath, dpi=200, bbox_inches='tight')\n",
    "    print path_out\n",
    "    #plt.show()        \n",
    "    #fig.clf()        \n",
    "    #plt.close()\n",
    "    #del record#,county\n",
    "    ram = None    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"C:\\Program Files\\GDAL//gdal_rasterize.exe\" -a DC20041015 -at -l world_simplified_CONCAT -tr 0.05 0.05 -ot Float32 -init 7 D:\\Data\\WorldShapefile\\world_simplified_CONCAT.shp D:\\Downloads\\Mattijn@Zhou\\GlobalDroughtProvince\\shp//DC20041015.tif\n"
     ]
    }
   ],
   "source": [
    "def rasterize(date_str = '20041015', in_shp = r'D:\\Data\\WorldShapefile\\world_simplified_CONCAT.shp'):\n",
    "    prefix = ['P0','P1','P2','P3','MEAN', 'DC']\n",
    "    folder = r'D:\\Downloads\\Mattijn@Zhou\\GlobalDroughtProvince\\tif//'\n",
    "    gdal_rasterize = r'C:\\Program Files\\GDAL//gdal_rasterize.exe'\n",
    "    for pre in prefix:\n",
    "        attribute  = pre + date_str\n",
    "        out_raster = folder + pre + date_str + '.tif'\n",
    "\n",
    "        command = [gdal_rasterize, '-a', attribute, '-at', '-l', 'world_simplified_CONCAT', \n",
    "                   '-tr', str(0.05),str(0.05), '-ot', float32,'-init',str(7), in_shp, out_raster]\n",
    "        #, \"-a\", attribute, \"-a_nodata value\", str(-1), out_filename, out_raster]\n",
    "\n",
    "        #logging.info(sp.list2cmdline(command))\n",
    "        print (sp.list2cmdline(command))\n",
    "\n",
    "        norm = sp.Popen(sp.list2cmdline(command),stdout=sp.PIPE, shell=True)\n",
    "        norm.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"C:\\Program Files\\GDAL//gdal_rasterize.exe\" -a MEAN -at -l world_simplified_CONCAT -tr 0.05 0.05 -ot Float32 -init 7 D:\\Data\\WorldShapefile\\world_simplified_CONCAT.shp D:\\Data\\WorldShapefile//MEAN20041015.tif\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('0...10...20...30...40...50...60...70...80...90...100 - done.\\r\\n', None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "out_raster = r'D:\\Data\\WorldShapefile//MEAN20041015.tif'\n",
    "out = r'D:\\out.tif'\n",
    "#gdalwarp = r'C:\\Program Files\\GDAL//gdal_rasterize.exe'\n",
    "gdal_rasterize = r'C:\\Program Files\\GDAL//gdal_rasterize.exe'\n",
    "P0 = 'P020041015'\n",
    "DC = 'DC20041015'\n",
    "P1 = 'P120041015'\n",
    "P2 = 'P220041015'\n",
    "P3 = 'P320041015'\n",
    "MEAN = 'MEAN'\n",
    "\n",
    "float32 = 'Float32'\n",
    "byte = 'Byte'\n",
    "\n",
    "#command = [\"/usr/bin/gdalwarp\",\"-cutline\",CHN_adm_gpkg,\"-csql\",\"SELECT NAME_3 FROM CHN_adm3 \n",
    "#WHERE NAME_1 = '\"+NAME_1+\"' and NAME_2 = '\"+NAME_2+\"' and NAME_3 = '\"+NAME_3+\"'\",\"-crop_to_cutline\",\n",
    "#\"-of\",\"GTiff\",\"-dstnodata\",\"-9999\",tmpfilename,clippedfilename, \"-overwrite\"]\n",
    "command = [gdal_rasterize, '-a', MEAN, '-at','-l', 'world_simplified_CONCAT', '-tr',str(0.05),str(0.05), \n",
    "           '-ot', float32,'-init',str(7),out_filename, out_raster]\n",
    "#, \"-a\", attribute, \"-a_nodata value\", str(-1), out_filename, out_raster]\n",
    "\n",
    "#logging.info(sp.list2cmdline(command))\n",
    "print (sp.list2cmdline(command))\n",
    "\n",
    "norm = sp.Popen(sp.list2cmdline(command),stdout=sp.PIPE, shell=True)\n",
    "norm.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def start():\n",
    "    #date='2015-06-30'\n",
    "    endpoint='http://192.168.1.104:8080/rasdaman/ows'\n",
    "    field={}\n",
    "    field['SERVICE']='WCS'\n",
    "    field['VERSION']='2.0.1'\n",
    "    field['REQUEST']='DescribeCoverage'\n",
    "    field['COVERAGEID']='NDVI_MOD13C1005_uptodate'#'NDVI_MOD13C1005'#'trmm_3b42_coverage_1'\n",
    "    url_values = urllib.urlencode(field,doseq=True)\n",
    "    full_url = endpoint + '?' + url_values\n",
    "    data = urllib.urlopen(full_url).read()\n",
    "    root = etree.fromstring(data)\n",
    "    lc = root.find(\".//{http://www.opengis.net/gml/3.2}lowerCorner\").text\n",
    "    uc = root.find(\".//{http://www.opengis.net/gml/3.2}upperCorner\").text\n",
    "    start_date=int((lc.split(' '))[2])\n",
    "    end_date=int((uc.split(' '))[2])\n",
    "    #print [start_date, end_date]\n",
    "\n",
    "    #generate the dates list \n",
    "    #cur_date=datetime.strptime(date,\"%Y-%m-%d\")\n",
    "    #startt=145775\n",
    "    start=datetime.fromtimestamp((start_date-(datetime(1970,01,01)-datetime(1601,01,01)).days)*24*60*60)\n",
    "    #print start\n",
    "\n",
    "    #tmp_date=datetime(start.year,cur_date.month,cur_date.day)\n",
    "    #if tmp_date > start :\n",
    "    #    start=(tmp_date-datetime(1601,01,01)).days\n",
    "    #else: start=(datetime(start.year+1,cur_date.month,cur_date.day)-datetime(1601,01,01)).days\n",
    "    #datelist=range(start+1,end_date-1,365)\n",
    "    #print datelist\n",
    "\n",
    "    #find the position of the requested date in the datelist\n",
    "    #cur_epoch=(cur_date-datetime(1601,01,01)).days\n",
    "    #cur_pos=min(range(len(datelist)),key=lambda x:abs(datelist[x]-cur_epoch))\n",
    "    #print ('Current position:',cur_pos)\n",
    "\n",
    "    try:    \n",
    "        # get sample size coefficients from XML root\n",
    "        sample_size = root[0][3][0][5][0][1].text #sample size\n",
    "        #print root[0][3][0][5][0][1].text #sample size\n",
    "\n",
    "        # use coverage start_date and sample_size array to create all dates in ANSI\n",
    "        array_stepsize = np.fromstring(sample_size, dtype=int, sep=' ')\n",
    "        #print np.fromstring(sample_size, dtype=int, sep=' ')\n",
    "        array_all_ansi = array_stepsize + start_date  \n",
    "        print 'irregular'\n",
    "        print array_all_ansi\n",
    "    except IndexError:\n",
    "        datelist, cur_pos = datelist_regular_coverage(root, start_date, start, cur_date)\n",
    "        print 'regular'\n",
    "        \n",
    "    # create array of all dates in ISO\n",
    "    list_all_dates = []\n",
    "    for stepsize in array_stepsize:\n",
    "        date_and_stepsize = start + timedelta(stepsize - 1)\n",
    "        list_all_dates.append(date_and_stepsize)\n",
    "        #print date_and_stepsize\n",
    "    array_all_dates = np.array(list_all_dates)   \n",
    "    \n",
    "    # create array of all dates in string\n",
    "    array_all_date_string = []\n",
    "    for i in array_all_dates:\n",
    "        date_string = str(i.year).zfill(2)+'-'+str(i.month).zfill(2)+'-'+str(i.day).zfill(2)\n",
    "        array_all_date_string.append(date_string)\n",
    "    array_all_date_string    \n",
    "\n",
    "    for date in array_all_date_string[100:101]:\n",
    "\n",
    "        #spl_arr = [70,30,80,50]\n",
    "        #extent = [73.5,140,14,53.6]\n",
    "        extent = [-179, 179, -60, 90]\n",
    "        spl_arr = [extent[0], extent[2], extent[1], extent[3]]\n",
    "        #ndai_wcs=_NDAI_CAL(date, spl_arr)\n",
    "        ndai_wcs = 'NDAI20041015.tif'\n",
    "        array = gdal.Open(ndai_wcs).ReadAsArray()\n",
    "        #band = raster.GetRasterBand(1)\n",
    "        #array = band.ReadAsArray()\n",
    "        #band.GetNoDataValue()\n",
    "\n",
    "        array_msk = np.ma.masked_equal(array,array.min())\n",
    "        #plt.imshow(array_msk)    \n",
    "\n",
    "        vector_path = r'D:\\Data\\WorldShapefile//world_simplified.shp'\n",
    "        raster_path = ndai_wcs\n",
    "        nodata_value = array.min()\n",
    "        # get date in format DOY+YEAR: eg. 0652011\n",
    "        # NDAI_2014_008.tif\n",
    "\n",
    "        year = int(ndai_wcs[-12:-8])\n",
    "        month = int(ndai_wcs[-8:-6])\n",
    "        day = int(ndai_wcs[-6:-4])\n",
    "        date = datetime(year,month,day)\n",
    "        try: \n",
    "            date_str = str(date.year)+str(date.month).zfill(2)+str(date.day).zfill(2)\n",
    "            print date_str\n",
    "        except:        \n",
    "            print date, ' aaahh'    \n",
    "\n",
    "        stats = zonal_stats(vector_path, raster_path, nodata_value, date)   \n",
    "\n",
    "        df_stats = pd.DataFrame(stats)\n",
    "        #df_stats.set_index('FID', inplace=True)\n",
    "        #print df_stats.head(2)\n",
    "\n",
    "        # read shapefile and concatate on index using a 'inner' join\n",
    "        # meaning counties without statistics info will be ignored\n",
    "        gdf = gpd.read_file(vector_path)\n",
    "        gdf.index.rename('FID', inplace=True)\n",
    "        gdf.reset_index(inplace=True)\n",
    "        frames  = [df_stats,gdf]\n",
    "        gdf_df_stats = gdf.merge(df_stats, on='FID')\n",
    "        gdf_df_stats.set_index('FID', inplace=True)\n",
    "\n",
    "        # get column names\n",
    "        ax1_head = gdf_df_stats.columns[10] # P00082014\n",
    "        ax2_head = gdf_df_stats.columns[11] # P10082014\n",
    "        ax3_head = gdf_df_stats.columns[12] # P20082014\n",
    "        ax4_head = gdf_df_stats.columns[13] # P30082014\n",
    "        ax5_head = gdf_df_stats.columns[8]  # MEAN\n",
    "        ax6_head = gdf_df_stats.columns[6]  # DC0082014\n",
    "        print ax1_head, ax2_head, ax3_head, ax4_head, ax5_head, ax6_head\n",
    "        columns_shp = [ax1_head, ax2_head, ax3_head, ax4_head, ax5_head, ax6_head]\n",
    "        # drop NaN values for axis\n",
    "        gdf_df_stats.dropna(inplace=True, subset=columns_shp)\n",
    "        #gdf_df_stats.head(50)\n",
    "        #gdf_df_stats = gpd.pd.concat(frames, axis=1, join='inner')\n",
    "        #gdf_df_stats.index.rename('FID', inplace=True)\n",
    "        #gdf_df_stats.geometry = gdf_df_stats.geometry.astype(gpd.geoseries.GeoSeries) # overcome bug \n",
    "        #gdf_df_stats.head(2) \n",
    "\n",
    "        # if necessary save to shapefile\n",
    "        out_filename = r'D:\\Data\\WorldShapefile//world_simplified_CONCAT.shp'\n",
    "        gdf_df_stats.to_file(out_filename)  \n",
    "\n",
    "        china_adm3 = out_filename\n",
    "        china_adm3_shp = shapereader.Reader(china_adm3)\n",
    "        \n",
    "        \n",
    "\n",
    "        #plot_map(china_adm3_shp, date, extent, columns_shp )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class Process(WPSProcess):\n",
    "\n",
    "#     def __init__(self):\n",
    "\n",
    "#         ##\n",
    "#         # Process initialization\n",
    "#         WPSProcess.__init__(self,\n",
    "#             identifier = \"WPS_NDAI_MAPS_FULL_ARCHIVE\",\n",
    "#             title=\"Automated Computation of Maps NDAI for FULL ARCHIVE\",\n",
    "#             abstract=\"\"\"This process intend to calculate the NDAI maps for everything\"\"\",\n",
    "#             version = \"1.0\",\n",
    "#             storeSupported = True,\n",
    "#             statusSupported = True)\n",
    "    \n",
    "#     ##\n",
    "#     # Execution part of the process\n",
    "#     def execute(self):\n",
    "#         #run\n",
    "#         start()\n",
    "\n",
    "#         return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date = datetime(2004,10,15)\n",
    "extent = [73.5,140,14,53.6] #minlon maxlon minlat maxlat\n",
    "extent = [-179,179,-60,90]\n",
    "columns_shp = ['P020041015', 'P120041015', 'P220041015', 'P320041015', 'MEAN', 'DC20041015']\n",
    "#columns_shp = ['P00082014','P10082014','P20082014','P30082014','MEAN','DC0082014']\n",
    "\n",
    "china_adm3 = r'D:\\Data\\WorldShapefile//world_simplified_CONCAT_elim.shp'\n",
    "china_adm3_shp = shapereader.Reader(china_adm3)\n",
    "\n",
    "plot_map(china_adm3_shp, date, extent, columns_shp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
