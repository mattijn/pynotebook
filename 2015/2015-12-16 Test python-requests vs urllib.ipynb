{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import requests\n",
    "import urllib\n",
    "from datetime import datetime, timedelta\n",
    "import numpy\n",
    "from lxml import etree\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datelist_regular_coverage(root, start_date, start, cur_date):\n",
    "    \"\"\"\n",
    "    retrieve regular datelist and requested current position in regards to total no. of observations\n",
    "    \"\"\"\n",
    "\n",
    "    #print start\n",
    "    tmp_date=datetime(start.year,cur_date.month,cur_date.day)\n",
    "    if tmp_date > start :\n",
    "        start=(tmp_date-datetime(1601,1,1)).days\n",
    "    else: start=(datetime(start.year+1,cur_date.month,cur_date.day)-datetime(1601,1,1)).days\n",
    "    datelist=range(start+1,end_date-1,365)\n",
    "    print datelist\n",
    "\n",
    "    #find the position of the requested date in the datelist\n",
    "    cur_epoch=(cur_date-datetime(1601,1,1)).days\n",
    "    cur_pos=min(range(len(datelist)),key=lambda x:abs(datelist[x]-cur_epoch))\n",
    "    print ('Current position:',cur_pos)    \n",
    "    \n",
    "    return datelist, cur_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datelist_irregular_coverage(root, start_date, start, cur_date):\n",
    "    \"\"\"\n",
    "    retrieve irregular datelist and requested current position in regards to total no. of observations\n",
    "    \"\"\"\n",
    "    \n",
    "    #root[0]                - wcs:CoverageDescription\n",
    "    #root[0][0]             - boundedBy \n",
    "    #root[0][0][0]          - Envelope\n",
    "    #root[0][0][0][0]       - lowerCorner\n",
    "    # --- \n",
    "    #root[0]                - wcs:CoverageDescription\n",
    "    #root[0][3]             - domainSet\n",
    "    #root[0][3][0]          - gmlrgrid:ReferenceableGridByVectors\n",
    "    #root[0][3][0][5]       - gmlrgrid:generalGridAxis\n",
    "    #root[0][3][0][5][0]    - gmlrgrid:GeneralGridAxis\n",
    "    #root[0][3][0][5][0][1] - gmlrgrid:coefficients\n",
    "\n",
    "    # get sample size coefficients from XML root\n",
    "    sample_size = root[0][3][0][5][0][1].text #sample size\n",
    "    #print root[0][3][0][5][0][1].text #sample size\n",
    "    \n",
    "    # use coverage start_date and sample_size array to create all dates in ANSI\n",
    "    array_stepsize = np.fromstring(sample_size, dtype=int, sep=' ')\n",
    "    #print np.fromstring(sample_size, dtype=int, sep=' ')\n",
    "    array_all_ansi = array_stepsize + start_date   \n",
    "    \n",
    "    # create array of all dates in ISO\n",
    "    list_all_dates = []\n",
    "    for stepsize in array_stepsize:\n",
    "        date_and_stepsize = start + timedelta(stepsize - 1)\n",
    "        list_all_dates.append(date_and_stepsize)\n",
    "        #print date_and_stepsize\n",
    "    array_all_dates = np.array(list_all_dates)  \n",
    "    \n",
    "    # create array of all dates as DOY\n",
    "    list_all_yday = []\n",
    "    for j in array_all_dates:\n",
    "        yday = j.timetuple().tm_yday\n",
    "        list_all_yday.append(yday)\n",
    "        #print yday\n",
    "    array_all_yday = np.array(list_all_yday)    \n",
    "    \n",
    "    # subtract user date of all dates in ISO \n",
    "    # to find the nearest available coverage date\n",
    "    array_diff_dates = array_all_dates - cur_date\n",
    "    idx_nearest_date = find_nearest(array_diff_dates, timedelta(0))\n",
    "    nearest_date = array_all_dates[idx_nearest_date]    \n",
    "    \n",
    "    # select all coresponding DOY of all years for ANSI and ISO dates\n",
    "    array_selected_ansi = array_all_ansi[array_all_yday == nearest_date.timetuple().tm_yday]\n",
    "    array_selected_dates = array_all_dates[array_all_yday == nearest_date.timetuple().tm_yday]\n",
    "    print array_selected_ansi\n",
    "    \n",
    "    # get index of nearest date in selection array\n",
    "    idx_nearest_date_selected = numpy.where(array_selected_dates==nearest_date)[0][0]  \n",
    "    print idx_nearest_date_selected\n",
    "    \n",
    "    # return datelist in ANSI and the index of the nearest date\n",
    "    return array_selected_ansi, idx_nearest_date_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_nearest(array,value):\n",
    "    return (np.abs(array-value)).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date = '2014-06-25'\n",
    "##request image cube for the specified date and area by WCS.\n",
    "#firstly we get the temporal length of avaliable NDVI data from the DescribeCoverage of WCS\n",
    "endpoint='http://192.168.1.104:8080/rasdaman/ows'\n",
    "field={}\n",
    "field['SERVICE']='WCS'\n",
    "field['VERSION']='2.0.1'\n",
    "field['REQUEST']='DescribeCoverage'\n",
    "field['COVERAGEID']='NDVI_MOD13C1005_uptodate'#'NDVI_MOD13C1005'#'trmm_3b42_coverage_1'\n",
    "url_values = urllib.urlencode(field,doseq=True)\n",
    "full_url = endpoint + '?' + url_values\n",
    "data = urllib.urlopen(full_url).read()\n",
    "root = etree.fromstring(data)\n",
    "lc = root.find(\".//{http://www.opengis.net/gml/3.2}lowerCorner\").text\n",
    "uc = root.find(\".//{http://www.opengis.net/gml/3.2}upperCorner\").text\n",
    "start_date=int((lc.split(' '))[2])\n",
    "end_date=int((uc.split(' '))[2])\n",
    "#print [start_date, end_date]\n",
    "\n",
    "#generate the dates list \n",
    "cur_date=datetime.strptime(date,\"%Y-%m-%d\")\n",
    "#startt=145775\n",
    "start=datetime.fromtimestamp((start_date-(datetime(1970,01,01)-datetime(1601,01,01)).days)*24*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[145908 146274 146639 147004 147369 147735 148100 148465 148830 149196\n",
      " 149561 149926 150291 150657 151022 151387]\n",
      "14\n",
      "irregular\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    datelist, cur_pos = datelist_irregular_coverage(root, start_date, start, cur_date)\n",
    "    print 'irregular'\n",
    "except IndexError:\n",
    "    datelist, cur_pos = datelist_regular_coverage(root, start_date, start, cur_date)\n",
    "    print 'regular'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spl_arr = [-179,-60,179,90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def urlretrieve(datelist,spl_arr):    \n",
    "    #retrieve the data cube\n",
    "    cube_arr=[]\n",
    "    for d in datelist[0:4]:\n",
    "        print 'NDVI: ', d        \n",
    "        field={}\n",
    "        field['SERVICE']='WCS'\n",
    "        field['VERSION']='2.0.1'\n",
    "        field['REQUEST']='GetCoverage'\n",
    "        field['COVERAGEID']='NDVI_MOD13C1005_uptodate'#'NDVI_MOD13C1005'#'trmm_3b42_coverage_1'\n",
    "        field['SUBSET']=['ansi('+str(d)+')',\n",
    "                         'Lat('+str(spl_arr[1])+','+str(spl_arr[3])+')',\n",
    "                        'Long('+str(spl_arr[0])+','+str(spl_arr[2])+')']\n",
    "        field['FORMAT']='image/tiff'\n",
    "        url_values = urllib.urlencode(field,doseq=True)\n",
    "        full_url = endpoint + '?' + url_values\n",
    "        #print full_url\n",
    "        tmpfilename='test'+str(d)+'.tif'\n",
    "        f,h = urllib.urlretrieve(full_url,tmpfilename)\n",
    "        #print h\n",
    "        ds=gdal.Open(tmpfilename)\n",
    "        cube_arr.append(ds.ReadAsArray())\n",
    "        #print d\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def requests_session(datelist,spl_arr):    \n",
    "    #retrieve the data cube\n",
    "    s = requests.Session()\n",
    "    cube_arr=[]\n",
    "    for d in datelist[0:4]:\n",
    "        print 'NDVI: ', d        \n",
    "        field={}\n",
    "        field['SERVICE']='WCS'\n",
    "        field['VERSION']='2.0.1'\n",
    "        field['REQUEST']='GetCoverage'\n",
    "        field['COVERAGEID']='NDVI_MOD13C1005_uptodate'#'NDVI_MOD13C1005'#'trmm_3b42_coverage_1'\n",
    "        field['SUBSET']=['ansi('+str(d)+')',\n",
    "                         'Lat('+str(spl_arr[1])+','+str(spl_arr[3])+')',\n",
    "                        'Long('+str(spl_arr[0])+','+str(spl_arr[2])+')']\n",
    "        field['FORMAT']='image/tiff'\n",
    "        url_values = urllib.urlencode(field,doseq=True)\n",
    "        full_url = endpoint + '?' + url_values\n",
    "        #print full_url\n",
    "        tmpfilename='test'+str(d)+'.tif'\n",
    "        #f,h = urllib.urlretrieve(full_url,tmpfilename)\n",
    "        \n",
    "        r = s.get(full_url, stream=True)\n",
    "        chunk_size = 16 * 1024\n",
    "        with open(tmpfilename, 'wb') as fd:\n",
    "            for chunk in r.iter_content(chunk_size):\n",
    "                fd.write(chunk)        \n",
    "        \n",
    "        #print h\n",
    "        ds=gdal.Open(tmpfilename)\n",
    "        cube_arr.append(ds.ReadAsArray())\n",
    "        #print d  \n",
    "    return cube_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def urllib2_chunk(datelist,spl_arr):    \n",
    "    #retrieve the data cube\n",
    "    #r = requests.Session()\n",
    "    cube_arr = []\n",
    "    for d in datelist[0:4]:\n",
    "        print 'NDVI: ', d        \n",
    "        field={}\n",
    "        field['SERVICE']='WCS'\n",
    "        field['VERSION']='2.0.1'\n",
    "        field['REQUEST']='GetCoverage'\n",
    "        field['COVERAGEID']='NDVI_MOD13C1005_uptodate'#'NDVI_MOD13C1005'#'trmm_3b42_coverage_1'\n",
    "        field['SUBSET']=['ansi('+str(d)+')',\n",
    "                         'Lat('+str(spl_arr[1])+','+str(spl_arr[3])+')',\n",
    "                        'Long('+str(spl_arr[0])+','+str(spl_arr[2])+')']\n",
    "        field['FORMAT']='image/tiff'\n",
    "        url_values = urllib.urlencode(field,doseq=True)\n",
    "        full_url = endpoint + '?' + url_values\n",
    "        urls.append(full_url)\n",
    "        #print full_url\n",
    "        tmpfilename='test'+str(d)+'.tif'\n",
    "        #f,h = urllib.urlretrieve(full_url,tmpfilename)\n",
    "        \n",
    "        response = urllib2.urlopen(full_url)\n",
    "        CHUNK = 256 * 1024\n",
    "        with open(tmpfilename, 'wb') as f:\n",
    "            while True:\n",
    "                chunk = response.read(CHUNK)\n",
    "                if not chunk: break\n",
    "                f.write(chunk)        \n",
    "        \n",
    "        #print h\n",
    "        ds=gdal.Open(tmpfilename)\n",
    "        cube_arr.append(ds.ReadAsArray())\n",
    "        #print d  \n",
    "    return cube_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit cube_arr =  urlretrieve(datelist,spl_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit cube_arr = requests_session(datelist,spl_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%timeit cube_arr = urllib2_chunk(datelist,spl_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDVI:  145908\n",
      "NDVI:  146274\n",
      "NDVI:  146639\n",
      "NDVI:  147004\n"
     ]
    }
   ],
   "source": [
    "cube_arr=[]\n",
    "urls = []\n",
    "names = []\n",
    "for d in datelist[0:4]:\n",
    "    print 'NDVI: ', d        \n",
    "    field={}\n",
    "    field['SERVICE']='WCS'\n",
    "    field['VERSION']='2.0.1'\n",
    "    field['REQUEST']='GetCoverage'\n",
    "    field['COVERAGEID']='NDVI_MOD13C1005_uptodate'#'NDVI_MOD13C1005'#'trmm_3b42_coverage_1'\n",
    "    field['SUBSET']=['ansi('+str(d)+')',\n",
    "                     'Lat('+str(spl_arr[1])+','+str(spl_arr[3])+')',\n",
    "                    'Long('+str(spl_arr[0])+','+str(spl_arr[2])+')']\n",
    "    field['FORMAT']='image/tiff'\n",
    "    url_values = urllib.urlencode(field,doseq=True)\n",
    "    full_url = endpoint + '?' + url_values\n",
    "    urls.append(full_url)\n",
    "    #print full_url\n",
    "    tmpfilename='test'+str(d)+'.tif'\n",
    "    names.append(tmpfilename)\n",
    "    #f,h = urllib.urlretrieve(full_url,tmpfilename)\n",
    "    #print h\n",
    "    #ds=gdal.Open(tmpfilename)\n",
    "    #cube_arr.append(ds.ReadAsArray())\n",
    "    #print d  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get(ix,u):\n",
    "    r = grequests.get(u, stream=True)\n",
    "    chunk_size = 16 * 1024\n",
    "    tmpfilename = names[ix]\n",
    "    with open(tmpfilename, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size):\n",
    "            fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rs = (get(ix,u) for ix, u in enumerate(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grequests.map(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grequests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urlparse import urlparse\n",
    "from threading import Thread\n",
    "import httplib, sys\n",
    "from Queue import Queue\n",
    "\n",
    "concurrent = 200\n",
    "cube_arr=[]\n",
    "def doWork():\n",
    "    \n",
    "    s = requests.Session()    \n",
    "    while True:\n",
    "        url = q.get()\n",
    "        status, url = getStatus(url)\n",
    "        doSomethingWithResult(status, url,s, cube_arr)        \n",
    "        q.task_done()\n",
    "    return cube_arr\n",
    "\n",
    "def getStatus(ourl):\n",
    "    try:\n",
    "        url = urlparse(ourl)\n",
    "        conn = httplib.HTTPConnection(url.netloc)   \n",
    "        conn.request(\"HEAD\", url.path)\n",
    "        res = conn.getresponse()\n",
    "        return res.status, ourl\n",
    "    except:\n",
    "        return \"error\", ourl\n",
    "\n",
    "def doSomethingWithResult(status, url,s,cube_arr):\n",
    "    urls = ['http://192.168.1.104:8080/rasdaman/ows?SUBSET=ansi%28145908%29&SUBSET=Lat%28-60%2C90%29&SUBSET=Long%28-179%2C179%29&SERVICE=WCS&FORMAT=image%2Ftiff&REQUEST=GetCoverage&VERSION=2.0.1&COVERAGEID=NDVI_MOD13C1005_uptodate',\n",
    " 'http://192.168.1.104:8080/rasdaman/ows?SUBSET=ansi%28146274%29&SUBSET=Lat%28-60%2C90%29&SUBSET=Long%28-179%2C179%29&SERVICE=WCS&FORMAT=image%2Ftiff&REQUEST=GetCoverage&VERSION=2.0.1&COVERAGEID=NDVI_MOD13C1005_uptodate',\n",
    " 'http://192.168.1.104:8080/rasdaman/ows?SUBSET=ansi%28146639%29&SUBSET=Lat%28-60%2C90%29&SUBSET=Long%28-179%2C179%29&SERVICE=WCS&FORMAT=image%2Ftiff&REQUEST=GetCoverage&VERSION=2.0.1&COVERAGEID=NDVI_MOD13C1005_uptodate',\n",
    " 'http://192.168.1.104:8080/rasdaman/ows?SUBSET=ansi%28147004%29&SUBSET=Lat%28-60%2C90%29&SUBSET=Long%28-179%2C179%29&SERVICE=WCS&FORMAT=image%2Ftiff&REQUEST=GetCoverage&VERSION=2.0.1&COVERAGEID=NDVI_MOD13C1005_uptodate']\n",
    "    tmpfilename = names[urls.index(url)]\n",
    "    print status, url\n",
    "    r = s.get(full_url, stream=True)\n",
    "    chunk_size = 16 * 1024\n",
    "    print tmpfilename, '\\n'#tmpfilename = names[ix]\n",
    "    with open(tmpfilename, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size):\n",
    "            fd.write(chunk)\n",
    "    cube_arr.append(gdal.Open(tmpfilename).ReadAsArray())\n",
    "    \n",
    "\n",
    "q = Queue(concurrent * 2)\n",
    "for i in range(concurrent):\n",
    "    t = Thread(target=doWork)\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "try:\n",
    "    for url in urls:\n",
    "        q.put(url.strip())\n",
    "    q.join()\n",
    "except KeyboardInterrupt:\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cube_arr_ma=ma.masked_equal(numpy.asarray(cube_arr),-3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(cube_arr[1] - cube_arr[0])\n",
    "plt.show()\n",
    "# plt.imshow(cube_arr[1])\n",
    "# plt.show()\n",
    "# plt.imshow(cube_arr[2])\n",
    "# plt.show()\n",
    "# plt.imshow(cube_arr[3])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(cube_arr[1] == cube_arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy.ma as ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls.index('http://192.168.1.104:8080/rasdaman/ows?SUBSET=ansi%28147004%29&SUBSET=Lat%28-60%2C90%29&SUBSET=Long%28-179%2C179%29&SERVICE=WCS&FORMAT=image%2Ftiff&REQUEST=GetCoverage&VERSION=2.0.1&COVERAGEID=NDVI_MOD13C1005_uptodate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import eventlet\n",
    "from eventlet.green import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('opening url: ', 0)\n",
      "('opening url: ', 3)\n",
      "('opening url: ', 2)\n",
      "('opening url: ', 1)\n",
      "('done with url: ', 1)\n",
      "('done with url: ', 2)\n",
      "('done with url: ', 3)\n",
      "('done with url: ', 0)\n",
      "('finished url', 0)\n",
      "('finished url', 1)\n",
      "('finished url', 2)\n",
      "('finished url', 3)\n"
     ]
    }
   ],
   "source": [
    "ixs = range(len(urls))\n",
    "cube_arr = []\n",
    "\n",
    "def fetch(url, ix):\n",
    "    print \"opening url: \", ix\n",
    "    \n",
    "    response = urllib2.urlopen(url)\n",
    "    # Create in-memory file and initialize it with the content\n",
    "    gdal.FileFromMemBuffer('/vsimem/tiffinmem', response.read())\n",
    "    # Open the in-memory file\n",
    "    ds = gdal.Open('/vsimem/tiffinmem')\n",
    "    assert ds    \n",
    "    cube_arr.append(ds.ReadAsArray())\n",
    "    out = url     \n",
    "    print \"done with url: \", ix\n",
    "    return ix\n",
    "\n",
    "# create pool of threads\n",
    "pool = eventlet.GreenPool(200)\n",
    "\n",
    "# start farming jobs\n",
    "for ix in pool.imap(fetch, urls, ixs):\n",
    "    print \"finished url\", ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cube_arr_ma=ma.masked_equal(numpy.asarray(cube_arr),-3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "masked_array(data =\n",
       " [[-- -- -- ..., -- -- --]\n",
       " [-- -- -- ..., -- -- --]\n",
       " [-- -- -- ..., -- -- --]\n",
       " ..., \n",
       " [-- -- -- ..., -- -- --]\n",
       " [-- -- -- ..., -- -- --]\n",
       " [-- -- -- ..., -- -- --]],\n",
       "             mask =\n",
       " [[ True  True  True ...,  True  True  True]\n",
       " [ True  True  True ...,  True  True  True]\n",
       " [ True  True  True ...,  True  True  True]\n",
       " ..., \n",
       " [ True  True  True ...,  True  True  True]\n",
       " [ True  True  True ...,  True  True  True]\n",
       " [ True  True  True ...,  True  True  True]],\n",
       "       fill_value = -3000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube_arr_ma[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import urllib\n",
    "import urllib2\n",
    "import gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/mattijn/pynotebook/master/idata/ano_DOY2002170.tif'\n",
    "tmp_filename1 = 'tmp1.tif'\n",
    "tmp_filename2 = 'tmp2.tif'\n",
    "tmp_filename3 = 'tmp3.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content = urllib2.urlopen(url)\n",
    "chunk_size = 16 * 1024\n",
    "with open(tmp_filename1, 'wb') as f:\n",
    "    while True:\n",
    "        chunk = content.read(chunk_size)\n",
    "        if not chunk: break\n",
    "        f.write(chunk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = gdal.Open(tmp_filename1)\n",
    "assert ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "content = urllib2.urlopen(url)\n",
    "output = open(tmp_filename,'wb')\n",
    "output.write(content.read())\n",
    "output.close()\n",
    "\n",
    "#urllib.urlretrieve(url,filename_tmp)\n",
    "#ds = gdal.Open(filename_tmp)\n",
    "\n",
    "#assert ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content = open(tmp_filename1, mode='rb').read()\n",
    "# Create in-memory file and initialize it with the content\n",
    "gdal.FileFromMemBuffer('/vsimem/tiffinmem', content)\n",
    "# Open the in-memory file\n",
    "ds = gdal.Open('/vsimem/tiffinmem')\n",
    "assert ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content = urllib2.urlopen(url)\n",
    "gdal.FileFromMemBuffer('/vsimem/tiffinmem', content.read())\n",
    "# Open the in-memory file\n",
    "ds = gdal.Open('/vsimem/tiffinmem')\n",
    "assert ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65535.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.ReadAsArray()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = urllib2.urlopen(urls[0])\n",
    "gdal.FileFromMemBuffer('/vsimem/tiffinmem', content.read())\n",
    "# Open the in-memory file\n",
    "ds = gdal.Open('/vsimem/tiffinmem')\n",
    "assert ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdal.FileFromMemBuffer('tif_in_memory', response.read())\n",
    "ds = gdal.Open('tif_in_memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = r'D:\\Data\\NDAI\\NDAI_2014//NDAI_2014_008.tif'\n",
    "content = open(test_file, mode='rb').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = urllib3.connection_from_url('http://192.168.1.104:8080/rasdaman/ows?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "r = http.request(\"GET\", 'https://raw.githubusercontent.com/mattijn/pynotebook/master/idata/ano_DOY2002170.tif')\n",
    "r.getheader(\"transfer-encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdal.FileFromMemBuffer('tiffinmem2', r.data)\n",
    "ds = gdal.Open('tiffinmem2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for chunk in r.stream():\n",
    "    print chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('arh.tif', 'wb') as fp:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdal.FileFromMemBuffer('tiffinmem2', r1.data)\n",
    "ds = gdal.Open('tiffinmem2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_arr = gdal.Open(names[1]).ReadAsArray() - gdal.Open(names[0]).ReadAsArray()\n",
    "plt.imshow(new_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib2\n",
    "import math\n",
    "\n",
    "def downloadChunks(url):\n",
    "    \"\"\"Helper to download large files\n",
    "        the only arg is a url\n",
    "       this file will go to a temp directory\n",
    "       the file will also be downloaded\n",
    "       in chunks and print out how much remains\n",
    "    \"\"\"\n",
    "\n",
    "    baseFile = os.path.basename(url)\n",
    "\n",
    "    #move the file to a more uniq path\n",
    "    os.umask(0002)\n",
    "    temp_path = \"/tmp/\"\n",
    "    try:\n",
    "        file = os.path.join(temp_path,baseFile)\n",
    "\n",
    "        req = urllib2.urlopen(url)\n",
    "        total_size = int(req.info().getheader('Content-Length').strip())\n",
    "        downloaded = 0\n",
    "        CHUNK = 256 * 10240\n",
    "        with open(file, 'wb') as fp:\n",
    "            while True:\n",
    "                chunk = req.read(CHUNK)\n",
    "                downloaded += len(chunk)\n",
    "                print math.floor( (downloaded / total_size) * 100 )\n",
    "                if not chunk: break\n",
    "                fp.write(chunk)\n",
    "    except urllib2.HTTPError, e:\n",
    "        print \"HTTP Error:\",e.code , url\n",
    "        return False\n",
    "    except urllib2.URLError, e:\n",
    "        print \"URL Error:\",e.reason , url\n",
    "        return False\n",
    "\n",
    "    return file\n",
    "    \n",
    "#use it like this\n",
    "#downloadChunks(\"http://localhost/a.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileout = downloadChunks(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "to_mem = urllib2.urlopen(full_url)\n",
    "to_mem.read()[3::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tifinxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get(full_url)\n",
    "r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdal.FileFromMemBuffer('tiffinmem2', r.content)\n",
    "#ds = gdal.Open('tiffinmem2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "req = urllib2.urlopen(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_in = tmpfilename\n",
    "content = open(file_in, mode='rb').read()\n",
    "gdal.FileFromMemBuffer('/vsimem/tiffinmem', content)\n",
    "ds = gdal.Open('/vsimem/tiffinmem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content = None\n",
    "ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content == r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "open('tiffinmem2', mode='rb').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in req.info():\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdal.FileFromMemBuffer('tifinmem', output.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds = gdal.Open('tiffinmem')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "from cStringIO import StringIO\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "#s = requests.Session()\n",
    "#s.get(full_url)\n",
    "r = requests.get(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = gdal.Open(r.content)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(full_url, stream=True)\n",
    "chunk_size = 16 * 1024\n",
    "with open(tmpfilename, 'wb') as fd:\n",
    "    for chunk in r.iter_content(chunk_size):\n",
    "        fd.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = gdal.Open(tmpfilename)\n",
    "ds.ReadAsArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import grequests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
