{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import requests\n",
    "import urllib\n",
    "from datetime import datetime, timedelta\n",
    "import numpy\n",
    "from lxml import etree\n",
    "from osgeo import gdal\n",
    "import eventlet\n",
    "from eventlet.green import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datelist_regular_coverage(root, start_date, start, cur_date):\n",
    "    \"\"\"\n",
    "    retrieve regular datelist and requested current position in regards to total no. of observations\n",
    "    \"\"\"\n",
    "\n",
    "    #print start\n",
    "    tmp_date=datetime(start.year,cur_date.month,cur_date.day)\n",
    "    if tmp_date > start :\n",
    "        start=(tmp_date-datetime(1601,1,1)).days\n",
    "    else: start=(datetime(start.year+1,cur_date.month,cur_date.day)-datetime(1601,1,1)).days\n",
    "    datelist=range(start+1,end_date-1,365)\n",
    "    print datelist\n",
    "\n",
    "    #find the position of the requested date in the datelist\n",
    "    cur_epoch=(cur_date-datetime(1601,1,1)).days\n",
    "    cur_pos=min(range(len(datelist)),key=lambda x:abs(datelist[x]-cur_epoch))\n",
    "    print ('Current position:',cur_pos)    \n",
    "    \n",
    "    return datelist, cur_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datelist_irregular_coverage(root, start_date, start, cur_date):\n",
    "    \"\"\"\n",
    "    retrieve irregular datelist and requested current position in regards to total no. of observations\n",
    "    \"\"\"\n",
    "    \n",
    "    #root[0]                - wcs:CoverageDescription\n",
    "    #root[0][0]             - boundedBy \n",
    "    #root[0][0][0]          - Envelope\n",
    "    #root[0][0][0][0]       - lowerCorner\n",
    "    # --- \n",
    "    #root[0]                - wcs:CoverageDescription\n",
    "    #root[0][3]             - domainSet\n",
    "    #root[0][3][0]          - gmlrgrid:ReferenceableGridByVectors\n",
    "    #root[0][3][0][5]       - gmlrgrid:generalGridAxis\n",
    "    #root[0][3][0][5][0]    - gmlrgrid:GeneralGridAxis\n",
    "    #root[0][3][0][5][0][1] - gmlrgrid:coefficients\n",
    "\n",
    "    # get sample size coefficients from XML root\n",
    "    sample_size = root[0][3][0][5][0][1].text #sample size\n",
    "    #print root[0][3][0][5][0][1].text #sample size\n",
    "    \n",
    "    # use coverage start_date and sample_size array to create all dates in ANSI\n",
    "    array_stepsize = np.fromstring(sample_size, dtype=int, sep=' ')\n",
    "    #print np.fromstring(sample_size, dtype=int, sep=' ')\n",
    "    array_all_ansi = array_stepsize + start_date   \n",
    "    \n",
    "    # create array of all dates in ISO\n",
    "    list_all_dates = []\n",
    "    for stepsize in array_stepsize:\n",
    "        date_and_stepsize = start + timedelta(stepsize - 1)\n",
    "        list_all_dates.append(date_and_stepsize)\n",
    "        #print date_and_stepsize\n",
    "    array_all_dates = np.array(list_all_dates)  \n",
    "    \n",
    "    # create array of all dates as DOY\n",
    "    list_all_yday = []\n",
    "    for j in array_all_dates:\n",
    "        yday = j.timetuple().tm_yday\n",
    "        list_all_yday.append(yday)\n",
    "        #print yday\n",
    "    array_all_yday = np.array(list_all_yday)    \n",
    "    \n",
    "    # subtract user date of all dates in ISO \n",
    "    # to find the nearest available coverage date\n",
    "    array_diff_dates = array_all_dates - cur_date\n",
    "    idx_nearest_date = find_nearest(array_diff_dates, timedelta(0))\n",
    "    nearest_date = array_all_dates[idx_nearest_date]    \n",
    "    \n",
    "    # select all coresponding DOY of all years for ANSI and ISO dates\n",
    "    array_selected_ansi = array_all_ansi[array_all_yday == nearest_date.timetuple().tm_yday]\n",
    "    array_selected_dates = array_all_dates[array_all_yday == nearest_date.timetuple().tm_yday]\n",
    "    print array_selected_ansi\n",
    "    \n",
    "    # get index of nearest date in selection array\n",
    "    idx_nearest_date_selected = numpy.where(array_selected_dates==nearest_date)[0][0]  \n",
    "    print idx_nearest_date_selected\n",
    "    \n",
    "    # return datelist in ANSI and the index of the nearest date\n",
    "    return array_selected_ansi, idx_nearest_date_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_nearest(array,value):\n",
    "    return (np.abs(array-value)).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunkify(lst,n):\n",
    "     return [ lst[i::n] for i in xrange(n) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date = '2014-06-25'\n",
    "##request image cube for the specified date and area by WCS.\n",
    "#firstly we get the temporal length of avaliable NDVI data from the DescribeCoverage of WCS\n",
    "endpoint='http://192.168.1.104:8080/rasdaman/ows'\n",
    "field={}\n",
    "field['SERVICE']='WCS'\n",
    "field['VERSION']='2.0.1'\n",
    "field['REQUEST']='DescribeCoverage'\n",
    "field['COVERAGEID']='NDVI_MOD13C1005_uptodate'#'NDVI_MOD13C1005'#'trmm_3b42_coverage_1'\n",
    "url_values = urllib.urlencode(field,doseq=True)\n",
    "full_url = endpoint + '?' + url_values\n",
    "data = urllib.urlopen(full_url).read()\n",
    "root = etree.fromstring(data)\n",
    "lc = root.find(\".//{http://www.opengis.net/gml/3.2}lowerCorner\").text\n",
    "uc = root.find(\".//{http://www.opengis.net/gml/3.2}upperCorner\").text\n",
    "start_date=int((lc.split(' '))[2])\n",
    "end_date=int((uc.split(' '))[2])\n",
    "#print [start_date, end_date]\n",
    "\n",
    "#generate the dates list \n",
    "cur_date=datetime.strptime(date,\"%Y-%m-%d\")\n",
    "#startt=145775\n",
    "start=datetime.fromtimestamp((start_date-(datetime(1970,01,01)-datetime(1601,01,01)).days)*24*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[145908 146274 146639 147004 147369 147735 148100 148465 148830 149196\n",
      " 149561 149926 150291 150657 151022 151387]\n",
      "14\n",
      "irregular\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    datelist, cur_pos = datelist_irregular_coverage(root, start_date, start, cur_date)\n",
    "    print 'irregular'\n",
    "except IndexError:\n",
    "    datelist, cur_pos = datelist_regular_coverage(root, start_date, start, cur_date)\n",
    "    print 'regular'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spl_arr = [-179,-60,179,90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cube_arr=[]\n",
    "urls = []\n",
    "for d in datelist:\n",
    "    print 'NDVI: ', d        \n",
    "    field={}\n",
    "    field['SERVICE']='WCS'\n",
    "    field['VERSION']='2.0.1'\n",
    "    field['REQUEST']='GetCoverage'\n",
    "    field['COVERAGEID']='NDVI_MOD13C1005_uptodate'#'NDVI_MOD13C1005'#'trmm_3b42_coverage_1'\n",
    "    field['SUBSET']=['ansi('+str(d)+')',\n",
    "                     'Lat('+str(spl_arr[1])+','+str(spl_arr[3])+')',\n",
    "                    'Long('+str(spl_arr[0])+','+str(spl_arr[2])+')']\n",
    "    field['FORMAT']='image/tiff'\n",
    "    url_values = urllib.urlencode(field,doseq=True)\n",
    "    full_url = endpoint + '?' + url_values\n",
    "    urls.append(full_url)\n",
    "    #print full_url\n",
    "ixs = range(len(urls))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch(url, ix):\n",
    "    print \"opening url: \", ix\n",
    "    \n",
    "    response = urllib2.urlopen(url)\n",
    "    # Create in-memory file and initialize it with the content\n",
    "    gdal.FileFromMemBuffer('/vsimem/tiffinmem', response.read())\n",
    "    # Open the in-memory file\n",
    "    ds = gdal.Open('/vsimem/tiffinmem')\n",
    "    assert ds    \n",
    "    cube_arr.append(ds.ReadAsArray())\n",
    "    out = url     \n",
    "    print \"done with url: \", ix\n",
    "    return ix\n",
    "\n",
    "# create pool of threads\n",
    "pool = eventlet.GreenPool(200)\n",
    "\n",
    "for urls_chunk in chunkify(urls,5):\n",
    "    # start farming jobs\n",
    "    for ix in pool.imap(fetch, urls_chunk, datelist):\n",
    "        print \"finished url\", ix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def fetch(url, ix):\n",
    "#     print \"opening url: \", ix\n",
    "    \n",
    "#     response = urllib2.urlopen(url)\n",
    "#     # Create in-memory file and initialize it with the content\n",
    "#     gdal.FileFromMemBuffer('/vsimem/tiffinmem', response.read())\n",
    "#     # Open the in-memory file\n",
    "#     ds = gdal.Open('/vsimem/tiffinmem')\n",
    "#     assert ds    \n",
    "#     cube_arr.append(ds.ReadAsArray())\n",
    "#     out = url     \n",
    "#     print \"done with url: \", ix\n",
    "#     return ix\n",
    "\n",
    "# # create pool of threads\n",
    "# pool = eventlet.GreenPool(200)\n",
    "\n",
    "# # start farming jobs\n",
    "# for ix in pool.imap(fetch, urls, ixs):\n",
    "#     print \"finished url\", ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
