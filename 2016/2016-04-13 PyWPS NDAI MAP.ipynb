{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# # NDAI calculation process\n",
    "# This process intend to calculate the Vegetation Condition index (VCI) for a specific area. The fomula of the index is:\n",
    "# NDAI =a*NVAI-(1-a)*NTAI, where a is 0.5\n",
    "# where the NDAI is Normalized Droguht Anomaly Index.\n",
    "# This is a WPS process served by PyWPS.\n",
    "#\n",
    "# Input:\n",
    "# bBox:a rectangle box which specifies the processing area.\n",
    "# date: a date string specifies the date to be calculated. The date format should be \"YYYY-MM-DD\".\n",
    "#\n",
    "# Output:\n",
    "# file:\n",
    "# format:\n",
    "#\n",
    "# The process internally retrieves NDVI data set from a rasdaman database.\n",
    "#\n",
    "# Client side execute script:\n",
    "# http://localhost/cgi-bin/wpswsgi?service=wps&version=1.0.0&request=execute&identifier=WPS_NDAI_DI_CAL&datainputs=[date=2005-02-06;bbox=50,10,120,60]&responsedocument=image=@asReference=true\n",
    "\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "#from pywps.Process import WPSProcess \n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "from lxml import etree\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import numpy\n",
    "import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.io.shapereader import Reader\n",
    "import subprocess as sp\n",
    "%matplotlib inline\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def GetCoverageNames():\n",
    "    file_json = r'D:\\Downloads\\Mattijn@Zhou\\coverages_names.json'\n",
    "    with open(file_json) as json_data:\n",
    "        d = json.load(json_data)\n",
    "    _CoverageID_NDVI = d['COVG_NAME_NDVI_MOD13C1005']\n",
    "    _CoverageID_LST  = d['COVG_NAME_LST_MOD11C2005']\n",
    "    return _CoverageID_NDVI, _CoverageID_LST\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def datelist_regular_coverage(root, start_date, start, cur_date):\n",
    "    \"\"\"\n",
    "    retrieve regular datelist and requested current position in regards to total no. of observations\n",
    "    \"\"\"\n",
    "\n",
    "    #print start\n",
    "    tmp_date=datetime(start.year,cur_date.month,cur_date.day)\n",
    "    if tmp_date > start :\n",
    "        start=(tmp_date-datetime(1601,1,1)).days\n",
    "    else: start=(datetime(start.year+1,cur_date.month,cur_date.day)-datetime(1601,1,1)).days\n",
    "    datelist=range(start+1,end_date-1,365)\n",
    "    print datelist\n",
    "    logging.info(datelist)\n",
    "\n",
    "    #find the position of the requested date in the datelist\n",
    "    cur_epoch=(cur_date-datetime(1601,1,1)).days\n",
    "    cur_pos=min(range(len(datelist)),key=lambda x:abs(datelist[x]-cur_epoch))\n",
    "    print ('Current position:',cur_pos)    \n",
    "    \n",
    "    return datelist, cur_pos\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def datelist_irregular_coverage(root, start_date, start, cur_date):\n",
    "    \"\"\"\n",
    "    retrieve irregular datelist and requested current position in regards to total no. of observations\n",
    "    \"\"\"\n",
    "    \n",
    "    #root[0]                - wcs:CoverageDescription\n",
    "    #root[0][0]             - boundedBy \n",
    "    #root[0][0][0]          - Envelope\n",
    "    #root[0][0][0][0]       - lowerCorner\n",
    "    # --- \n",
    "    #root[0]                - wcs:CoverageDescription\n",
    "    #root[0][3]             - domainSet\n",
    "    #root[0][3][0]          - gmlrgrid:ReferenceableGridByVectors\n",
    "    #root[0][3][0][5]       - gmlrgrid:generalGridAxis\n",
    "    #root[0][3][0][5][0]    - gmlrgrid:GeneralGridAxis\n",
    "    #root[0][3][0][5][0][1] - gmlrgrid:coefficients\n",
    "\n",
    "    # get sample size coefficients from XML root\n",
    "    sample_size = root[0][3][0][5][0][1].text #sample size\n",
    "    #print root[0][3][0][5][0][1].text #sample size\n",
    "    \n",
    "    # use coverage start_date and sample_size array to create all dates in ANSI\n",
    "    array_stepsize = np.fromstring(sample_size, dtype=int, sep=' ')\n",
    "    #print np.fromstring(sample_size, dtype=int, sep=' ')\n",
    "    array_all_ansi = array_stepsize + start_date   \n",
    "    \n",
    "    # create array of all dates in ISO\n",
    "    list_all_dates = []\n",
    "    for stepsize in array_stepsize:\n",
    "        date_and_stepsize = start + timedelta(stepsize - 1)\n",
    "        list_all_dates.append(date_and_stepsize)\n",
    "        #print date_and_stepsize\n",
    "    array_all_dates = np.array(list_all_dates)  \n",
    "    \n",
    "    # create array of all dates as DOY\n",
    "    list_all_yday = []\n",
    "    for j in array_all_dates:\n",
    "        yday = j.timetuple().tm_yday\n",
    "        list_all_yday.append(yday)\n",
    "        #print yday\n",
    "    array_all_yday = np.array(list_all_yday)    \n",
    "    \n",
    "    # subtract user date of all dates in ISO \n",
    "    # to find the nearest available coverage date\n",
    "    array_diff_dates = array_all_dates - cur_date\n",
    "    idx_nearest_date = find_nearest(array_diff_dates, timedelta(0))\n",
    "    nearest_date = array_all_dates[idx_nearest_date]    \n",
    "    \n",
    "    # select all coresponding DOY of all years for ANSI and ISO dates\n",
    "    array_selected_ansi = array_all_ansi[array_all_yday == nearest_date.timetuple().tm_yday]\n",
    "    array_selected_dates = array_all_dates[array_all_yday == nearest_date.timetuple().tm_yday]\n",
    "    print array_selected_ansi\n",
    "    logging.info(array_selected_ansi)\n",
    "    \n",
    "    # get index of nearest date in selection array\n",
    "    idx_nearest_date_selected = numpy.where(array_selected_dates==nearest_date)[0][0]  \n",
    "    print idx_nearest_date_selected\n",
    "    \n",
    "    # return datelist in ANSI and the index of the nearest date\n",
    "    return array_selected_ansi, idx_nearest_date_selected\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def find_nearest(array,value):\n",
    "    return (np.abs(array-value)).argmin()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "def _NVAI_CAL(date,spl_arr,CoverageID_NDVI):\n",
    "\n",
    "    #request image cube for the specified date and area by WCS.\n",
    "    #firstly we get the temporal length of avaliable NDVI data from the DescribeCoverage of WCS\n",
    "    endpoint='http://192.168.1.104:8080/rasdaman/ows'\n",
    "    field={}\n",
    "    field['SERVICE']='WCS'\n",
    "    field['VERSION']='2.0.1'\n",
    "    field['REQUEST']='DescribeCoverage'\n",
    "    field['COVERAGEID']=CoverageID_NDVI#'NDVI_MOD13C1005_uptodate'#'NDVI_MOD13C1005'#'trmm_3b42_coverage_1'\n",
    "    url_values = urllib.urlencode(field,doseq=True)\n",
    "    full_url = endpoint + '?' + url_values\n",
    "    data = urllib.urlopen(full_url).read()\n",
    "    root = etree.fromstring(data)\n",
    "    lc = root.find(\".//{http://www.opengis.net/gml/3.2}lowerCorner\").text\n",
    "    uc = root.find(\".//{http://www.opengis.net/gml/3.2}upperCorner\").text\n",
    "    start_date=int((lc.split(' '))[2])\n",
    "    end_date=int((uc.split(' '))[2])\n",
    "    #print [start_date, end_date]\n",
    "\n",
    "    #generate the dates list \n",
    "    cur_date=datetime.strptime(date,\"%Y-%m-%d\")\n",
    "    #startt=145775\n",
    "    start=datetime.fromtimestamp((start_date-(datetime(1970,01,01)-datetime(1601,01,01)).days)*24*60*60)\n",
    "    #print start\n",
    "\n",
    "    #tmp_date=datetime(start.year,cur_date.month,cur_date.day)\n",
    "    #if tmp_date > start :\n",
    "    #    start=(tmp_date-datetime(1601,01,01)).days\n",
    "    #else: start=(datetime(start.year+1,cur_date.month,cur_date.day)-datetime(1601,01,01)).days\n",
    "    #datelist=range(start+1,end_date-1,365)\n",
    "    #print datelist\n",
    "\n",
    "    #find the position of the requested date in the datelist\n",
    "    #cur_epoch=(cur_date-datetime(1601,01,01)).days\n",
    "    #cur_pos=min(range(len(datelist)),key=lambda x:abs(datelist[x]-cur_epoch))\n",
    "    #print ('Current position:',cur_pos)\n",
    "\n",
    "    try:    \n",
    "        datelist, cur_pos = datelist_irregular_coverage(root, start_date, start, cur_date)\n",
    "        print 'irregular'\n",
    "    except IndexError:\n",
    "        datelist, cur_pos = datelist_regular_coverage(root, start_date, start, cur_date)\n",
    "        print 'regular'\n",
    "\n",
    "    #retrieve the data cube\n",
    "    cube_arr=[]\n",
    "    for d in datelist:\n",
    "        print 'NDVI', d\n",
    "        field={}\n",
    "        field['SERVICE']='WCS'\n",
    "        field['VERSION']='2.0.1'\n",
    "        field['REQUEST']='GetCoverage'\n",
    "        field['COVERAGEID']=CoverageID_NDVI#'NDVI_MOD13C1005_uptodate'#'NDVI_MOD13C1005'#'trmm_3b42_coverage_1'\n",
    "        field['SUBSET']=['ansi('+str(d)+')',\n",
    "                         'Lat('+str(spl_arr[1])+','+str(spl_arr[3])+')',\n",
    "                        'Long('+str(spl_arr[0])+','+str(spl_arr[2])+')']\n",
    "        field['FORMAT']='image/tiff'\n",
    "        url_values = urllib.urlencode(field,doseq=True)\n",
    "        full_url = endpoint + '?' + url_values\n",
    "        #print full_url\n",
    "        tmpfilename='test'+str(d)+'.tif'\n",
    "        f,h = urllib.urlretrieve(full_url,tmpfilename)\n",
    "        #print h\n",
    "        ds=gdal.Open(tmpfilename)\n",
    "\n",
    "        cube_arr.append(ds.ReadAsArray())\n",
    "        #print d\n",
    "\n",
    "    #calculate the regional VCI\n",
    "    cube_arr_ma=ma.masked_equal(numpy.asarray(cube_arr),-3000)\n",
    "    NVAI=(cube_arr_ma[cur_pos,:,:]-numpy.mean(cube_arr_ma,0))*1.0/(numpy.amax(cube_arr_ma,0)-numpy.amin(cube_arr_ma,0))\n",
    "    \n",
    "    #VCI *= 1000\n",
    "    #VCI //= (1000 - 0 + 1) / 256.\n",
    "    #VCI = VCI.astype(numpy.uint8) \n",
    "    return NVAI,ds\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def _NTAI_CAL(date,spl_arr, CoverageID_LST):\n",
    "\n",
    "    #request image cube for the specified date and area by WCS.\n",
    "    #firstly we get the temporal length of avaliable NDVI data from the DescribeCoverage of WCS\n",
    "    endpoint='http://192.168.1.104:8080/rasdaman/ows'\n",
    "    field={}\n",
    "    field['SERVICE']='WCS'\n",
    "    field['VERSION']='2.0.1'\n",
    "    field['REQUEST']='DescribeCoverage'\n",
    "    field['COVERAGEID']=CoverageID_LST#'LST_MOD11C2005_uptodate'#'LST_MOD11C2005'#'trmm_3b42_coverage_1'\n",
    "    url_values = urllib.urlencode(field,doseq=True)\n",
    "    full_url = endpoint + '?' + url_values\n",
    "    data = urllib.urlopen(full_url).read()\n",
    "    root = etree.fromstring(data)\n",
    "    lc = root.find(\".//{http://www.opengis.net/gml/3.2}lowerCorner\").text\n",
    "    uc = root.find(\".//{http://www.opengis.net/gml/3.2}upperCorner\").text\n",
    "    start_date=int((lc.split(' '))[2])\n",
    "    end_date=int((uc.split(' '))[2])\n",
    "    #print [start_date, end_date]\n",
    "\n",
    "    #generate the dates list \n",
    "    cur_date=datetime.strptime(date,\"%Y-%m-%d\")\n",
    "    #startt=145775\n",
    "    start=datetime.fromtimestamp((start_date-(datetime(1970,01,01)-datetime(1601,01,01)).days)*24*60*60)\n",
    "    #print start\n",
    "\n",
    "    #tmp_date=datetime(start.year,cur_date.month,cur_date.day)\n",
    "    #if tmp_date > start :\n",
    "    #    start=(tmp_date-datetime(1601,01,01)).days\n",
    "    #else: start=(datetime(start.year+1,cur_date.month,cur_date.day)-datetime(1601,01,01)).days\n",
    "    #datelist=range(start+1,end_date-1,365)\n",
    "    #print datelist\n",
    "\n",
    "    #find the position of the requested date in the datelist\n",
    "    #cur_epoch=(cur_date-datetime(1601,01,01)).days\n",
    "    #cur_pos=min(range(len(datelist)),key=lambda x:abs(datelist[x]-cur_epoch))\n",
    "    #print ('Current position:',cur_pos)\n",
    "\n",
    "    try:    \n",
    "        datelist, cur_pos = datelist_irregular_coverage(root, start_date, start, cur_date)\n",
    "        print 'irregular'\n",
    "        logging.info('irregular')\n",
    "    except IndexError:\n",
    "        datelist, cur_pos = datelist_regular_coverage(root, start_date, start, cur_date)\n",
    "        print 'regular'\n",
    "        logging.info('regular')\n",
    "        \n",
    "    #retrieve the data cube\n",
    "    cube_arr=[]\n",
    "    for d in datelist:\n",
    "        print 'LST', d\n",
    "        field={}\n",
    "        field['SERVICE']='WCS'\n",
    "        field['VERSION']='2.0.1'\n",
    "        field['REQUEST']='GetCoverage'\n",
    "        field['COVERAGEID']=CoverageID_LST#'LST_MOD11C2005_uptodate'#'LST_MOD11C2005'#'trmm_3b42_coverage_1'\n",
    "        field['SUBSET']=['ansi('+str(d)+')',\n",
    "                         'Lat('+str(spl_arr[1])+','+str(spl_arr[3])+')',\n",
    "                        'Long('+str(spl_arr[0])+','+str(spl_arr[2])+')']\n",
    "        field['FORMAT']='image/tiff'\n",
    "        url_values = urllib.urlencode(field,doseq=True)\n",
    "        full_url = endpoint + '?' + url_values\n",
    "        #print full_url\n",
    "        tmpfilename='test'+str(d)+'.tif'\n",
    "        f,h = urllib.urlretrieve(full_url,tmpfilename)\n",
    "        #print h\n",
    "        ds=gdal.Open(tmpfilename)\n",
    "\n",
    "        cube_arr.append(ds.ReadAsArray())\n",
    "        #print d\n",
    "\n",
    "    #calculate the regional VCI\n",
    "    cube_arr_ma=ma.masked_equal(numpy.asarray(cube_arr),0) #nan val is 0 for lst\n",
    "    #VCI=(cube_arr_ma[cur_pos,:,:]-numpy.amin(cube_arr_ma,0))*1.0/(numpy.amax(cube_arr_ma,0)-numpy.amin(cube_arr_ma,0))\n",
    "    NTAI=(cube_arr_ma[cur_pos,:,:]-numpy.mean(cube_arr_ma,0))*1.0/(numpy.amax(cube_arr_ma,0)-numpy.amin(cube_arr_ma,0))\n",
    "    \n",
    "    return NTAI, cur_date\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "def _NDAI_CAL(date,spl_arr,alpha = 0.5):\n",
    "\n",
    "    CoverageID_NDVI, CoverageID_LST = GetCoverageNames()\n",
    "    \n",
    "    NTAI, cur_date = _NTAI_CAL(date, spl_arr, CoverageID_LST)\n",
    "    NVAI, ds = _NVAI_CAL(date, spl_arr, CoverageID_NDVI)\n",
    "    NDAI = (alpha * NVAI ) - ((1-alpha) * NTAI)\n",
    "    \n",
    "    NDAI_255 = NDAI.copy()\n",
    "    NDAI_255 += 1\n",
    "    NDAI_255 *= 1000\n",
    "    NDAI_255 //= (2000 - 0 + 1) / 255. #instead of 256\n",
    "    NDAI_255 = NDAI_255.astype(numpy.uint8)\n",
    "    NDAI_255 += 1 #so mask values are reserverd for 0\n",
    "    NDAI_255.fill_value = 0    \n",
    "    try:        \n",
    "        NDAI_min = numpy.min(NDAI_255[~NDAI_255.mask])\n",
    "        NDAI_255[ma.getmaskarray(NDAI)] = 0\n",
    "    except ValueError:\n",
    "        #only masked values set full array to 0 values\n",
    "        NDAI_255 = numpy.zeros(NDAI_255.shape)\n",
    "\n",
    "    #write the result VCI to disk\n",
    "    # get parameters\n",
    "    geotransform = ds.GetGeoTransform()\n",
    "    spatialreference = ds.GetProjection()\n",
    "    ncol = ds.RasterXSize\n",
    "    nrow = ds.RasterYSize\n",
    "    nband = 1\n",
    "\n",
    "    # create dictionary metadata for datetime\n",
    "    dict = {}\n",
    "    dict['TIFFTAG_DATETIME'] = cur_date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # create dataset for output\n",
    "    fmt = 'GTiff'\n",
    "    ndaiFileName = 'NDAI'+cur_date.strftime(\"%Y%m%d\")+'.tif'\n",
    "    driver = gdal.GetDriverByName(fmt)\n",
    "    dst_dataset = driver.Create(ndaiFileName, ncol, nrow, nband, gdal.GDT_Byte)\n",
    "    dst_dataset.SetGeoTransform(geotransform)\n",
    "    dst_dataset.SetProjection(spatialreference)\n",
    "    dst_dataset.SetMetadata(dict)# = cur_date.strftime(\"%Y%m%d\")\n",
    "    dst_dataset.GetRasterBand(1).SetNoDataValue(int(NDAI_255.fill_value))\n",
    "    dst_dataset.GetRasterBand(1).WriteArray(NDAI_255)\n",
    "    dst_dataset = None\n",
    "    return ndaiFileName, NDAI_255   \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# class Process(WPSProcess):\n",
    "\n",
    "\n",
    "#     def __init__(self):\n",
    "\n",
    "#         #\n",
    "#         # Process initialization\n",
    "#         WPSProcess.__init__(self,\n",
    "#             identifier       = \"WPS_NDAI_DI_CAL\",\n",
    "#             title            = \"NDAI calculation process\",\n",
    "#             abstract         = \"\"\"\n",
    "#                                This process intend to calculate the \n",
    "#                                Normalized Drought Anomaly Index (NDAI) \n",
    "#                                for a specific area..\n",
    "#                                \"\"\",\n",
    "#             version          = \"1.0\",\n",
    "#             storeSupported   = True,\n",
    "#             statusSupported  = True)\n",
    "\n",
    "#         #\n",
    "#         # Adding process inputs\n",
    "\n",
    "#         self.boxIn = self.addBBoxInput(      identifier   = \"bbox\",\n",
    "#                                              title        = \"Spatial region\")\n",
    "\n",
    "#         self.dateIn = self.addLiteralInput(  identifier   = \"date\",\n",
    "#                                              title        = \"The date to be calcualted\",\n",
    "#                                              type         = type(''))\n",
    "\n",
    "#         #\n",
    "#         # Adding process outputs\n",
    "\n",
    "#         self.dataOut = self.addComplexOutput(identifier   = \"map\",\n",
    "#                                              title        = \"Output NDAI image\",\n",
    "#                                              useMapscript = True,\n",
    "#                                              formats      = [{'mimeType':'image/tiff'}])\n",
    "\n",
    "#     #\n",
    "#     # Execution part of the process\n",
    "#     def execute(self):\n",
    "\n",
    "#         # Get the box value\n",
    "#         BBOXObject = self.boxIn.getValue()\n",
    "#         CoordTuple = BBOXObject.coords\n",
    "\n",
    "#         #Get the date string\n",
    "#         date = self.dateIn.getValue()\n",
    "\n",
    "#         logging.info(CoordTuple)\n",
    "#         logging.info(date)\n",
    "\n",
    "#         #date='2013-06-30'\n",
    "#         #spl_arr=[70,30,80,50]\n",
    "#         spl_arr=[CoordTuple[0][0],CoordTuple[0][1],CoordTuple[1][0],CoordTuple[1][1]]\n",
    "\n",
    "#         logging.info(date)\n",
    "#         logging.info(spl_arr)\n",
    "\n",
    "#         ndaifn=_NDAI_CAL(date,spl_arr)\n",
    "#         self.dataOut.setValue( ndaifn )\n",
    "#         #self.textOut.setValue( self.textIn.getValue() )\n",
    "#         #os.remove(vcifn)\n",
    "#         logging.info(os.getcwd())\n",
    "#         return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_colormap(seq):\n",
    "    \"\"\"Return a LinearSegmentedColormap\n",
    "    seq: a sequence of floats and RGB-tuples. The floats should be increasing\n",
    "    and in the interval (0,1).\n",
    "    \"\"\"\n",
    "    seq = [(None,) * 3, 0.0] + list(seq) + [1.0, (None,) * 3]\n",
    "    cdict = {'red': [], 'green': [], 'blue': []}\n",
    "    for i, item in enumerate(seq):\n",
    "        if isinstance(item, float):\n",
    "            r1, g1, b1 = seq[i - 1]\n",
    "            r2, g2, b2 = seq[i + 1]\n",
    "            cdict['red'].append([item, r1, r2])\n",
    "            cdict['green'].append([item, g1, g2])\n",
    "            cdict['blue'].append([item, b1, b2])\n",
    "    return mcolors.LinearSegmentedColormap('CustomMap', cdict)\n",
    "c = mcolors.ColorConverter().to_rgb\n",
    "\n",
    "def cmap_discretize(cmap, N):\n",
    "    \"\"\"Return a discrete colormap from the continuous colormap cmap.\n",
    "    \n",
    "        cmap: colormap instance, eg. cm.jet. \n",
    "        N: number of colors.\n",
    "    \n",
    "    Example\n",
    "        x = resize(arange(100), (5,100))\n",
    "        djet = cmap_discretize(cm.jet, 5)\n",
    "        imshow(x, cmap=djet)\n",
    "    \"\"\"\n",
    "\n",
    "    if type(cmap) == str:\n",
    "        cmap = get_cmap(cmap)\n",
    "    colors_i = np.concatenate((np.linspace(0, 1., N), (0.,0.,0.,0.)))\n",
    "    colors_rgba = cmap(colors_i)\n",
    "    indices = np.linspace(0, 1., N+1)\n",
    "    cdict = {}\n",
    "    for ki,key in enumerate(('red','green','blue')):\n",
    "        cdict[key] = [ (indices[i], colors_rgba[i-1,ki], colors_rgba[i,ki]) for i in xrange(N+1) ]\n",
    "    # Return colormap object.\n",
    "    return matplotlib.colors.LinearSegmentedColormap(cmap.name + \"_%d\"%N, cdict, 1024)\n",
    "\n",
    "def setMap(rasterBase):\n",
    "\n",
    "    # Read the data and metadata\n",
    "    ds = gdal.Open(rasterBase)\n",
    "    #band = ds.GetRasterBand(20)\n",
    "    \n",
    "    data = ds.ReadAsArray()\n",
    "    gt = ds.GetGeoTransform()\n",
    "    #proj = ds.GetProjection()\n",
    "    \n",
    "    nan = ds.GetRasterBand(1).GetNoDataValue()\n",
    "    if nan != None:\n",
    "        data = np.ma.masked_equal(data,value=nan)\n",
    "    \n",
    "    xres = gt[1]\n",
    "    yres = gt[5]\n",
    "    \n",
    "    # get the edge coordinates and add half the resolution \n",
    "    # to go to center coordinates\n",
    "    xmin = gt[0] + xres * 0.5\n",
    "    xmax = gt[0] + (xres * ds.RasterXSize) - xres * 0.5\n",
    "    ymin = gt[3] + (yres * ds.RasterYSize) + yres * 0.5\n",
    "    ymax = gt[3] - yres * 0.5\n",
    "    \n",
    "    x = ds.RasterXSize \n",
    "    y = ds.RasterYSize  \n",
    "    extent = [ gt[0],gt[0]+x*gt[1], gt[3],gt[3]+y*gt[5]]\n",
    "    #ds = None\n",
    "    img_extent = (extent[0], extent[1], extent[2], extent[3])\n",
    "    \n",
    "    # create a grid of xy coordinates in the original projection\n",
    "    #xy_source = np.mgrid[xmin:xmax+xres:xres, ymax+yres:ymin:yres]\n",
    "    \n",
    "    return extent, img_extent#, xy_source, proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def MapThatShit(NDAI_BASE,date,spl_arr,drought_avg_tci_cmap):\n",
    "    # get array informaiton\n",
    "    extent, img_extent = setMap(NDAI_BASE)\n",
    "    ds = gdal.Open(NDAI_BASE)\n",
    "    array = ds.ReadAsArray()\n",
    "    #array[ma.getmaskarray(NDAI)] = 0\n",
    "    #nan = ds.GetRasterBand(1).GetNoDataValue()\n",
    "    array = ma.masked_equal(array, 0)\n",
    "    array = np.flipud(array)\n",
    "    logging.info(extent)\n",
    "\n",
    "    # set shape of figure\n",
    "    width = array.shape[0]\n",
    "    height = array.shape[1]\n",
    "    base_width = 10.\n",
    "    base_height = (base_width * height) / width\n",
    "    print base_width, base_height\n",
    "    logging.info(base_width, base_height)\n",
    "\n",
    "    # figure\n",
    "    fig = plt.figure(figsize=(base_height,base_width)) \n",
    "    ax = plt.subplot(1,1,1, projection=ccrs.PlateCarree())\n",
    "    ax.set_extent(extent, ccrs.Geodetic())\n",
    "    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                      linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "    gl.ylabels_right = False\n",
    "    gl.xlabels_top = False\n",
    "    # prulletaria on the map\n",
    "    \n",
    "    ogr2ogr = r'C:\\Program Files\\GDAL//ogr2ogr.exe'\n",
    "    base_geom = r'D:\\Data\\ChinaShapefile\\natural_earth'\n",
    "    # ocean\n",
    "    in_file_ocean = base_geom + '\\physical//10m_ocean.shp'    \n",
    "    outfile_ocean = 'ocean.shp'\n",
    "    # country boudaries\n",
    "    in_file_bound = base_geom + '\\cultural//10m_admin_0_boundary_lines_land.shp' \n",
    "    outfile_bound = 'boundaries.shp'\n",
    "    # costlie\n",
    "    in_file_coast = base_geom + '\\physical//10m_coastline.shp'\n",
    "    outfile_coast = 'coastline.shp'\n",
    "    #lakes\n",
    "    in_file_lakes = base_geom + '\\physical//10m_lakes.shp'\n",
    "    outfile_lakes = 'lakes.shp'\n",
    "    # run the clip functions\n",
    "    \n",
    "    command = [ogr2ogr, '-f', \"ESRI Shapefile\", outfile_ocean, in_file_ocean,'-clipsrc',\n",
    "               str(spl_arr[0]),str(spl_arr[1]),str(spl_arr[2]),str(spl_arr[3]),'-overwrite']    \n",
    "    print (sp.list2cmdline(command))\n",
    "    norm = sp.Popen(sp.list2cmdline(command),stdout=sp.PIPE, shell=True)\n",
    "    norm.communicate()\n",
    "    \n",
    "    command = [ogr2ogr, '-f', \"ESRI Shapefile\", outfile_bound, in_file_bound,'-clipsrc',\n",
    "               str(spl_arr[0]),str(spl_arr[1]),str(spl_arr[2]),str(spl_arr[3]),'-overwrite']    \n",
    "    print (sp.list2cmdline(command))\n",
    "    norm = sp.Popen(sp.list2cmdline(command),stdout=sp.PIPE, shell=True)\n",
    "    norm.communicate()    \n",
    "    \n",
    "    command = [ogr2ogr, '-f', \"ESRI Shapefile\", outfile_coast, in_file_coast,'-clipsrc',\n",
    "               str(spl_arr[0]),str(spl_arr[1]),str(spl_arr[2]),str(spl_arr[3]),'-overwrite']    \n",
    "    print (sp.list2cmdline(command))\n",
    "    norm = sp.Popen(sp.list2cmdline(command),stdout=sp.PIPE, shell=True)\n",
    "    norm.communicate()    \n",
    "    \n",
    "    command = [ogr2ogr, '-f', \"ESRI Shapefile\", outfile_lakes, in_file_lakes,'-clipsrc',\n",
    "               str(spl_arr[0]),str(spl_arr[1]),str(spl_arr[2]),str(spl_arr[3]),'-overwrite']    \n",
    "    print (sp.list2cmdline(command))\n",
    "    norm = sp.Popen(sp.list2cmdline(command),stdout=sp.PIPE, shell=True)\n",
    "    norm.communicate()        \n",
    "\n",
    "    ax.add_geometries(Reader(outfile_ocean).geometries(), ccrs.PlateCarree(), facecolor='lightblue')\n",
    "    ax.add_geometries(Reader(outfile_bound).geometries(), ccrs.PlateCarree(), \n",
    "                      facecolor='',linestyle=':', linewidth=2)\n",
    "    ax.add_geometries(Reader(outfile_coast).geometries(), ccrs.PlateCarree(), facecolor='')\n",
    "    ax.add_geometries(Reader(outfile_lakes).geometries(), ccrs.PlateCarree(), facecolor='lightskyblue')\n",
    "    \n",
    "    # ticks of classes\n",
    "    bounds = [   0.   ,   82.875,   95.625,  108.375,  127.5  ,  146.625,\n",
    "            159.375,  172.125,  255.   ]\n",
    "    # ticklabels plus colorbar\n",
    "    ticks = ['-1','-0.35','-0.25','-0.15','+0','+.15','+.25','+.35','+1']\n",
    "    cmap = cmap_discretize(drought_avg_tci_cmap,8)\n",
    "    norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    im = ax.imshow(array, origin='upper', extent=img_extent,norm=norm, cmap=cmap, vmin=0, vmax=255, interpolation='nearest')#, transform=ccrs.Mercator())\n",
    "    title = 'NDAI '+date\n",
    "    plt.title(title, fontsize=22)\n",
    "    cb = plt.colorbar(im, fraction=0.0476, pad=0.04, ticks=bounds,norm=norm, orientation='horizontal')\n",
    "    cb.set_label('Normalized Drought Anomaly Index')\n",
    "    cb.set_ticklabels(ticks)\n",
    "    \n",
    "    spl_arr_str = str(spl_arr)\n",
    "    spl_arr_str = spl_arr_str.replace('[','')\n",
    "    spl_arr_str = spl_arr_str.replace(']','')\n",
    "    spl_arr_str = spl_arr_str.replace(', ','#')\n",
    "    spl_arr_str = spl_arr_str.replace('.','.')\n",
    "    # and save the shit\n",
    "    outpath = 'NDAI_'+date+'_'+spl_arr_str+'.png'\n",
    "    plt.savefig(outpath, dpi=200, bbox_inches='tight')\n",
    "    print outpath\n",
    "    logging.info(outpath)\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    fig.clf() \n",
    "    return outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tci_cmap = make_colormap([c('#F29813'), c('#D8DC44'),0.2, c('#D8DC44'), c('#7EC5AD'),0.4, c('#7EC5AD'), c('#5786BE'),0.6, \n",
    "#                          c('#5786BE'), c('#41438D'),0.8, c('#41438D')])\n",
    "\n",
    "drought_avg_tci_cmap = make_colormap([c('#993406'), c('#D95E0E'),0.1, c('#D95E0E'), c('#FE9829'),0.2, \n",
    "                                      c('#FE9829'), c('#FFD98E'),0.3, c('#FFD98E'), c('#FEFFD3'),0.4, \n",
    "                                      c('#FEFFD3'), c('#C4DC73'),0.5, c('#C4DC73'), c('#93C83D'),0.6,\n",
    "                                      c('#93C83D'), c('#69BD45'),0.7, c('#69BD45'), c('#6ECCDD'),0.8,\n",
    "                                      c('#6ECCDD'), c('#3553A4'),0.9, c('#3553A4')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date='2015-05-01'\n",
    "#spl_arr=[73.2,-9.5,140.9,53.9]\n",
    "spl_arr = [91.8,-14.3,164.4,13] #minlon, minlat, maxlon, maxlat\n",
    "#spl_arr = [0.3,48.41,8.66,56.46]\n",
    "#spl_arr = [-69.6,-12.8,-33.6,-0.2]\n",
    "#spl_arr=[CoordTuple[0][0],CoordTuple[0][1],CoordTuple[1][0],CoordTuple[1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[145844 146210 146575 146940 147305 147671 148036 148401 148766 149132\n",
      " 149497 149862 150227 150593 150958 151323]\n",
      "15\n",
      "irregular\n",
      "LST 145844\n",
      "LST 146210\n",
      "LST 146575\n",
      "LST 146940\n",
      "LST 147305\n",
      "LST 147671\n",
      "LST 148036\n",
      "LST 148401\n",
      "LST 148766\n",
      "LST 149132\n",
      "LST 149497\n",
      "LST 149862\n",
      "LST 150227\n",
      "LST 150593\n",
      "LST 150958\n",
      "LST 151323\n",
      "[145844 146210 146575 146940 147305 147671 148036 148401 148766 149132\n",
      " 149497 149862 150227 150593 150958 151323]\n",
      "15\n",
      "irregular\n",
      "NDVI 145844\n",
      "NDVI 146210\n",
      "NDVI 146575\n",
      "NDVI 146940\n",
      "NDVI 147305\n",
      "NDVI 147671\n",
      "NDVI 148036\n",
      "NDVI 148401\n",
      "NDVI 148766\n",
      "NDVI 149132\n",
      "NDVI 149497\n",
      "NDVI 149862\n",
      "NDVI 150227\n",
      "NDVI 150593\n",
      "NDVI 150958\n",
      "NDVI 151323\n"
     ]
    }
   ],
   "source": [
    "NDAI_BASE, NDAI_255 =_NDAI_CAL(date,spl_arr)\n",
    "#outpath = MapThatShit(NDAI_BASE,date,spl_arr,drought_avg_tci_cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_url = 'http://192.168.1.104/wps/wpsoutputs/map-fd5a09d0-a221-11e5-883c-4437e647de9f.tif'\n",
    "tif = urllib.urlretrieve(full_url, 'tif_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = gdal.Open(NDAI_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2016-15-10'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.GetMetadata()['TIFFTAG_DATETIME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([162.9, 180.0, -32.35, -48.9], (162.9, 180.0, -32.35, -48.9))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setMap('tif_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spl_arr_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##write the result VCI to disk\n",
    "# get parameters\n",
    "geotransform = ds.GetGeoTransform()\n",
    "spatialreference = ds.GetProjection()\n",
    "ncol = ds.RasterXSize\n",
    "nrow = ds.RasterYSize\n",
    "nband = 1\n",
    "\n",
    "trans = ds.GetGeoTransform()\n",
    "extent = (trans[0], trans[0] + ds.RasterXSize*trans[1],\n",
    "  trans[3] + ds.RasterYSize*trans[5], trans[3])\n",
    "\n",
    "# Create figure\n",
    "fig = plt.imshow(NDAI, cmap=cmap_discretize(drought_avg_tci_cmap,8), vmin=-1, vmax=1, extent=extent)#vmin=-0.4, vmax=0.4\n",
    "plt.colorbar(fig)\n",
    "plt.axis('off')\n",
    "#plt.colorbar()\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bounds = np.array([0,6,12,24,36,48,60,72,84,100])\n",
    "(bounds * 255 / 100 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "tci_cmap = make_colormap([c('#F29813'), c('#D8DC44'),0.2, c('#D8DC44'), c('#7EC5AD'),0.4, c('#7EC5AD'), c('#5786BE'),0.6, \n",
    "                          c('#5786BE'), c('#41438D'),0.8, c('#41438D')])\n",
    "\n",
    "drought_avg_tci_cma2 = make_colormap([c('#993406'), c('#D95E0E'),0.1, c('#D95E0E'), c('#FE9829'),0.2, \n",
    "                                      c('#FE9829'), c('#FFD98E'),0.3, c('#FFD98E'), c('#FEFFD3'),0.4, \n",
    "                                      c('#FEFFD3'), c('#C4DC73'),0.5, c('#C4DC73'), c('#93C83D'),0.6,\n",
    "                                      c('#93C83D'), c('#69BD45'),0.7, c('#69BD45'), c('#6ECCDD'),0.8,\n",
    "                                      c('#6ECCDD'), c('#3553A4'),0.9, c('#3553A4')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##write the result VCI to disk\n",
    "# get parameters\n",
    "geotransform = ds.GetGeoTransform()\n",
    "spatialreference = ds.GetProjection()\n",
    "ncol = ds.RasterXSize\n",
    "nrow = ds.RasterYSize\n",
    "nband = 1\n",
    "\n",
    "trans = ds.GetGeoTransform()\n",
    "extent = (trans[0], trans[0] + ds.RasterXSize*trans[1],\n",
    "  trans[3] + ds.RasterYSize*trans[5], trans[3])\n",
    "\n",
    "# Create figure\n",
    "fig = plt.imshow(NDAI_255, cmap=cmap_discretize(drought_avg_tci_cma2,8), vmin=0, vmax=255, extent=extent)#vmin=-0.4, vmax=0.4\n",
    "plt.colorbar(fig)\n",
    "plt.axis('off')\n",
    "#plt.colorbar()\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(NDAI_255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(NDAI.compressed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bounds = (np.array([-1,-0.35,-0.25,-0.15,0,.15,.25,.35,1])+1)*(255/2.)\n",
    "#\n",
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bounds /= 2\n",
    "bounds = (bounds * 255 / 100)\n",
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cb = cmap_discretize(drought_avg_tci_cma2,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(NDAI_255.compressed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
