{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from pywps.Process import WPSProcess \n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import urllib2\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from cStringIO import StringIO\n",
    "import jdcal\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[62]:\n",
    "\n",
    "\n",
    "# In[49]:\n",
    "def GetCoverageNames():\n",
    "    file_json = r'/var/www/html/wps/pywps/processes/coverages_names.json'\n",
    "    with open(file_json) as json_data:\n",
    "        d = json.load(json_data)\n",
    "    _CoverageID_NDVI = d['COVG_NAME_NDVI_MOD13C1005']\n",
    "    _CoverageID_LST  = d['COVG_NAME_LST_MOD11C2005']\n",
    "    return _CoverageID_NDVI, _CoverageID_LST\n",
    "\n",
    "# Computing diagonal for each row of a 2d array. See: http://stackoverflow.com/q/27214027/2459096\n",
    "def makediag3d(M):\n",
    "    b = np.zeros((M.shape[0], M.shape[1] * M.shape[1]))\n",
    "    b[:, ::M.shape[1] + 1] = M\n",
    "    \n",
    "    logging.info('function `makediag3d` complete')    \n",
    "    return b.reshape(M.shape[0], M.shape[1], M.shape[1]) \n",
    "\n",
    "def get_starter_matrix(base_period_len, sample_count, frequencies_considered_count):\n",
    "    nr = min(2 * frequencies_considered_count + 1,\n",
    "                  sample_count)  # number of 2*+1 frequencies, or number of input images\n",
    "    mat = np.zeros(shape=(nr, sample_count))\n",
    "    mat[0, :] = 1\n",
    "    ang = 2 * np.pi * np.arange(base_period_len) / base_period_len\n",
    "    cs = np.cos(ang)\n",
    "    sn = np.sin(ang)\n",
    "    # create some standard sinus and cosinus functions and put in matrix\n",
    "    i = np.arange(1, frequencies_considered_count + 1)\n",
    "    ts = np.arange(sample_count)\n",
    "    for column in xrange(sample_count):\n",
    "        index = np.mod(i * ts[column], base_period_len)\n",
    "        # index looks like 000, 123, 246, etc, until it wraps around (for len(i)==3)\n",
    "        mat[2 * i - 1, column] = cs.take(index)\n",
    "        mat[2 * i, column] = sn.take(index)\n",
    "\n",
    "    logging.info('function `get_starter_matrix` complete')\n",
    "    return mat\n",
    "\n",
    "def HANTS(sample_count, inputs,\n",
    "          frequencies_considered_count=3,\n",
    "          outliers_to_reject='Lo',\n",
    "          low=0., high=255,\n",
    "          fit_error_tolerance=5,\n",
    "          delta=0.1):\n",
    "    \"\"\"\n",
    "    Function to apply the Harmonic analysis of time series applied to arrays\n",
    "\n",
    "    sample_count    = nr. of images (total number of actual samples of the time series)\n",
    "    base_period_len    = length of the base period, measured in virtual samples\n",
    "            (days, dekads, months, etc.)\n",
    "    frequencies_considered_count    = number of frequencies to be considered above the zero frequency\n",
    "    inputs     = array of input sample values (e.g. NDVI values)\n",
    "    ts    = array of size sample_count of time sample indicators\n",
    "            (indicates virtual sample number relative to the base period);\n",
    "            numbers in array ts maybe greater than base_period_len\n",
    "            If no aux file is used (no time samples), we assume ts(i)= i,\n",
    "            where i=1, ..., sample_count\n",
    "    outliers_to_reject  = 2-character string indicating rejection of high or low outliers\n",
    "            select from 'Hi', 'Lo' or 'None'\n",
    "    low   = valid range minimum\n",
    "    high  = valid range maximum (values outside the valid range are rejeced\n",
    "            right away)\n",
    "    fit_error_tolerance   = fit error tolerance (points deviating more than fit_error_tolerance from curve\n",
    "            fit are rejected)\n",
    "    dod   = degree of overdeterminedness (iteration stops if number of\n",
    "            points reaches the minimum required for curve fitting, plus\n",
    "            dod). This is a safety measure\n",
    "    delta = small positive number (e.g. 0.1) to suppress high amplitudes\n",
    "    \"\"\"\n",
    "\n",
    "    # define some parameters\n",
    "    base_period_len = sample_count  #\n",
    "\n",
    "    # check which setting to set for outlier filtering\n",
    "    if outliers_to_reject == 'Hi':\n",
    "        sHiLo = -1\n",
    "    elif outliers_to_reject == 'Lo':\n",
    "        sHiLo = 1\n",
    "    else:\n",
    "        sHiLo = 0\n",
    "\n",
    "    nr = min(2 * frequencies_considered_count + 1,\n",
    "             sample_count)  # number of 2*+1 frequencies, or number of input images\n",
    "\n",
    "    # create empty arrays to fill\n",
    "    outputs = np.zeros(shape=(inputs.shape[0], sample_count))\n",
    "\n",
    "    mat = get_starter_matrix(base_period_len, sample_count, frequencies_considered_count)\n",
    "\n",
    "    # repeat the mat array over the number of arrays in inputs\n",
    "    # and create arrays with ones with shape inputs where high and low values are set to 0\n",
    "    mat = np.tile(mat[None].T, (1, inputs.shape[0])).T\n",
    "    p = np.ones_like(inputs)\n",
    "    p[(low >= inputs) | (inputs > high)] = 0\n",
    "    nout = np.sum(p == 0, axis=-1)  # count the outliers for each timeseries\n",
    "\n",
    "    # prepare for while loop\n",
    "    ready = np.zeros((inputs.shape[0]), dtype=bool)  # all timeseries set to false\n",
    "\n",
    "    dod = 1  # (2*frequencies_considered_count-1)  # Um, no it isn't :/\n",
    "    noutmax = sample_count - nr - dod\n",
    "    # prepare to add delta to suppress high amplitudes but not for [0,0]\n",
    "    Adelta = np.tile(np.diag(np.ones(nr))[None].T, (1, inputs.shape[0])).T * delta\n",
    "    Adelta[:, 0, 0] -= delta\n",
    "    \n",
    "    for _ in xrange(sample_count):\n",
    "        if ready.all():\n",
    "            break        \n",
    "        \n",
    "        # multiply outliers with timeseries\n",
    "        za = np.einsum('ijk,ik->ij', mat, p * inputs)\n",
    "        #print za\n",
    "\n",
    "        # multiply mat with the multiplication of multiply diagonal of p with transpose of mat\n",
    "        diag = makediag3d(p)\n",
    "        #print diag\n",
    "        \n",
    "        A = np.einsum('ajk,aki->aji', mat, np.einsum('aij,jka->ajk', diag, mat.T))\n",
    "        # add delta to suppress high amplitudes but not for [0,0]\n",
    "        A += Adelta\n",
    "        #A[:, 0, 0] = A[:, 0, 0] - delta\n",
    "        #print A\n",
    "\n",
    "        # solve linear matrix equation and define reconstructed timeseries\n",
    "        zr = np.linalg.solve(A, za)\n",
    "        #print zr\n",
    "        \n",
    "        outputs = np.einsum('ijk,kj->ki', mat.T, zr)\n",
    "        #print outputs\n",
    "\n",
    "        # calculate error and sort err by index\n",
    "        err = p * (sHiLo * (outputs - inputs))\n",
    "        rankVec = np.argsort(err, axis=1, )\n",
    "\n",
    "        # select maximum error and compute new ready status\n",
    "        maxerr = np.max(err, axis=-1)\n",
    "        #maxerr = np.diag(err.take(rankVec[:, sample_count - 1], axis=-1))\n",
    "        ready = (maxerr <= fit_error_tolerance) | (nout == noutmax)        \n",
    "\n",
    "        # if ready is still false\n",
    "        if not ready.all():\n",
    "            j = rankVec.take(sample_count - 1, axis=-1)\n",
    "\n",
    "            p.T[j.T, np.indices(j.shape)] = p.T[j.T, np.indices(j.shape)] * ready.astype(\n",
    "                int)  #*check\n",
    "            nout += 1\n",
    "\n",
    "    logging.info('function `HANTS` complete')\n",
    "    return outputs\n",
    "\n",
    "def convert_ansi_date(date, offset=0.5):\n",
    "    logging.info('function `convert_ansi_date` complete')\n",
    "    return jdcal.jd2gcal(2305812.5, date + offset) # 0.5 offset is to adjust from night to noon\n",
    "\n",
    "def unix_time(dt):\n",
    "    epoch = datetime.datetime.utcfromtimestamp(0)\n",
    "    delta = dt - epoch\n",
    "    logging.info('function `unix_time` complete')\n",
    "    return delta.total_seconds()    \n",
    "\n",
    "def unix_time_millis(dt):\n",
    "    logging.info('function `unix_time_millis` complete')\n",
    "    return int(unix_time(dt) * 1000)\n",
    "\n",
    "def region(pixel):\n",
    "    \"\"\"\n",
    "    Extract pixel or regio:\n",
    "    region1pix = single lat/lon\n",
    "    region4pix = block of 4 pixels [2x2]\n",
    "    region9pix = block of 9 pixels [3x3]\n",
    "    \"\"\"\n",
    "    if pixel == 0:\n",
    "        regionpix = [0,0]\n",
    "    elif pixel == 4:\n",
    "        regionpix = [-0.025,0.025]\n",
    "    elif pixel == 9:\n",
    "        regionpix = [-0.075,0.075]\n",
    "    else:\n",
    "        return\n",
    "    return regionpix\n",
    "\n",
    "\n",
    "# In[50]:\n",
    "\n",
    "def REQUEST_DATA_XML(lon_center, lat_center, pix_offset, coverageID):\n",
    "    \n",
    "    regionpix = region(pix_offset)\n",
    "    \n",
    "    Long1 = str(lon_center + regionpix[0])\n",
    "    Long2 = str(lon_center + regionpix[1])\n",
    "    Lat1 = str(lat_center + regionpix[0])\n",
    "    Lat2 = str(lat_center + regionpix[1])\n",
    "    \n",
    "    full_url = \"http://localhost:8080/rasdaman/ows/wcs2?service=WCS&version=2.0.1&request=GetCoverage&coverageId=\"+coverageID+\"&subset=Long(\"+Long1+\",\"+Long2+\")&subset=Lat(\"+Lat1+\",\"+Lat2+\")\"\n",
    "    f = urllib2.urlopen(full_url)     \n",
    "    root = etree.fromstring(f.read())    \n",
    "    \n",
    "    # read grid envelope of domain set\n",
    "    xml_low_env = StringIO(root[1][0][0][0][0].text)\n",
    "    xml_high_env = StringIO(root[1][0][0][0][1].text)\n",
    "\n",
    "    # load grid envelope as numpy array\n",
    "    low_env = np.loadtxt(xml_low_env, dtype='int', delimiter=' ')\n",
    "    high_env = np.loadtxt(xml_high_env, dtype='int', delimiter=' ')\n",
    "    ts_shape = high_env - low_env + 1\n",
    "\n",
    "    easting = ts_shape[0]\n",
    "    northing = ts_shape[1]\n",
    "    time = ts_shape[2]    \n",
    "\n",
    "    ## extract the dates\n",
    "    #sd = ansi_date_to_greg_date(low_env[2]+140734)\n",
    "    #ed = ansi_date_to_greg_date(high_env[2]+140734)\n",
    "\n",
    "    # extract the values we need from the parsed XML\n",
    "    sta_date_ansi = np.loadtxt(StringIO(root[0][0][0].text))[2] # 150116\n",
    "    end_date_ansi = np.loadtxt(StringIO(root[0][0][1].text))[2] # 150852\n",
    "    sta_date_rasd = np.loadtxt(StringIO(root[1][0][0][0][0].text))[2] # 9382\n",
    "    end_date_rasd = np.loadtxt(StringIO(root[1][0][0][0][1].text))[2] # 9427\n",
    "    #timestep_date = np.loadtxt(StringIO(root[1][0][5].text))[2] # 16\n",
    "\n",
    "    try:\n",
    "        # check if regular coverage and ignore empty warnings\n",
    "        with warnings.catch_warnings():        \n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            timestep_date = np.loadtxt(StringIO(root[1][0][5].text))[2] # 16    \n",
    "        cov_reg = 1\n",
    "        print 'regular coverages'\n",
    "    except:        \n",
    "        # check if irregular coverage\n",
    "        array_stepsize = np.loadtxt(StringIO(root[1][0][5][0][1].text)) #array sample interval \n",
    "        cov_reg = 0    \n",
    "        print 'irregular coverages'    \n",
    "\n",
    "    # compute the start and end-date\n",
    "    dif_date_anra = sta_date_ansi - sta_date_rasd\n",
    "    dif_date_rasd = end_date_rasd - sta_date_rasd + 1\n",
    "\n",
    "\n",
    "    # convert dates to pandas date_range\n",
    "    str_date = pd.Timestamp.fromtimestamp((sta_date_ansi.astype('<m8[D]') - \n",
    "                                           (np.datetime64('1970-01-01') - np.datetime64('1601-01-01'))\n",
    "                                           ).astype('<m8[s]').astype(int))\n",
    "    end_date = pd.Timestamp.fromtimestamp((end_date_ansi.astype('<m8[D]') - \n",
    "                                           (np.datetime64('1970-01-01') - np.datetime64('1601-01-01'))\n",
    "                                           ).astype('<m8[s]').astype(int))\n",
    "    print cov_reg\n",
    "    if cov_reg == 1:\n",
    "        # regular coverage    \n",
    "        freq_date = str(int(timestep_date))+'D'\n",
    "        dates = pd.date_range(str_date,end_date, freq=freq_date)\n",
    "        dates = dates[:-1]\n",
    "        print 'dates regular'\n",
    "    elif cov_reg == 0:\n",
    "        # irregular coverage\n",
    "        time_delta = pd.TimedeltaIndex(array_stepsize, unit = 'D')\n",
    "        dates = pd.Series(np.array(str_date).repeat(len(array_stepsize)))\n",
    "        dates += time_delta\n",
    "        print 'dates irregular'    \n",
    "\n",
    "    logging.info('dates converted from ANSI to ISO 8601')    \n",
    "\n",
    "    # read data block of range set\n",
    "    xml_ts = StringIO(root[2][0][1].text)\n",
    "\n",
    "    # load data block as numpy array\n",
    "    ts = np.loadtxt(xml_ts, dtype='float', delimiter=',')\n",
    "\n",
    "    try:\n",
    "        ts_reshape = ts.reshape((easting*northing,time)) #Easting = ts_shape[0], Northing = ts_shape[1], time = ts_shape[2]\n",
    "    except:\n",
    "        # sometimes length regular coverages is incorrect\n",
    "        ts = ts[:-1]\n",
    "        ts_reshape = ts.reshape((easting*northing,time)) \n",
    "\n",
    "    return ts_reshape, dates,str_date,end_date, time\n",
    "\n",
    "\n",
    "# In[51]:\n",
    "\n",
    "def COMPUTE_NVAI(ts_reshape, dates, str_date, end_date, time, pyhants=0, ann_freq=6, outliers_rj='None', \n",
    "                 from_date='2010-01-01', to_date='2012-01-01'):\n",
    "    \"\"\"\n",
    "    \n",
    "    pyhants: 0 means will NOT use, 1 will use\n",
    "    ann_freq: number of frequencies (PER YEAR!)\n",
    "    outliers_rj = 'None', 'Hi' or 'Lo'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # compute mean so regional statistics can be computed as well\n",
    "    ts_mean = ts_reshape.mean(axis=0)\n",
    "    ndvi = pd.Series(ts_mean.flatten()/10000.,dates, name='nvdi')\n",
    "\n",
    "    # Interpolate NDVI 16 day interval to 1 day interval using linear interpolation\n",
    "    x = pd.date_range(str_date,end_date,freq='D')\n",
    "    ndvi_int = ndvi.reindex(x)\n",
    "    #ndvi_int = ndvi_int.fillna(method='pad')\n",
    "    ndvi_int = ndvi_int.interpolate(method='linear')\n",
    "    ndvi_int.name = 'ndvi_int'\n",
    "    \n",
    "    \n",
    "    if pyhants == 0:\n",
    "\n",
    "        #Compute NVAI using 16 day interval NDVI data        \n",
    "        #nvai = ndvi.groupby([ndvi.index.month]).apply(lambda g: (g - g.mean())/(g.max() - g.min()))\n",
    "        #nvai.name = 'nvai'\n",
    "        \n",
    "        # Compute NVAI using daily interpolated NDVI data\n",
    "        nvai = ndvi_int.groupby([ndvi_int.index.month,ndvi_int.index.day]).apply(lambda g: (g - g.mean())/(g.max() - g.min()))\n",
    "        nvai.name = 'nvai'  \n",
    "        nvai_sel = nvai.ix[from_date:to_date]        \n",
    "    \n",
    "    elif pyhants == 1:\n",
    "        # Compute PyHANTS first and then calculate mean\n",
    "        frequencies = len(ndvi_int) / 365 * ann_freq\n",
    "        outliers = outliers_rj\n",
    "        #ts_reshape = ts_reshape.mean(axis=0)\n",
    "        pyhants = HANTS(sample_count=time, inputs=ts_reshape/100, frequencies_considered_count=frequencies,  outliers_to_reject=outliers)\n",
    "        pyhants *= 100\n",
    "        pyhants_mean = pyhants.mean(axis=0)\n",
    "        ndvi_pyhants = pd.Series(pyhants_mean.flatten()/10000.,dates, name='ndvi_pyhants')        \n",
    "        \n",
    "        # interpolate reconstructed NDVI to daily values\n",
    "        x = pd.date_range(str_date,end_date,freq='D')\n",
    "        ndvi_pyhants_int = ndvi_pyhants.reindex(x)\n",
    "        ndvi_pyhants_int = ndvi_pyhants_int.interpolate(method='linear')\n",
    "        ndvi_pyhants_int.name = 'ndvi_pyhants_int'     \n",
    "        \n",
    "        # Compute NVAI using daily interpolated PyHANTS reconstructed NDVI data\n",
    "        nvai = ndvi_pyhants_int.groupby([ndvi_pyhants_int.index.month,ndvi_pyhants_int.index.day]).apply(lambda g: (g - g.mean())/(g.max() - g.min()))\n",
    "        nvai.name = 'nvai'\n",
    "        nvai_sel = nvai.ix[from_date:to_date]\n",
    "    \n",
    "    # data preparation for HighCharts: Output need to be in JSON format with time \n",
    "    # in Unix milliseconds\n",
    "    dthandler = lambda obj: (\n",
    "    unix_time_millis(obj)\n",
    "    if isinstance(obj, datetime.datetime)\n",
    "    or isinstance(obj, datetime.date)\n",
    "    else None)\n",
    "\n",
    "    nvai_json = StringIO()\n",
    "\n",
    "    logging.info('ready to dump files to JSON')\n",
    "    # np.savetxt(output, pyhants, delimiter=',')\n",
    "    out1 = json.dump(nvai_sel.reset_index().as_matrix().tolist(), nvai_json, default=dthandler)\n",
    "\n",
    "    logging.info('dates converted from ISO 8601 to UNIX in ms')             \n",
    "    \n",
    "    return nvai_json #nvai_sel\n",
    "\n",
    "\n",
    "# In[52]:\n",
    "\n",
    "def COMPUTE_NTAI(ts_reshape, dates, str_date, end_date, time, pyhants=0, ann_freq=6, outliers_rj='None', \n",
    "                 from_date='2010-01-01', to_date='2012-01-01'):\n",
    "    \"\"\"\n",
    "    \n",
    "    pyhants: 0 means will NOT use, 1 will use\n",
    "    ann_freq: number of frequencies (PER YEAR!)\n",
    "    outliers_rj = 'None', 'Hi' or 'Lo'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # compute mean so regional statistics can be computed as well\n",
    "    ts_mean = ts_reshape.mean(axis=0)\n",
    "    lst = pd.Series(ts_mean.flatten()/10000.,dates, name='lst')\n",
    "\n",
    "    # Interpolate NDVI 16 day interval to 1 day interval using linear interpolation\n",
    "    x = pd.date_range(str_date,end_date,freq='D')\n",
    "    lst_int = lst.reindex(x)\n",
    "    #ndvi_int = ndvi_int.fillna(method='pad')\n",
    "    lst_int = lst_int.interpolate(method='linear')\n",
    "    lst_int.name = 'lst_int'\n",
    "    \n",
    "    \n",
    "    if pyhants == 0:\n",
    "\n",
    "        #Compute NVAI using 16 day interval NDVI data        \n",
    "        #nvai = ndvi.groupby([ndvi.index.month]).apply(lambda g: (g - g.mean())/(g.max() - g.min()))\n",
    "        #nvai.name = 'nvai'\n",
    "        \n",
    "        # Compute NVAI using daily interpolated NDVI data\n",
    "        ntai = lst_int.groupby([lst_int.index.month,lst_int.index.day]).apply(lambda g: (g - g.mean())/(g.max() - g.min()))\n",
    "        ntai.name = 'ntai'  \n",
    "        ntai_sel = ntai.ix[from_date:to_date]        \n",
    "    \n",
    "    elif pyhants == 1:\n",
    "        # Compute PyHANTS first and then calculate mean\n",
    "        frequencies = len(lst_int) / 365 * ann_freq\n",
    "        outliers = outliers_rj\n",
    "        #ts_reshape = ts_reshape.mean(axis=0)\n",
    "        pyhants = HANTS(sample_count=time, inputs=ts_reshape/100, frequencies_considered_count=frequencies,  outliers_to_reject=outliers)\n",
    "        pyhants *= 100\n",
    "        pyhants_mean = pyhants.mean(axis=0)\n",
    "        lst_pyhants = pd.Series(pyhants_mean.flatten()/10000.,dates, name='lst_pyhants')        \n",
    "        \n",
    "        # interpolate reconstructed LST to daily values\n",
    "        x = pd.date_range(str_date,end_date,freq='D')\n",
    "        lst_pyhants_int = lst_pyhants.reindex(x)\n",
    "        lst_pyhants_int = lst_pyhants_int.interpolate(method='linear')\n",
    "        lst_pyhants_int.name = 'lst_pyhants_int'     \n",
    "        \n",
    "        # Compute NTAI using daily interpolated PyHANTS reconstructed NTAI data\n",
    "        ntai = lst_pyhants_int.groupby([lst_pyhants_int.index.month,lst_pyhants_int.index.day]).apply(lambda g: (g - g.mean())/(g.max() - g.min()))\n",
    "        ntai.name = 'ntai'\n",
    "        ntai_sel = ntai.ix[from_date:to_date]\n",
    "    \n",
    "    # data preparation for HighCharts: Output need to be in JSON format with time \n",
    "    # in Unix milliseconds\n",
    "    dthandler = lambda obj: (\n",
    "    unix_time_millis(obj)\n",
    "    if isinstance(obj, datetime.datetime)\n",
    "    or isinstance(obj, datetime.date)\n",
    "    else None)\n",
    "\n",
    "    ntai_json = StringIO()\n",
    "\n",
    "    logging.info('ready to dump files to JSON')\n",
    "    # np.savetxt(output, pyhants, delimiter=',')\n",
    "    out1 = json.dump(ntai_sel.reset_index().as_matrix().tolist(), ntai_json, default=dthandler)\n",
    "\n",
    "    logging.info('dates converted from ISO 8601 to UNIX in ms')             \n",
    "    \n",
    "    return ntai_json #nvai_sel\n",
    "\n",
    "\n",
    "# In[53]:\n",
    "\n",
    "def COMPUTE_VCI(ts_reshape, dates, str_date, end_date, time, pyhants=0, ann_freq=6, outliers_rj='None', \n",
    "                 from_date='2010-01-01', to_date='2012-01-01'):\n",
    "    \"\"\"\n",
    "    \n",
    "    pyhants: 0 means will NOT use, 1 will use\n",
    "    ann_freq: number of frequencies (PER YEAR!)\n",
    "    outliers_rj = 'None', 'Hi' or 'Lo'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # compute mean so regional statistics can be computed as well\n",
    "    ts_mean = ts_reshape.mean(axis=0)\n",
    "    ndvi = pd.Series(ts_mean.flatten()/10000.,dates, name='nvdi')\n",
    "\n",
    "    # Interpolate NDVI 16 day interval to 1 day interval using linear interpolation\n",
    "    x = pd.date_range(str_date,end_date,freq='D')\n",
    "    ndvi_int = ndvi.reindex(x)\n",
    "    #ndvi_int = ndvi_int.fillna(method='pad')\n",
    "    ndvi_int = ndvi_int.interpolate(method='linear')\n",
    "    ndvi_int.name = 'ndvi_int'\n",
    "    \n",
    "    \n",
    "    if pyhants == 0:\n",
    "\n",
    "        #Compute NVAI using 16 day interval NDVI data        \n",
    "        #nvai = ndvi.groupby([ndvi.index.month]).apply(lambda g: (g - g.mean())/(g.max() - g.min()))\n",
    "        #nvai.name = 'nvai'\n",
    "        \n",
    "        # Compute NVAI using daily interpolated NDVI data\n",
    "        vci = ndvi_int.groupby([ndvi_int.index.month,ndvi_int.index.day]).apply(lambda g: (g - g.min())/(g.max() - g.min()))\n",
    "        vci.name = 'vci'  \n",
    "        vci_sel = vci.ix[from_date:to_date]        \n",
    "    \n",
    "    elif pyhants == 1:\n",
    "        # Compute PyHANTS first and then calculate mean\n",
    "        frequencies = len(ndvi_int) / 365 * ann_freq\n",
    "        outliers = outliers_rj\n",
    "        #ts_reshape = ts_reshape.mean(axis=0)\n",
    "        pyhants = HANTS(sample_count=time, inputs=ts_reshape/100, frequencies_considered_count=frequencies,  outliers_to_reject=outliers)\n",
    "        pyhants *= 100\n",
    "        pyhants_mean = pyhants.mean(axis=0)\n",
    "        ndvi_pyhants = pd.Series(pyhants_mean.flatten()/10000.,dates, name='ndvi_pyhants')        \n",
    "        \n",
    "        # interpolate reconstructed NDVI to daily values\n",
    "        x = pd.date_range(str_date,end_date,freq='D')\n",
    "        ndvi_pyhants_int = ndvi_pyhants.reindex(x)\n",
    "        ndvi_pyhants_int = ndvi_pyhants_int.interpolate(method='linear')\n",
    "        ndvi_pyhants_int.name = 'ndvi_pyhants_int'     \n",
    "        \n",
    "        # Compute NVAI using daily interpolated PyHANTS reconstructed NDVI data\n",
    "        vci = ndvi_pyhants_int.groupby([ndvi_pyhants_int.index.month,ndvi_pyhants_int.index.day]).apply(lambda g: (g - g.min())/(g.max() - g.min()))\n",
    "        vci.name = 'vci'\n",
    "        vci_sel = vci.ix[from_date:to_date]\n",
    "    \n",
    "    # data preparation for HighCharts: Output need to be in JSON format with time \n",
    "    # in Unix milliseconds\n",
    "    dthandler = lambda obj: (\n",
    "    unix_time_millis(obj)\n",
    "    if isinstance(obj, datetime.datetime)\n",
    "    or isinstance(obj, datetime.date)\n",
    "    else None)\n",
    "\n",
    "    vci_json = StringIO()\n",
    "\n",
    "    logging.info('ready to dump files to JSON')\n",
    "    # np.savetxt(output, pyhants, delimiter=',')\n",
    "    out1 = json.dump(vci_sel.reset_index().as_matrix().tolist(), vci_json, default=dthandler)\n",
    "\n",
    "    logging.info('dates converted from ISO 8601 to UNIX in ms')             \n",
    "    \n",
    "    return vci_json #nvai_sel\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "def COMPUTE_TCI(ts_reshape, dates, str_date, end_date, time, pyhants=0, ann_freq=6, outliers_rj='None', \n",
    "                 from_date='2010-01-01', to_date='2012-01-01'):\n",
    "    \"\"\"\n",
    "    \n",
    "    pyhants: 0 means will NOT use, 1 will use\n",
    "    ann_freq: number of frequencies (PER YEAR!)\n",
    "    outliers_rj = 'None', 'Hi' or 'Lo'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # compute mean so regional statistics can be computed as well\n",
    "    ts_mean = ts_reshape.mean(axis=0)\n",
    "    lst = pd.Series(ts_mean.flatten()/10000.,dates, name='lst')\n",
    "\n",
    "    # Interpolate NDVI 16 day interval to 1 day interval using linear interpolation\n",
    "    x = pd.date_range(str_date,end_date,freq='D')\n",
    "    lst_int = lst.reindex(x)\n",
    "    #ndvi_int = ndvi_int.fillna(method='pad')\n",
    "    lst_int = lst_int.interpolate(method='linear')\n",
    "    lst_int.name = 'lst_int'\n",
    "    \n",
    "    \n",
    "    if pyhants == 0:\n",
    "\n",
    "        #Compute NVAI using 16 day interval NDVI data        \n",
    "        #nvai = ndvi.groupby([ndvi.index.month]).apply(lambda g: (g - g.mean())/(g.max() - g.min()))\n",
    "        #nvai.name = 'nvai'\n",
    "        \n",
    "        # Compute NVAI using daily interpolated NDVI data\n",
    "        tci = lst_int.groupby([lst_int.index.month,lst_int.index.day]).apply(lambda g: (g.max() - g)/(g.max() - g.min()))\n",
    "        tci.name = 'tci'  \n",
    "        tci_sel = tci.ix[from_date:to_date]        \n",
    "    \n",
    "    elif pyhants == 1:\n",
    "        # Compute PyHANTS first and then calculate mean\n",
    "        frequencies = len(lst_int) / 365 * ann_freq\n",
    "        outliers = outliers_rj\n",
    "        #ts_reshape = ts_reshape.mean(axis=0)\n",
    "        pyhants = HANTS(sample_count=time, inputs=ts_reshape/100, frequencies_considered_count=frequencies,  outliers_to_reject=outliers)\n",
    "        pyhants *= 100\n",
    "        pyhants_mean = pyhants.mean(axis=0)\n",
    "        lst_pyhants = pd.Series(pyhants_mean.flatten()/10000.,dates, name='lst_pyhants')        \n",
    "        \n",
    "        # interpolate reconstructed LST to daily values\n",
    "        x = pd.date_range(str_date,end_date,freq='D')\n",
    "        lst_pyhants_int = lst_pyhants.reindex(x)\n",
    "        lst_pyhants_int = lst_pyhants_int.interpolate(method='linear')\n",
    "        lst_pyhants_int.name = 'lst_pyhants_int'     \n",
    "        \n",
    "        # Compute NTAI using daily interpolated PyHANTS reconstructed NTAI data\n",
    "        tci = lst_pyhants_int.groupby([lst_pyhants_int.index.month,lst_pyhants_int.index.day]).apply(lambda g: (g.max() - g)/(g.max() - g.min()))\n",
    "        tci.name = 'tci'\n",
    "        tci_sel = tci.ix[from_date:to_date]\n",
    "    \n",
    "    # data preparation for HighCharts: Output need to be in JSON format with time \n",
    "    # in Unix milliseconds\n",
    "    dthandler = lambda obj: (\n",
    "    unix_time_millis(obj)\n",
    "    if isinstance(obj, datetime.datetime)\n",
    "    or isinstance(obj, datetime.date)\n",
    "    else None)\n",
    "\n",
    "    tci_json = StringIO()\n",
    "\n",
    "    logging.info('ready to dump files to JSON')\n",
    "    # np.savetxt(output, pyhants, delimiter=',')\n",
    "    out1 = json.dump(tci_sel.reset_index().as_matrix().tolist(), tci_json, default=dthandler)\n",
    "\n",
    "    logging.info('dates converted from ISO 8601 to UNIX in ms')             \n",
    "    \n",
    "    return tci_json #nvai_sel\n",
    "\n",
    "\n",
    "# In[80]:\n",
    "\n",
    "def COMPUTE_VHI(vci_json, tci_json, alpha = 0.5):\n",
    "    # load VCI data\n",
    "    vci = pd.read_json(vci_json.getvalue())\n",
    "    vci.columns = [\"date\",\"vci\"]\n",
    "    vci['date'] = pd.to_datetime(vci['date'],unit='ms')\n",
    "    vci.set_index(['date'],inplace=True)\n",
    "\n",
    "    # load TCI data\n",
    "    tci = pd.read_json(tci_json.getvalue())\n",
    "    tci.columns = [\"date\",\"tci\"]\n",
    "    tci['date'] = pd.to_datetime(tci['date'],unit='ms')\n",
    "    tci.set_index(['date'],inplace=True)\n",
    "    \n",
    "    # compute VHI    \n",
    "    vhi = (alpha * vci['vci']) + ((1-alpha) * tci['tci'])\n",
    "    \n",
    "    # data preparation for HighCharts: Output need to be in JSON format with time \n",
    "    # in Unix milliseconds\n",
    "    dthandler = lambda obj: (\n",
    "    unix_time_millis(obj)\n",
    "    if isinstance(obj, datetime.datetime)\n",
    "    or isinstance(obj, datetime.date)\n",
    "    else None)\n",
    "\n",
    "    vhi_json = StringIO()\n",
    "\n",
    "    logging.info('ready to dump files to JSON')\n",
    "    # np.savetxt(output, pyhants, delimiter=',')\n",
    "    out1 = json.dump(vhi.reset_index().as_matrix().tolist(), vhi_json, default=dthandler)\n",
    "\n",
    "    logging.info('dates converted from ISO 8601 to UNIX in ms')             \n",
    "    \n",
    "    return vhi_json #nvai_sel    \n",
    "\n",
    "# In[ ]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lon_center = float(self.lonIn.getValue())\n",
    "lat_center = float(self.latIn.getValue())\n",
    "pix_offset = int(self.pixOff.getValue())                \n",
    "\n",
    "from_date = str(self.fromDateIn.getValue())\n",
    "to_date = str(self.toDateIn.getValue())\n",
    "\n",
    "pyhants = int(self.pyHants.getValue())\n",
    "ann_freq = int(self.freqan.getValue())\n",
    "outliers_rj = str(self.outlier.getValue())    \n",
    "\n",
    "CoverageID_NDVI, CoverageID_LST = GetCoverageNames()    \n",
    "\n",
    "# 2. Do the Work\n",
    "# Request NDVI data and compute VCI and compute NVAI \n",
    "ndvi,dates,str_date,end_date,time=REQUEST_DATA_XML(lon_center,lat_center,pix_offset,coverageID=CoverageID_NDVI)\n",
    "vci_json = COMPUTE_VCI(ndvi,dates,str_date,end_date,time,pyhants,ann_freq,outliers_rj,from_date,to_date)\n",
    "nvai_json = COMPUTE_NVAI(ndvi,dates,str_date,end_date,time,pyhants,ann_freq,outliers_rj,from_date,to_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Process(WPSProcess):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        ##\n",
    "        # Process initialization\n",
    "        WPSProcess.__init__(self,\n",
    "            identifier = \"WPS_NVAI_VCI_TS\",\n",
    "            title=\"Compute NVAI_VCI TIMESERIES\",\n",
    "            abstract=\"\"\"Module to compute NVAI_VCI TimeSeries based on NDVI data\"\"\",\n",
    "            version = \"1.0\",\n",
    "            storeSupported = True,\n",
    "            statusSupported = True)\n",
    "\n",
    "        ##\n",
    "        # Adding process inputs\n",
    "        self.lonIn = self.addLiteralInput(identifier=\"lon_center\",\n",
    "                    title=\"Longitude\",\n",
    "                    type=type(''))\n",
    "\n",
    "        self.latIn = self.addLiteralInput(identifier=\"lat_center\",\n",
    "                    title=\"Latitude\",\n",
    "                    type=type(''))\n",
    "\n",
    "        self.fromDateIn = self.addLiteralInput(identifier=\"from_date\",\n",
    "                    title = \"The start date to be calcualted\",\n",
    "                                          type=type(''))\n",
    "\n",
    "        self.toDateIn = self.addLiteralInput(identifier=\"to_date\",\n",
    "                    title = \"The final date to be calcualted\",\n",
    "                                          type=type(''))   \n",
    "        \n",
    "        self.pixOff = self.addLiteralInput(identifier=\"pix_offset\",\n",
    "                    title=\"pixel offset, 0, 4 or 9\",\n",
    "                    type=type(''))\n",
    "\n",
    "        self.pyHants = self.addLiteralInput(identifier=\"pyhants\",\n",
    "                    title=\"exclude pyhants (0) or include pyhants (1)\",\n",
    "                    type=type(''))\n",
    "\n",
    "        self.freqan = self.addLiteralInput(identifier=\"ann_freq\",\n",
    "                    title = \"number of annual (!) frequencies\",\n",
    "                                          type=type(''))\n",
    "\n",
    "        self.outlier = self.addLiteralInput(identifier=\"outliers_rj\",\n",
    "                    title = \"what type of outliers to reject, Hi, Lo, or None\",\n",
    "                                          type=type(''))        \n",
    "        \n",
    "        ##\n",
    "        # Adding process outputs\n",
    "        self.nvaiOut = self.addComplexOutput(identifier  = \"nvai_ts\", \n",
    "                                        title       = \"NVAI Timeseries\",\n",
    "                                        formats     = [{'mimeType':'text/xml'}]) #xml/application ||application/json   \n",
    "\n",
    "        self.vciOut = self.addComplexOutput(identifier  = \"vci_ts\", \n",
    "                                        title       = \"VCI Timeseries\",\n",
    "                                        formats     = [{'mimeType':'text/xml'}]) #xml/application ||application/json   \n",
    "        \n",
    "    ##\n",
    "    # Execution part of the process\n",
    "    def execute(self):\n",
    "        # 1. Load the data\n",
    "        lon_center = float(self.lonIn.getValue())\n",
    "        lat_center = float(self.latIn.getValue())\n",
    "        pix_offset = int(self.pixOff.getValue())                \n",
    "        \n",
    "        from_date = str(self.fromDateIn.getValue())\n",
    "        to_date = str(self.toDateIn.getValue())\n",
    "        \n",
    "        pyhants = int(self.pyHants.getValue())\n",
    "        ann_freq = int(self.freqan.getValue())\n",
    "        outliers_rj = str(self.outlier.getValue())    \n",
    "\n",
    "        CoverageID_NDVI, CoverageID_LST = GetCoverageNames()    \n",
    "        \n",
    "        # 2. Do the Work\n",
    "        # Request NDVI data and compute VCI and compute NVAI \n",
    "        ndvi,dates,str_date,end_date,time=REQUEST_DATA_XML(lon_center,lat_center,pix_offset,coverageID=CoverageID_NDVI)\n",
    "        vci_json = COMPUTE_VCI(ndvi,dates,str_date,end_date,time,pyhants,ann_freq,outliers_rj,from_date,to_date)\n",
    "        nvai_json = COMPUTE_NVAI(ndvi,dates,str_date,end_date,time,pyhants,ann_freq,outliers_rj,from_date,to_date)\n",
    "        \n",
    "        # 3. Save to out\n",
    "        self.vciOut.setValue( vci_json )\n",
    "        self.nvaiOut.setValue( nvai_json )\n",
    "        return\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
