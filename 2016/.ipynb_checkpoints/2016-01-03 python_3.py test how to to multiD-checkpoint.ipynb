{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Process(WPSProcess):\n",
    "     def __init__(self):\n",
    "         \n",
    "         WPSProcess.__init__(self,\n",
    "              identifier       = \"pyhants_3\", \n",
    "              title            = \"***---*** PyHANTS Time series reconstruction ***---***\",\n",
    "              version          = \"1\",\n",
    "              storeSupported   = \"true\",\n",
    "              statusSupported  = \"false\",\n",
    "              abstract         = \"This service provide access to the Harmonic Analysis of Time Series for Python. It include features as high/low outlier filtering and the number of freqiencies\")\n",
    "              \n",
    "         #self.input  = self.addLiteralInput(identifier  = \"input_ts\",\n",
    "         #                                   title       = \"Input time series to reconstruct\",\n",
    "         #                                   type        = FloatType)\n",
    "         self.covID = self.addLiteralInput(identifier    = \"covID\",\n",
    "  \t\t\t                   title         = \"coverage ID\",\n",
    "\t\t                           type          = type(''))\n",
    "\n",
    "         self.lonIn = self.addLiteralInput(identifier    = \"lon_center\",\n",
    "  \t\t\t                   title         = \"Longitude\",\n",
    "\t\t                           type          = type(''))\n",
    "\n",
    "         self.latIn = self.addLiteralInput(identifier    = \"lat_center\",\n",
    " \t\t                           title         = \"Latitude\",\n",
    "\t\t                           type          = type(''))\n",
    "\n",
    "         self.fromDateIn = self.addLiteralInput(identifier  = \"from_date\",\n",
    "                                           title            = \"The start date to be calcualted\",\n",
    "                                           type             = type(''))\n",
    "\n",
    "         self.toDateIn = self.addLiteralInput(identifier     = \"to_date\",\n",
    "                                           title            = \"The final date to be calcualted\",\n",
    "                                           type             = type(''))  \n",
    "\n",
    "         self.pixOff = self.addLiteralInput(identifier   = \"pix_offset\",\n",
    "\t\t\t                    title        = \"pixel offset, 0, 4 or 9\",\n",
    "\t\t\t                    type         = type(''))\n",
    "\n",
    "         self.nfreq  = self.addLiteralInput(identifier  = \"no_freq\",\n",
    "                                            title       = \"Number of frequencies to consider\")\n",
    "\n",
    "         self.outli  = self.addLiteralInput(identifier  = \"outlier_type\",\n",
    "                                            title       = \"Which type of values are considered as outliers\")\n",
    "\n",
    " \n",
    "         self.resu1 = self.addComplexOutput(identifier = \"output_ts1\", \n",
    "                                            title       = \"Output PyHANTS Time series reconstruction\",\n",
    "                                            formats     = [{'mimeType':'text/xml'}]) #xml/application ||application/json\n",
    "\n",
    "         self.resu2 = self.addComplexOutput(identifier = \"output_ts2\", \n",
    "                                            title       = \"Output PyHANTS Time series reconstruction\",\n",
    "                                            formats     = [{'mimeType':'text/xml'}]) #xml/application ||application/json\n",
    "                                            \n",
    "     def execute(self):\n",
    "\n",
    "\n",
    "         # Input  \n",
    "\n",
    "\n",
    "         regionpix = region(int(self.pixOff.getValue()))\n",
    "    \n",
    "         Long1 = str(float(self.lonIn.getValue()) + regionpix[0])\n",
    "         Long2 = str(float(self.lonIn.getValue()) + regionpix[1])\n",
    "         Lat1 = str(float(self.latIn.getValue()) + regionpix[0])\n",
    "         Lat2 = str(float(self.latIn.getValue()) + regionpix[1])\n",
    "\n",
    "\t coverageID = self.covID.getValue()\n",
    "\t from_date = self.fromDateIn.getValue()\n",
    "\t to_date = self.toDateIn.getValue()\n",
    "    \n",
    "         full_url = \"http://localhost:8080/rasdaman/ows/wcs2?service=WCS&version=2.0.1&request=GetCoverage&coverageId=\"+coverageID+\"&subset=Long(\"+Long1+\",\"+Long2+\")&subset=Lat(\"+Lat1+\",\"+Lat2+\")&subset=ansi(%22\"+str(from_date)+\"%22,%22\"+str(to_date)+\"%22)\"\n",
    "\t logging.info(full_url)\n",
    "         f = urllib2.urlopen(full_url)  \n",
    "\t f_ = f.read()\n",
    "\n",
    "         CoverageID_NDVI, CoverageID_LST = GetCoverageNames()\n",
    "\n",
    "\t if f_[0:400].split('\"')[5] == CoverageID_LST: \n",
    "\t     scaling_value = 1000.\n",
    "\t if f_[0:400].split('\"')[5] == CoverageID_NDVI:\n",
    "\t     scaling_value = 10000.   \n",
    "         \n",
    "\t logging.info(scaling_value)\n",
    "\t root = etree.fromstring(f_)    \n",
    "\n",
    "         #parser = etree.XMLParser(encoding=\"utf-8\")\n",
    "\t #xml_in = self.input.getValue()\n",
    "\n",
    "\n",
    "         #tree = etree.parse(xml_in, parser=parser)\n",
    "\n",
    "\t #logging.info(tree)\n",
    "\n",
    "\n",
    "\n",
    "         #root = tree.getroot()  \n",
    "         \n",
    "         frequencies = int(self.nfreq.getValue())\n",
    "         variable_out = int(self.outli.getValue())\n",
    "         if variable_out == 1:\n",
    "             outliers = \"Hi\"    \n",
    "         elif variable_out == 2:\n",
    "             outliers = \"Lo\"\n",
    "         else:\n",
    "             outliers = \"None\"         \n",
    "         \n",
    "\n",
    "         # read grid envelope of domain set\n",
    "         xml_low_env = cStringIO.StringIO(root[1][0][0][0][0].text)\n",
    "         xml_high_env = cStringIO.StringIO(root[1][0][0][0][1].text)\n",
    "\n",
    "         # load grid envelope as numpy array\n",
    "         low_env = np.loadtxt(xml_low_env, dtype='int', delimiter=' ')\n",
    "         high_env = np.loadtxt(xml_high_env, dtype='int', delimiter=' ')\n",
    "         ts_shape = high_env - low_env + 1\n",
    "\n",
    "         easting = ts_shape[0]\n",
    "         northing = ts_shape[1]\n",
    "         time = ts_shape[2]\n",
    "\n",
    "         ## extract the dates\n",
    "         #sd = ansi_date_to_greg_date(low_env[2]+140734)\n",
    "         #ed = ansi_date_to_greg_date(high_env[2]+140734)\n",
    "\n",
    "         # extract the values we need from the parsed XML\n",
    "         sta_date_ansi = np.loadtxt(cStringIO.StringIO(root[0][0][0].text))[2] # 150116\n",
    "         end_date_ansi = np.loadtxt(cStringIO.StringIO(root[0][0][1].text))[2] # 150852\n",
    "         sta_date_rasd = np.loadtxt(cStringIO.StringIO(root[1][0][0][0][0].text))[2] # 9382\n",
    "         end_date_rasd = np.loadtxt(cStringIO.StringIO(root[1][0][0][0][1].text))[2] # 9427\n",
    "         #timestep_date = np.loadtxt(cStringIO.StringIO(root[1][0][5].text))[2] # 16\n",
    "\n",
    "         ## compute the start and end-date\n",
    "         #dif_date_anra = sta_date_ansi - sta_date_rasd\n",
    "         #dif_date_rasd = end_date_rasd - sta_date_rasd + 1\n",
    "         #end_date_anra = dif_date_rasd * timestep_date + sta_date_rasd + dif_date_anra\n",
    "\n",
    "         #sd = convert_ansi_date(sta_date_ansi) # (2012, 1, 2, 0.5)\n",
    "         #ed = convert_ansi_date(end_date_anra) # (2014, 1, 7, 0.5)\n",
    "\n",
    "         ## convert dates to pandas date_range\n",
    "         #str_date = str(sd[1])+'.'+str(sd[2])+'.'+str(sd[0])+'.'+str(int(np.round(sd[3]*24)))+':00'\n",
    "         #end_date = str(ed[1])+'.'+str(ed[2])+'.'+str(ed[0])+'.'+str(int(np.round(ed[3]*24)))+':00'\n",
    "         #freq_date = str(int(timestep_date))+'D'\n",
    "         #dates = pd.date_range(str_date,end_date, freq=freq_date)\n",
    "         #dates = dates[:-1]\n",
    "\n",
    "         #logging.info('dates converted from ANSI to ISO 8601')\n",
    "\n",
    "         ## read data block of range set\n",
    "         #xml_ts = cStringIO.StringIO(root[2][0][1].text)\n",
    "\n",
    "         ## load data block as numpy array\n",
    "         #ts = np.loadtxt(xml_ts, dtype='float', delimiter=',')         \n",
    "         #ts_reshape = ts.reshape((easting*northing,time)) #Easting = ts_shape[0], Northing = ts_shape[1], time = ts_shape[2]\n",
    "\n",
    "         try:\n",
    "             # check if regular coverage and ignore empty warnings\n",
    "             with warnings.catch_warnings():        \n",
    "                 warnings.simplefilter(\"ignore\")\n",
    "                 timestep_date = np.loadtxt(StringIO(root[1][0][5].text))[2] # 16    \n",
    "             cov_reg = 1\n",
    "             print 'regular coverages'\n",
    "         except:        \n",
    "             # check if irregular coverage\n",
    "             array_stepsize = np.loadtxt(StringIO(root[1][0][5][0][1].text)) #array sample interval \n",
    "             cov_reg = 0    \n",
    "             print 'irregular coverages'    \n",
    "\n",
    "         # compute the start and end-date\n",
    "         dif_date_anra = sta_date_ansi - sta_date_rasd\n",
    "         dif_date_rasd = end_date_rasd - sta_date_rasd + 1\n",
    "\n",
    "         # convert dates to pandas date_range\n",
    "         #str_date = pd.Timestamp.fromtimestamp((sta_date_ansi.astype('<m8[D]') - \n",
    "         #                                       (np.datetime64('1970-01-01') - np.datetime64('1601-01-01'))\n",
    "         #                                       ).astype('<m8[s]').astype(int))\n",
    "         #end_date = pd.Timestamp.fromtimestamp((end_date_ansi.astype('<m8[D]') - \n",
    "         #                                       (np.datetime64('1970-01-01') - np.datetime64('1601-01-01'))\n",
    "         #                                       ).astype('<m8[s]').astype(int))\n",
    "         str_date = pd.Timestamp(str(from_date)+' 08:00:00') # get start date from user input instead of rasdaman coverage\n",
    "         end_date = pd.Timestamp(str(to_date)+' 08:00:00') # get end date from user input instead of rasdaman coverage\n",
    "        \n",
    "         print cov_reg\n",
    "         if cov_reg == 1:\n",
    "             # regular coverage    \n",
    "             freq_date = str(int(timestep_date))+'D'\n",
    "             dates = pd.date_range(str_date,end_date, freq=freq_date)\n",
    "             dates = dates[:-1]\n",
    "             print 'dates regular'\n",
    "         elif cov_reg == 0:\n",
    "             # irregular coverage\n",
    "             time_delta = pd.TimedeltaIndex(array_stepsize, unit = 'D')\n",
    "             dates = pd.Series(np.array(str_date).repeat(len(array_stepsize)))\n",
    "             dates += time_delta\n",
    "             print 'dates irregular'    \n",
    "\n",
    "         logging.info('dates converted from ANSI to ISO 8601')    \n",
    "\n",
    "         # read data block of range set\n",
    "         xml_ts = StringIO(root[2][0][1].text)\n",
    "\n",
    "         # load data block as numpy array\n",
    "         ts = np.loadtxt(xml_ts, dtype='float', delimiter=',')\n",
    "\n",
    "         try:\n",
    "             ts_reshape = ts.reshape((easting*northing,time)) #Easting = ts_shape[0], Northing = ts_shape[1], time = ts_shape[2]\n",
    "         except:\n",
    "             # sometimes length regular coverages is incorrect\n",
    "             ts = ts[:-1]\n",
    "             ts_reshape = ts.reshape((easting*northing,time)) \n",
    "\n",
    "\n",
    "\t ts_reshape_mean = ts_reshape.mean(axis=0)\n",
    "         # compute HANTS\n",
    "         frequencies = (len(ts_reshape_mean)*16) / 365 * frequencies\n",
    "         pyhants = HANTS(sample_count=time, inputs=ts_reshape/100, frequencies_considered_count=frequencies,  outliers_to_reject=outliers)\n",
    "         #pyhants = HANTS(time, ts_reshape/100, frequencies, outliers)\n",
    "         pyhants *= 100\n",
    "         pyhants_mean = pyhants.mean(axis=0)\n",
    "         df = pd.DataFrame(pyhants_mean.flatten()/scaling_value,dates)\n",
    "         df_org = pd.DataFrame(ts_reshape_mean.flatten()/scaling_value,dates)\n",
    "\n",
    "         # data preparation for HighCharts: Output need to be in JSON format with time \n",
    "         # in Unix milliseconds\n",
    "         dthandler = lambda obj: (\n",
    "          unix_time_millis(obj)\n",
    "          if isinstance(obj, datetime.datetime)\n",
    "          or isinstance(obj, datetime.date)\n",
    "          else None)\n",
    "\n",
    "         output1 = cStringIO.StringIO()\n",
    "         output2 = cStringIO.StringIO()   \n",
    "\n",
    "         logging.info('ready to dump files to JSON')\n",
    "         # np.savetxt(output, pyhants, delimiter=',')\n",
    "         out1 = json.dump(df.reset_index().as_matrix().tolist(), output1, default=dthandler)\n",
    "         out2 = json.dump(df_org.reset_index().as_matrix().tolist(), output2, default=dthandler)\n",
    "         logging.info('dates converted from ISO 8601 to UNIX in ms')         \n",
    "         \n",
    "         # ouput\n",
    "         self.resu1.setValue(output1)\n",
    "         self.resu2.setValue(output2)\n",
    "         return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
