{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from os.path import isdir\n",
    "#from fewslogger import Logger\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "import sys, getopt, shutil\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "from xml.etree import ElementTree\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "localtz = timezone('Europe/Amsterdam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def createNETCDF(filename, in_nc_arr=\"\", STATE_COPY_CREATE=0, TIME_LIST=[], HM_ARRAY=[], fillValue = -999.0):\n",
    "    \"\"\"\n",
    "    filename           ::  path to new NETCDF file\n",
    "    in_nc_arr          ::  base NETCDF file\n",
    "    STATE_COPY_CREATE  ::  set 0 for state\n",
    "                           set 1 for copy\n",
    "                           set 2 for create\n",
    "    TIME_LIST          ::  when creating, provide datetime array with dates\n",
    "    HM_ARRAY           ::  provide array with h.m data to create NETCDF\n",
    "    fillValue          ::  only used during creation [default: -999.0] \n",
    "    \"\"\"\n",
    "    # 0 is TimeSeriesSet STATE FILE\n",
    "    # 1 is TimeSeriesSet COPY\n",
    "    # 2 is TimeSeriesSet CREATE\n",
    "    # ------------------------------------------            \n",
    "    # create new warmState\n",
    "    # create new.nc url and open file to write\n",
    "    new_nc = filename\n",
    "    nc_new = netCDF4.Dataset(new_nc, 'w', format=\"NETCDF3_CLASSIC\")\n",
    "\n",
    "    # create dimensions for new nc file based on in.nc\n",
    "    if STATE_COPY_CREATE == 0 :\n",
    "        nt = 1\n",
    "    elif STATE_COPY_CREATE == 1:\n",
    "        nt = in_nc_arr.dimensions['time'].size\n",
    "    elif STATE_COPY_CREATE == 2:\n",
    "        nt = len(TIME_LIST)        \n",
    "    ny = in_nc_arr.dimensions['y'].size\n",
    "    nx = in_nc_arr.dimensions['x'].size\n",
    "    nc_new.createDimension('time', nt)\n",
    "    nc_new.createDimension('y', ny)\n",
    "    nc_new.createDimension('x', nx)\n",
    "\n",
    "    # copy over time variable from in.nc [only first slice]\n",
    "    time_innc = in_nc_arr.variables['time']\n",
    "    time = nc_new.createVariable('time', 'f8', ('time',))\n",
    "    if STATE_COPY_CREATE == 0:\n",
    "        time[:] = time_innc[-1]\n",
    "    elif STATE_COPY_CREATE == 1:\n",
    "        time[:] = time_innc[:]\n",
    "    elif STATE_COPY_CREATE == 2:\n",
    "        time[:] = TIME_LIST\n",
    "#     ws_epoch_min = time_innc[-1] * 60\n",
    "#     ws_datetime = datetime.datetime.fromtimestamp(ws_epoch_min).strftime('%Y-%m-%d')\n",
    "#     log.write(3,\"This is updateDepth.py: warmStateTime in datetime: \"+str(ws_datetime))\n",
    "    time.standard_name = time_innc.standard_name\n",
    "    time.long_name = time_innc.long_name\n",
    "    time.units = time_innc.units\n",
    "    time.axis = time_innc.axis\n",
    "\n",
    "    # copy over y variable from in.nc\n",
    "    y_innc = in_nc_arr.variables['y']\n",
    "    y = nc_new.createVariable('y', 'f8', ('y',), fill_value = y_innc._FillValue ) \n",
    "    y[:] = y_innc[:]\n",
    "    y.standard_name = y_innc.standard_name\n",
    "    y.long_name = y_innc.long_name\n",
    "    y.units = y_innc.units\n",
    "    y.axis = y_innc.axis\n",
    "\n",
    "    # copy over x variable from in.nc\n",
    "    x_innc = in_nc_arr.variables['x']\n",
    "    x = nc_new.createVariable('x', 'f8', ('x'), fill_value = x_innc._FillValue)\n",
    "    x[:] = x_innc[:]\n",
    "    x.standard_name = x_innc.standard_name\n",
    "    x.long_name = x_innc.long_name\n",
    "    x.units = x_innc.units\n",
    "    x.axis = x_innc.axis\n",
    "\n",
    "    # copy over z variable from in.nc\n",
    "    z_innc = in_nc_arr.variables['z']\n",
    "    z = nc_new.createVariable('z', 'f8', ('y', 'x'), fill_value = z_innc._FillValue)\n",
    "    z[:] = z_innc[:]\n",
    "    z.long_name = z_innc.long_name\n",
    "    z.units = z_innc.units\n",
    "    z.axis = z_innc.axis\n",
    "    try:\n",
    "        z.postive = z_innc.positive\n",
    "    except:\n",
    "        l = 1\n",
    "\n",
    "    # copy over lat variable from in.nc\n",
    "    lat_innc = in_nc_arr.variables['lat']\n",
    "    lat = nc_new.createVariable('lat', 'f8', ('y', 'x'), fill_value = lat_innc._FillValue)\n",
    "    lat[:] = lat_innc[:]\n",
    "    lat.standard_name = lat_innc.standard_name\n",
    "    lat.long_name = lat_innc.long_name\n",
    "    lat.units = lat_innc.units\n",
    "\n",
    "    # copy over lat variable from in.nc\n",
    "    lon_innc = in_nc_arr.variables['lon']\n",
    "    lon = nc_new.createVariable('lon', 'f8', ('y', 'x'), fill_value = lon_innc._FillValue)\n",
    "    lon[:] = lon_innc[:]\n",
    "    lon.standard_name = lon_innc.standard_name\n",
    "    lon.long_name = lon_innc.long_name\n",
    "    lon.units = lon_innc.units\n",
    "\n",
    "    # copy over crs variable from in.nc\n",
    "    crs_innc = in_nc_arr.variables['crs']\n",
    "    crs = nc_new.createVariable('crs', 'i4', ())\n",
    "    crs.long_name = crs_innc.long_name\n",
    "    crs.crs_wkt = crs_innc.crs_wkt\n",
    "    crs.proj4_params = crs_innc.proj4_params\n",
    "    crs.epsg_code = crs_innc.epsg_code\n",
    "\n",
    "    # copy over HM variable from in.nc [only first slice]\n",
    "    HM_innc = in_nc_arr.variables['HM']\n",
    "    if STATE_COPY_CREATE == 0:\n",
    "        noDataVal = HM_innc._FillValue\n",
    "    elif STATE_COPY_CREATE == 1:\n",
    "        noDataVal = HM_innc._FillValue\n",
    "    elif STATE_COPY_CREATE == 2:\n",
    "        noDataVal = fillValue\n",
    "    HM = nc_new.createVariable('HM', 'f8', ('time', 'y', 'x'), fill_value = noDataVal)\n",
    "    HM[:] = HM_ARRAY[:]\n",
    "    #HM[:] = hm_out[:]\n",
    "    HM.long_name = HM_innc.long_name\n",
    "    HM.units = HM_innc.units\n",
    "    HM.coordinates = HM_innc.coordinates\n",
    "    HM.grid_mapping = HM_innc.grid_mapping\n",
    "\n",
    "    #close newly created warmState nc file\n",
    "    nc_new.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inputdir = r'D:\\OMS_Waddenzee\\trunk\\fews\\Modules\\depthUpdate\\input'\n",
    "outputdir = r'D:\\OMS_Waddenzee\\trunk\\fews\\Modules\\depthUpdate\\output'\n",
    "runfiledir = r'D:\\OMS_Waddenzee\\trunk\\fews\\Modules\\depthUpdate\\runfile'\n",
    "statedir = r'D:\\OMS_Waddenzee\\trunk\\fews\\Modules\\depthUpdate\\state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "in_nc = \"%s/in.nc\" % inputdir\n",
    "state_nc = \"%s/state.nc\" % statedir\n",
    "out_nc = \"%s/out.nc\" % outputdir\n",
    "runfile_xml = \"%s/runfile.xml\" % runfiledir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    in_nc_arr = netCDF4.Dataset(in_nc, )\n",
    "except:\n",
    "    state_nc = \"%s/state.nc\" % statedir\n",
    "    runfile_xml = \"%s/runfile.xml\" % runfiledir\n",
    "    state_nc_arr = netCDF4.Dataset(state_nc, )\n",
    "    hm_state = state_nc_arr.variables['HM']  # coldState\n",
    "    hm_state_time = state_nc_arr.variables['time']  # coldState\n",
    "    cs_epoch_min = hm_state_time[0] * 60\n",
    "    cs_date = datetime.datetime.fromtimestamp(cs_epoch_min)#.strftime('%Y-%m-%d')    \n",
    "\n",
    "    # open runfile.xml and get time0 date in datetime format\n",
    "    e = ElementTree.parse(runfile_xml).getroot()\n",
    "    \n",
    "    # get timezone element as integer\n",
    "    timezone=int(float(e[0].text))\n",
    "    \n",
    "    # get t0 element as datetime object\n",
    "    time0_date = e[3].attrib.get('date') # e[3] is time0 in xml file\n",
    "    time0_date_list = time0_date.replace('-', ' ').split(' ')\n",
    "    time0_time = e[3].attrib.get('time')\n",
    "    time0_time_list = time0_time.replace(':', ' ').split(' ')\n",
    "    time0_dt = datetime.datetime(int(time0_date_list[0]), # year\n",
    "                                 int(time0_date_list[1]), # month\n",
    "                                 int(time0_date_list[2]), # day                                \n",
    "                                 int(time0_time_list[0]), # hour\n",
    "                                 int(time0_time_list[0]), # minute\n",
    "                                 int(time0_time_list[0]), # second\n",
    "                                 ) \n",
    "    time0_dt = time0_dt + datetime.timedelta(hours=timezone)\n",
    "    time0_dt = datetime.datetime(*time0_dt.timetuple()[:3])\n",
    "\n",
    "    # get new datelist in MINUTES since epoch\n",
    "    numdays = time0_dt - cs_date\n",
    "    datesISO = [time0_dt - datetime.timedelta(days=x) for x in range(0, numdays.days + 1)]\n",
    "\n",
    "    datesEpoch = []\n",
    "    for date in datesISO:\n",
    "        datesEpoch.append(date.timestamp() / 60.)    \n",
    "    datesEpoch.reverse()\n",
    "    # create numpy array of epoch\n",
    "    #datesEpoch = np.array(datesEpoch)\n",
    "\n",
    "    # repeat array N times where N is number of days between state and time0\n",
    "    N = len(datesEpoch)\n",
    "    A = np.array(hm_state[0,::])\n",
    "    B = np.asarray([A]*N)\n",
    "\n",
    "    # mask nodata values\n",
    "    hm_out = np.ma.masked_equal(B, -999)\n",
    "    \n",
    "    # create NETCDF file for new timeSeriesSet        \n",
    "    createNETCDF(out_nc, in_nc_arr = state_nc_arr, STATE_COPY_CREATE = 2, TIME_LIST = datesEpoch, HM_ARRAY = hm_out )\n",
    "    \n",
    "    # open out_NC as input for new WarmStateFile\n",
    "    out_nc_arr = netCDF4.Dataset(out_nc, )    \n",
    "    newHM_out = hm_out[-1]\n",
    "    \n",
    "    # create NETCDF file for new WarmStateFile        \n",
    "    # createNETCDF(state_nc, in_nc_arr = out_nc_arr, STATE_COPY_CREATE = 0, HM_ARRAY = newHM_out ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 12, 13, 0, 0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time0_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 12, 13, 0, 0, tzinfo=<DstTzInfo 'Europe/Lisbon' WET0:00:00 STD>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localtz.localize(time0_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-01-01 02:00:00\n",
      "2016-01-02 02:00:00\n",
      "2016-01-03 02:00:00\n",
      "2016-01-04 02:00:00\n",
      "2016-01-05 02:00:00\n",
      "2016-01-06 02:00:00\n",
      "2016-01-07 02:00:00\n",
      "2016-01-08 02:00:00\n",
      "2016-01-09 02:00:00\n",
      "2016-01-10 02:00:00\n",
      "2016-01-11 02:00:00\n",
      "2016-01-12 02:00:00\n",
      "2016-01-13 02:00:00\n",
      "2016-01-14 02:00:00\n",
      "2016-01-15 02:00:00\n",
      "2016-01-16 02:00:00\n",
      "2016-01-17 02:00:00\n",
      "2016-01-18 02:00:00\n",
      "2016-01-19 02:00:00\n",
      "2016-01-20 02:00:00\n",
      "2016-01-21 02:00:00\n",
      "2016-01-22 02:00:00\n",
      "2016-01-23 02:00:00\n",
      "2016-01-24 02:00:00\n",
      "2016-01-25 02:00:00\n",
      "2016-01-26 02:00:00\n",
      "2016-01-27 02:00:00\n",
      "2016-01-28 02:00:00\n",
      "2016-01-29 02:00:00\n",
      "2016-01-30 02:00:00\n",
      "2016-01-31 02:00:00\n",
      "2016-02-01 02:00:00\n",
      "2016-02-02 02:00:00\n",
      "2016-02-03 02:00:00\n",
      "2016-02-04 02:00:00\n",
      "2016-02-05 02:00:00\n",
      "2016-02-06 02:00:00\n",
      "2016-02-07 02:00:00\n",
      "2016-02-08 02:00:00\n",
      "2016-02-09 02:00:00\n",
      "2016-02-10 02:00:00\n",
      "2016-02-11 02:00:00\n",
      "2016-02-12 02:00:00\n",
      "2016-02-13 02:00:00\n",
      "2016-02-14 02:00:00\n",
      "2016-02-15 02:00:00\n",
      "2016-02-16 02:00:00\n",
      "2016-02-17 02:00:00\n",
      "2016-02-18 02:00:00\n",
      "2016-02-19 02:00:00\n",
      "2016-02-20 02:00:00\n",
      "2016-02-21 02:00:00\n",
      "2016-02-22 02:00:00\n",
      "2016-02-23 02:00:00\n",
      "2016-02-24 02:00:00\n",
      "2016-02-25 02:00:00\n",
      "2016-02-26 02:00:00\n",
      "2016-02-27 02:00:00\n",
      "2016-02-28 02:00:00\n",
      "2016-02-29 02:00:00\n",
      "2016-03-01 02:00:00\n",
      "2016-03-02 02:00:00\n",
      "2016-03-03 02:00:00\n",
      "2016-03-04 02:00:00\n",
      "2016-03-05 02:00:00\n",
      "2016-03-06 02:00:00\n",
      "2016-03-07 02:00:00\n",
      "2016-03-08 02:00:00\n",
      "2016-03-09 02:00:00\n",
      "2016-03-10 02:00:00\n",
      "2016-03-11 02:00:00\n",
      "2016-03-12 02:00:00\n",
      "2016-03-13 02:00:00\n",
      "2016-03-14 02:00:00\n",
      "2016-03-15 02:00:00\n",
      "2016-03-16 02:00:00\n",
      "2016-03-17 02:00:00\n",
      "2016-03-18 02:00:00\n",
      "2016-03-19 02:00:00\n",
      "2016-03-20 02:00:00\n",
      "2016-03-21 02:00:00\n",
      "2016-03-22 02:00:00\n",
      "2016-03-23 02:00:00\n",
      "2016-03-24 02:00:00\n",
      "2016-03-25 02:00:00\n",
      "2016-03-26 02:00:00\n",
      "2016-03-27 01:00:00\n",
      "2016-03-28 02:00:00\n",
      "2016-03-29 02:00:00\n",
      "2016-03-30 02:00:00\n",
      "2016-03-31 02:00:00\n",
      "2016-04-01 02:00:00\n",
      "2016-04-02 02:00:00\n",
      "2016-04-03 02:00:00\n",
      "2016-04-04 02:00:00\n",
      "2016-04-05 02:00:00\n",
      "2016-04-06 02:00:00\n",
      "2016-04-07 02:00:00\n",
      "2016-04-08 02:00:00\n",
      "2016-04-09 02:00:00\n",
      "2016-04-10 02:00:00\n",
      "2016-04-11 02:00:00\n",
      "2016-04-12 02:00:00\n",
      "2016-04-13 02:00:00\n",
      "2016-04-14 02:00:00\n",
      "2016-04-15 02:00:00\n",
      "2016-04-16 02:00:00\n",
      "2016-04-17 02:00:00\n",
      "2016-04-18 02:00:00\n",
      "2016-04-19 02:00:00\n",
      "2016-04-20 02:00:00\n",
      "2016-04-21 02:00:00\n",
      "2016-04-22 02:00:00\n",
      "2016-04-23 02:00:00\n",
      "2016-04-24 02:00:00\n",
      "2016-04-25 02:00:00\n",
      "2016-04-26 02:00:00\n",
      "2016-04-27 02:00:00\n",
      "2016-04-28 02:00:00\n",
      "2016-04-29 02:00:00\n",
      "2016-04-30 02:00:00\n",
      "2016-05-01 02:00:00\n",
      "2016-05-02 02:00:00\n",
      "2016-05-03 02:00:00\n",
      "2016-05-04 02:00:00\n",
      "2016-05-05 02:00:00\n",
      "2016-05-06 02:00:00\n",
      "2016-05-07 02:00:00\n",
      "2016-05-08 02:00:00\n",
      "2016-05-09 02:00:00\n",
      "2016-05-10 02:00:00\n",
      "2016-05-11 02:00:00\n",
      "2016-05-12 02:00:00\n",
      "2016-05-13 02:00:00\n",
      "2016-05-14 02:00:00\n",
      "2016-05-15 02:00:00\n",
      "2016-05-16 02:00:00\n",
      "2016-05-17 02:00:00\n",
      "2016-05-18 02:00:00\n",
      "2016-05-19 02:00:00\n",
      "2016-05-20 02:00:00\n",
      "2016-05-21 02:00:00\n",
      "2016-05-22 02:00:00\n",
      "2016-05-23 02:00:00\n",
      "2016-05-24 02:00:00\n",
      "2016-05-25 02:00:00\n",
      "2016-05-26 02:00:00\n",
      "2016-05-27 02:00:00\n",
      "2016-05-28 02:00:00\n",
      "2016-05-29 02:00:00\n",
      "2016-05-30 02:00:00\n",
      "2016-05-31 02:00:00\n",
      "2016-06-01 02:00:00\n",
      "2016-06-02 02:00:00\n",
      "2016-06-03 02:00:00\n",
      "2016-06-04 02:00:00\n",
      "2016-06-05 02:00:00\n",
      "2016-06-06 02:00:00\n",
      "2016-06-07 02:00:00\n",
      "2016-06-08 02:00:00\n",
      "2016-06-09 02:00:00\n",
      "2016-06-10 02:00:00\n",
      "2016-06-11 02:00:00\n",
      "2016-06-12 02:00:00\n",
      "2016-06-13 02:00:00\n",
      "2016-06-14 02:00:00\n",
      "2016-06-15 02:00:00\n",
      "2016-06-16 02:00:00\n",
      "2016-06-17 02:00:00\n",
      "2016-06-18 02:00:00\n",
      "2016-06-19 02:00:00\n",
      "2016-06-20 02:00:00\n",
      "2016-06-21 02:00:00\n",
      "2016-06-22 02:00:00\n",
      "2016-06-23 02:00:00\n",
      "2016-06-24 02:00:00\n",
      "2016-06-25 02:00:00\n",
      "2016-06-26 02:00:00\n",
      "2016-06-27 02:00:00\n",
      "2016-06-28 02:00:00\n",
      "2016-06-29 02:00:00\n",
      "2016-06-30 02:00:00\n",
      "2016-07-01 02:00:00\n",
      "2016-07-02 02:00:00\n",
      "2016-07-03 02:00:00\n",
      "2016-07-04 02:00:00\n",
      "2016-07-05 02:00:00\n",
      "2016-07-06 02:00:00\n",
      "2016-07-07 02:00:00\n",
      "2016-07-08 02:00:00\n",
      "2016-07-09 02:00:00\n",
      "2016-07-10 02:00:00\n",
      "2016-07-11 02:00:00\n",
      "2016-07-12 02:00:00\n",
      "2016-07-13 02:00:00\n",
      "2016-07-14 02:00:00\n",
      "2016-07-15 02:00:00\n",
      "2016-07-16 02:00:00\n",
      "2016-07-17 02:00:00\n",
      "2016-07-18 02:00:00\n",
      "2016-07-19 02:00:00\n",
      "2016-07-20 02:00:00\n",
      "2016-07-21 02:00:00\n",
      "2016-07-22 02:00:00\n",
      "2016-07-23 02:00:00\n",
      "2016-07-24 02:00:00\n",
      "2016-07-25 02:00:00\n",
      "2016-07-26 02:00:00\n",
      "2016-07-27 02:00:00\n",
      "2016-07-28 02:00:00\n",
      "2016-07-29 02:00:00\n",
      "2016-07-30 02:00:00\n",
      "2016-07-31 02:00:00\n",
      "2016-08-01 02:00:00\n",
      "2016-08-02 02:00:00\n",
      "2016-08-03 02:00:00\n",
      "2016-08-04 02:00:00\n",
      "2016-08-05 02:00:00\n",
      "2016-08-06 02:00:00\n",
      "2016-08-07 02:00:00\n",
      "2016-08-08 02:00:00\n",
      "2016-08-09 02:00:00\n",
      "2016-08-10 02:00:00\n",
      "2016-08-11 02:00:00\n",
      "2016-08-12 02:00:00\n",
      "2016-08-13 02:00:00\n",
      "2016-08-14 02:00:00\n",
      "2016-08-15 02:00:00\n",
      "2016-08-16 02:00:00\n",
      "2016-08-17 02:00:00\n",
      "2016-08-18 02:00:00\n",
      "2016-08-19 02:00:00\n",
      "2016-08-20 02:00:00\n",
      "2016-08-21 02:00:00\n",
      "2016-08-22 02:00:00\n",
      "2016-08-23 02:00:00\n",
      "2016-08-24 02:00:00\n",
      "2016-08-25 02:00:00\n",
      "2016-08-26 02:00:00\n",
      "2016-08-27 02:00:00\n",
      "2016-08-28 02:00:00\n",
      "2016-08-29 02:00:00\n",
      "2016-08-30 02:00:00\n",
      "2016-08-31 02:00:00\n",
      "2016-09-01 02:00:00\n",
      "2016-09-02 02:00:00\n",
      "2016-09-03 02:00:00\n",
      "2016-09-04 02:00:00\n",
      "2016-09-05 02:00:00\n",
      "2016-09-06 02:00:00\n",
      "2016-09-07 02:00:00\n",
      "2016-09-08 02:00:00\n",
      "2016-09-09 02:00:00\n",
      "2016-09-10 02:00:00\n",
      "2016-09-11 02:00:00\n",
      "2016-09-12 02:00:00\n",
      "2016-09-13 02:00:00\n",
      "2016-09-14 02:00:00\n",
      "2016-09-15 02:00:00\n",
      "2016-09-16 02:00:00\n",
      "2016-09-17 02:00:00\n",
      "2016-09-18 02:00:00\n",
      "2016-09-19 02:00:00\n",
      "2016-09-20 02:00:00\n",
      "2016-09-21 02:00:00\n",
      "2016-09-22 02:00:00\n",
      "2016-09-23 02:00:00\n",
      "2016-09-24 02:00:00\n",
      "2016-09-25 02:00:00\n",
      "2016-09-26 02:00:00\n",
      "2016-09-27 02:00:00\n",
      "2016-09-28 02:00:00\n",
      "2016-09-29 02:00:00\n",
      "2016-09-30 02:00:00\n",
      "2016-10-01 02:00:00\n",
      "2016-10-02 02:00:00\n",
      "2016-10-03 02:00:00\n",
      "2016-10-04 02:00:00\n",
      "2016-10-05 02:00:00\n",
      "2016-10-06 02:00:00\n",
      "2016-10-07 02:00:00\n",
      "2016-10-08 02:00:00\n",
      "2016-10-09 02:00:00\n",
      "2016-10-10 02:00:00\n",
      "2016-10-11 02:00:00\n",
      "2016-10-12 02:00:00\n",
      "2016-10-13 02:00:00\n",
      "2016-10-14 02:00:00\n",
      "2016-10-15 02:00:00\n",
      "2016-10-16 02:00:00\n",
      "2016-10-17 02:00:00\n",
      "2016-10-18 02:00:00\n",
      "2016-10-19 02:00:00\n",
      "2016-10-20 02:00:00\n",
      "2016-10-21 02:00:00\n",
      "2016-10-22 02:00:00\n",
      "2016-10-23 02:00:00\n",
      "2016-10-24 02:00:00\n",
      "2016-10-25 02:00:00\n",
      "2016-10-26 02:00:00\n",
      "2016-10-27 02:00:00\n",
      "2016-10-28 02:00:00\n",
      "2016-10-29 02:00:00\n",
      "2016-10-30 02:00:00\n",
      "2016-10-31 02:00:00\n",
      "2016-11-01 02:00:00\n",
      "2016-11-02 02:00:00\n",
      "2016-11-03 02:00:00\n",
      "2016-11-04 02:00:00\n",
      "2016-11-05 02:00:00\n",
      "2016-11-06 02:00:00\n",
      "2016-11-07 02:00:00\n",
      "2016-11-08 02:00:00\n",
      "2016-11-09 02:00:00\n",
      "2016-11-10 02:00:00\n",
      "2016-11-11 02:00:00\n",
      "2016-11-12 02:00:00\n",
      "2016-11-13 02:00:00\n",
      "2016-11-14 02:00:00\n",
      "2016-11-15 02:00:00\n",
      "2016-11-16 02:00:00\n",
      "2016-11-17 02:00:00\n",
      "2016-11-18 02:00:00\n",
      "2016-11-19 02:00:00\n",
      "2016-11-20 02:00:00\n",
      "2016-11-21 02:00:00\n",
      "2016-11-22 02:00:00\n",
      "2016-11-23 02:00:00\n",
      "2016-11-24 02:00:00\n",
      "2016-11-25 02:00:00\n",
      "2016-11-26 02:00:00\n",
      "2016-11-27 02:00:00\n",
      "2016-11-28 02:00:00\n",
      "2016-11-29 02:00:00\n",
      "2016-11-30 02:00:00\n",
      "2016-12-01 02:00:00\n",
      "2016-12-02 02:00:00\n",
      "2016-12-03 02:00:00\n",
      "2016-12-04 02:00:00\n",
      "2016-12-05 02:00:00\n",
      "2016-12-06 02:00:00\n",
      "2016-12-07 02:00:00\n",
      "2016-12-08 02:00:00\n",
      "2016-12-09 02:00:00\n",
      "2016-12-10 02:00:00\n",
      "2016-12-11 02:00:00\n",
      "2016-12-12 02:00:00\n",
      "2016-12-13 02:00:00\n"
     ]
    }
   ],
   "source": [
    "for stamp in datesEpoch:\n",
    "    print(datetime.datetime.fromtimestamp(stamp * 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    try:\n",
    "        # DOE IETS SLIMS VANAF HIER\n",
    "        \n",
    "        #shutil.copyfile(\"%s/in.nc\" % inputdir, \"%s/out.nc\" % outputdir)\n",
    "        \n",
    "        # load input netcdf and state netcdf\n",
    "        # in_nc = r'D:\\OMS_Waddenzee\\trunk\\fews\\Modules\\depthUpdate4pythontesting\\input//in.nc'\n",
    "        # state_nc = r'D:\\OMS_Waddenzee\\trunk\\fews\\Modules\\depthUpdate4pythontesting\\state//state.nc'\n",
    "        # out_nc = r'D:\\OMS_Waddenzee\\trunk\\fews\\Modules\\depthUpdate4pythontesting\\output//out.nc'\n",
    "\n",
    "        in_nc = \"%s/in.nc\" % inputdir\n",
    "        state_nc = \"%s/state.nc\" % statedir\n",
    "        out_nc = \"%s/out.nc\" % outputdir\n",
    "\n",
    "        in_nc_arr = netCDF4.Dataset(in_nc, )\n",
    "        state_nc_arr = netCDF4.Dataset(state_nc, )\n",
    "        # print(in_nc_arr)\n",
    "\n",
    "        # print(in_nc_arr.variables.keys()) # get all variable names\n",
    "        hm = in_nc_arr.variables['HM']  # bodemhoogte peiling\n",
    "        hm_state = state_nc_arr.variables['HM']  # coldState\n",
    "        hm_state_time = state_nc_arr.variables['time']  # coldState\n",
    "        cs_epoch_min = hm_state_time[0] * 60\n",
    "        cs_datetime = datetime.datetime.fromtimestamp(cs_epoch_min).strftime('%Y-%m-%d')        \n",
    "        log.write(3,\"This is updateDepth.py: coldStateTime in ms since epoch: \"+str(cs_datetime))\n",
    "\n",
    "        # create copy in.nc\n",
    "        hm_copy = np.copy(hm)\n",
    "        hm_copy = np.ma.masked_equal(hm_copy, -999)\n",
    "\n",
    "        # use coldState to update first slice in.nc\n",
    "        a = hm_copy[0,::] \n",
    "        b = hm_state[0,::]\n",
    "        # update first slice based on coldState\n",
    "        a[~b.mask] = b.compressed()\n",
    "        hm_copy[0,::] = a\n",
    "\n",
    "        # create init warmState\n",
    "        hm_out = hm_copy[0,::]\n",
    "\n",
    "        for ix in range(hm.shape[0] - 1):    \n",
    "            # print (ix)\n",
    "            # one by one fill/update and ammend arrays\n",
    "            # get first slice of copy, get second slice of original\n",
    "            a = hm_copy[ix,::] \n",
    "            b = hm[ix+1,::]  \n",
    "\n",
    "            # update first slice based on second slice\n",
    "            a[~b.mask] = b.compressed()\n",
    "            # update timeSeriesSet\n",
    "            hm_copy[ix+1,::] = a    \n",
    "            # update warmState\n",
    "            hm_out[~a.mask] = a.compressed()        \n",
    "\n",
    "        # ------------------------------------------            \n",
    "        # create new warmState\n",
    "        # create NETCDF file for new timeSeriesSet        \n",
    "        createNETCDF(state_nc, in_nc_arr = in_nc_arr, STATE_COPY_CREATE = 0, HM_ARRAY = hm_out )\n",
    " \n",
    "\n",
    "        # ------------------------------------------\n",
    "        # create out.nc timeSeriesSet\n",
    "        # create new.nc url and open file to write\n",
    "        createNETCDF(out_nc, in_nc_arr = in_nc_arr, STATE_COPY_CREATE = 1, HM_ARRAY = hm_out )\n",
    "\n",
    "        \n",
    "    except:\n",
    "        try:        \n",
    "            state_nc = \"%s/state.nc\" % statedir\n",
    "            runfile_xml = \"%s/runfile.xml\" % runfiledir\n",
    "            state_nc_arr = netCDF4.Dataset(state_nc, )\n",
    "            hm_state = state_nc_arr.variables['HM']  # coldState\n",
    "            hm_state_time = state_nc_arr.variables['time']  # coldState\n",
    "            cs_epoch_min = hm_state_time[0] * 60\n",
    "            cs_date = datetime.datetime.fromtimestamp(cs_epoch_min)#.strftime('%Y-%m-%d')    \n",
    "\n",
    "            # open runfile.xml and get time0 date in datetime format\n",
    "            e = ElementTree.parse(runfile_xml).getroot()\n",
    "            time0 = e[3].attrib.get('date') # e[3] is time0 in xml file\n",
    "\n",
    "            time0_list = time0.replace('-', ' ').split(' ')\n",
    "            time0_date = datetime.datetime(int(time0_list[0]), int(time0_list[1]), int(time0_list[2]),2)   \n",
    "\n",
    "            # get new datelist in MINUTES since epoch\n",
    "            numdays = time0_date - cs_date\n",
    "            datesISO = [time0_date - datetime.timedelta(days=x) for x in range(0, numdays.days + 1)]\n",
    "\n",
    "            datesEpoch = []\n",
    "            for date in datesISO:\n",
    "                datesEpoch.append(date.timestamp() / 60.)    \n",
    "            datesEpoch.reverse()\n",
    "            # create numpy array of epoch\n",
    "            #datesEpoch = np.array(datesEpoch)\n",
    "\n",
    "            # repeat array N times where N is number of days between state and time0\n",
    "            N = len(datesEpoch)\n",
    "            A = np.array(hm_state[0,::])\n",
    "            B = np.asarray([A]*N)\n",
    "\n",
    "            # mask nodata values\n",
    "            hm_out = np.ma.masked_equal(B, -999)\n",
    "\n",
    "            # create NETCDF file for new timeSeriesSet        \n",
    "            createNETCDF(out_nc, in_nc_arr = state_nc_arr, STATE_COPY_CREATE = 2, TIME_LIST = datesEpoch, HM_ARRAY = hm_out )\n",
    "\n",
    "            # open out_NC as input for new WarmStateFile\n",
    "            out_nc_arr = netCDF4.Dataset(out_nc, )    \n",
    "            newHM_out = hm_out[-1]\n",
    "\n",
    "            # create NETCDF file for new WarmStateFile        \n",
    "            createNETCDF(state_nc, in_nc_arr = out_nc_arr, STATE_COPY_CREATE = 0, HM_ARRAY = newHM_out )             \n",
    "        \n",
    "        \n",
    "        except:\n",
    "            if not (isdir(inputdir)):\n",
    "                log.write(1,\"%s is not a valid path\" % inputdir)\n",
    "            elif not (isdir(statedir)):\n",
    "                log.write(1,\"%s is not a valid path\" % statedir)\n",
    "            elif not (isdir(outputdir)):\n",
    "                log.write(1,\"%s is not a valid path\" % outputdir)\n",
    "            elif not (isdir(runfiledir)):\n",
    "                log.write(1,\"%r is not a valid path\" % runfiledir)\n",
    "            else:\n",
    "                log.write(1,\"something else is funky\")            \n",
    "        \n",
    "    # TOT HIER\n",
    "    log.write(3,\"Dat was het, FEWS take over please\")\n",
    "    log.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
